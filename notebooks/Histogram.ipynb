{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "from torchsample.transforms import RangeNormalize\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import gradcheck\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rangeNormalize(data):\n",
    "    return 0.99*(data-torch.min(data))/(torch.max(data)-torch.min(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.92577792  0.97461713  0.26061324]\n",
      "  [ 0.27619581  0.17157076  0.13439798]\n",
      "  [ 0.          0.46275909  0.59747843]\n",
      "  [ 0.84886718  0.18526368  0.7580244 ]\n",
      "  [ 0.69606309  0.99        0.49138517]]\n",
      "\n",
      " [[ 0.28175641  0.60825135  0.21435954]\n",
      "  [ 0.06277362  0.14309147  0.12175607]\n",
      "  [ 0.5701884   0.1956856   0.25646536]\n",
      "  [ 0.49377944  0.98901034  0.93569606]\n",
      "  [ 0.72656384  0.4124462   0.75128254]]\n",
      "\n",
      " [[ 0.05954154  0.40998872  0.43326102]\n",
      "  [ 0.32730437  0.58881543  0.90616193]\n",
      "  [ 0.88365218  0.39360379  0.37034296]\n",
      "  [ 0.14999468  0.74478583  0.27319051]\n",
      "  [ 0.15413811  0.0761583   0.08925235]]]\n"
     ]
    }
   ],
   "source": [
    "# data size: batch x num_points x feature\n",
    "nbins = 4\n",
    "batch_size = 3\n",
    "num_points = 5\n",
    "feature_size = 3\n",
    "test_data = np.random.random_sample((batch_size, num_points, feature_size))\n",
    "\n",
    "test_data = rangeNormalize(torch.from_numpy(test_data)).numpy()\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram 1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram(data, bins, range=(0,1)):\n",
    "    \"\"\"\n",
    "    find the histogram of given data\n",
    "    Current range, (0,1), others haven't been tested\n",
    "    \n",
    "    return histogram, TODO: ret_index, ret_value\n",
    "    \n",
    "    \"\"\"\n",
    "    ret_histogram = torch.zeros(bins)\n",
    "    ret_indexes = torch.zeros(data.size())\n",
    "    for idx, d in enumerate(data.view(data.numel())):\n",
    "        if d == 1.0:\n",
    "            bin_number = bins - 1\n",
    "        else:\n",
    "            bin_number = int(bins*d/(range[1]-range[0]))\n",
    "        if bin_number == bins:\n",
    "            bin_number = bin_number - 1\n",
    "        \n",
    "        ret_histogram[bin_number] += 1\n",
    "        if len(data.size()) == 1:\n",
    "            ret_indexes[idx] = bin_number\n",
    "        else:\n",
    "            idx_a = int(idx/data.size()[-1])\n",
    "            idx_b = idx%data.size()[-1]\n",
    "            ret_indexes[idx_a, idx_b] = bin_number\n",
    "    \n",
    "    return ret_histogram, ret_indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram1d Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       "  4  4  2  5\n",
       "  5  4  3  3\n",
       "  5  6  2  2\n",
       " [torch.FloatTensor of size 3x4], \n",
       " (0 ,.,.) = \n",
       "   3  3  1\n",
       "   1  0  0\n",
       "   0  1  2\n",
       "   3  0  3\n",
       "   2  3  1\n",
       " \n",
       " (1 ,.,.) = \n",
       "   1  2  0\n",
       "   0  0  0\n",
       "   2  0  1\n",
       "   1  3  3\n",
       "   2  1  3\n",
       " \n",
       " (2 ,.,.) = \n",
       "   0  1  1\n",
       "   1  2  3\n",
       "   3  1  1\n",
       "   0  2  1\n",
       "   0  0  0\n",
       " [torch.IntTensor of size 3x5x3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def histogram_vectorize(data, bins, range=(0,1)):\n",
    "    \"\"\"\n",
    "    find the histogram of given data: batch x seq x feature\n",
    "    Current range, (0,1), others haven't been tested\n",
    "    \n",
    "    return histogram, TODO: ret_index, ret_value\n",
    "    \n",
    "    \"\"\"\n",
    "    ret_histogram = torch.zeros(bins)\n",
    "    ret_indexes = torch.zeros(data.size())\n",
    "    \n",
    "    # calculate the the corresponding bin number \n",
    "    temp = (data*bins).int()\n",
    "    \n",
    "    # transform into one-hot vector and then sum\n",
    "    y_tensor = temp\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(data.size()[0],-1, 1)\n",
    "    # given a list of numbers, we transform each number {i} to one_hot vector(nbins x 1) where index i is 1\n",
    "    # scatter(dim, index, val)\n",
    "    y_one_hot = torch.zeros(data.size()[0],y_tensor.size()[1], bins).scatter_(2, y_tensor, 1)\n",
    "    \n",
    "    ret_indexes = temp.view(data.size())\n",
    "    ret_histogram = y_one_hot.sum(1)\n",
    "\n",
    "    return ret_histogram, ret_indexes\n",
    "histogram_vectorize(torch.from_numpy(test_data), nbins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing output (with numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# mirror output\n",
    "\n",
    "\n",
    "def np_solution(x,nbins):\n",
    "    histograms = np.zeros((batch_size, nbins))\n",
    "    for i in range(batch_size):\n",
    "        # assert sum is the same as the count\n",
    "        histograms[i,:] = np.histogram(x[i], bins=nbins, range=(0,1))[0]\n",
    "    return histograms\n",
    "\n",
    "def my_histogram(x,nbins):\n",
    "    histograms = torch.zeros((batch_size, nbins))\n",
    "    histograms_indexes = torch.zeros(x.shape)\n",
    "    for i in range(batch_size):\n",
    "        histograms[i,:], histograms_indexes[i,:] = histogram(torch.from_numpy(x[i]), bins=nbins)\n",
    "    return histograms, histograms_indexes\n",
    "\n",
    "def my_histogram_vec(x,nbins):\n",
    "    my_histogram_vec, my_indexes_vec = histogram_vectorize(torch.from_numpy(x), bins=nbins, range=(0,1))\n",
    "    return my_histogram_vec, my_indexes_vec\n",
    "\n",
    "np_hist = np_solution(test_data,nbins)\n",
    "my_hist, my_idx = my_histogram(test_data,nbins)\n",
    "my_hist_v, my_idx_vec = my_histogram_vec(test_data,nbins)\n",
    "\n",
    "assert(np.all(np_hist==my_hist))\n",
    "assert(np.all(np_hist==my_hist_v))\n",
    "assert(np.all(my_idx.float() == my_idx_vec.float()))\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np solution time\n",
      "0.00029921531677246094\n",
      "my hist time\n",
      "0.0007569789886474609\n",
      "my hist vec time\n",
      "0.00021958351135253906\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(\"np solution time\")\n",
    "t = time.time()\n",
    "np_hist = np_solution(test_data, nbins)\n",
    "print(time.time() - t)\n",
    "\n",
    "print(\"my hist time\")\n",
    "t = time.time()\n",
    "my_hist = my_histogram(test_data, nbins)\n",
    "print(time.time() - t)\n",
    "\n",
    "print(\"my hist vec time\")\n",
    "t = time.time()\n",
    "my_hist_v = my_histogram_vec(test_data, nbins)\n",
    "print(time.time() - t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram 2d Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram2d_vectorize(data, bins, range=(0,1)):\n",
    "    \"\"\"\n",
    "    find the histogram of given data: batch x seq x feature\n",
    "    Current range, (0,1), others haven't been tested\n",
    "    \n",
    "    return histogram along the second column. eg : batch x bins x feature\n",
    "    \n",
    "    \"\"\"\n",
    "    ret_histogram = torch.zeros(bins)\n",
    "    ret_indexes = torch.zeros(data.size())\n",
    "    \n",
    "    # calculate the the corresponding bin number \n",
    "    temp = (data*bins).int()\n",
    "    temp = temp.transpose(2,1)\n",
    "    # transform into one-hot vector and then sum\n",
    "    y_tensor = temp\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(data.size()[0], data.size()[-1],-1, 1)\n",
    "\n",
    "    # given a list of numbers, we transform each number {i} to one_hot vector(nbins x 1) where index i is 1\n",
    "    # scatter(dim, index, val)\n",
    "    y_one_hot = torch.zeros(data.size()[0], data.size()[-1], data.size()[1], bins).scatter_(3, y_tensor.long(), 1)\n",
    "\n",
    "    ret_indexes = temp.transpose(2,1)\n",
    "    ret_histogram = y_one_hot.sum(2).transpose(2,1)\n",
    "\n",
    "    return ret_histogram, ret_indexes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_solution2d(x, nbins):\n",
    "    histograms2d = np.zeros((batch_size, nbins, feature_size))\n",
    "    for i in range(batch_size):\n",
    "        for j in range(feature_size):\n",
    "            histograms2d[i,:,j] = np.histogram(x[i,:,j], bins=nbins, range=(0,1))[0]\n",
    "    return histograms2d\n",
    "\n",
    "def my_histogram2d(x, nbins):\n",
    "    histograms2d = torch.zeros((batch_size, nbins, feature_size))\n",
    "    histograms2d_idx = torch.zeros(x.shape)\n",
    "    for i in range(batch_size):\n",
    "        for j in range(feature_size):\n",
    "            histograms2d[i,:,j],histograms2d_idx[i,:,j] = histogram(x[i,:,j],bins=nbins)\n",
    "            \n",
    "    return histograms2d, histograms2d_idx\n",
    "def my_histogram2d_vec(x, nbins):\n",
    "    histogram2d, histogram2d_indexes = histogram2d_vectorize(x, nbins)\n",
    "    return histogram2d, histogram2d_indexes\n",
    "\n",
    "np_hist2d = np_solution2d(test_data, nbins)\n",
    "my_hist2d, my_hist2d_idx = my_histogram2d(torch.from_numpy(test_data), nbins)\n",
    "my_hist2d_vec, my_hist2d_vec_idx = my_histogram2d_vec(torch.from_numpy(test_data), nbins)\n",
    "\n",
    "assert(np.all(np_hist2d==my_hist2d))\n",
    "assert(np.all(np_hist2d==my_hist2d_vec))\n",
    "assert(np.all(my_hist2d_idx.float()==my_hist2d_vec_idx.float()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Vectorizing Backward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shows pytorch can boardcast\n",
    "bs, bn = 5, 6\n",
    "idx = torch.zeros(bs,bn).cuda()+0.1\n",
    "w = torch.ones(bn).cuda()\n",
    "hist = torch.zeros(bs,bn).cuda()+2\n",
    "# print(I*y/z)\n",
    "# values = grad_output.data*weight/histograms\n",
    "v = idx*w/hist\n",
    "for i in range(bs):\n",
    "    for j in range(bn):\n",
    "        assert(v[i,j]==(idx[i,j]*w[j]/hist[i,j]))\n",
    "\n",
    "# for i in range(input.size()[0]): #batch_size\n",
    "#     grad_input[i] = torch.index_select(values[i], 0, histograms_indexes[i].long().view(-1)).view(grad_input[i].size())\n",
    "#     for a in range(input.size()[1]): #num_of_points\n",
    "#         for b in range(input.size()[2]):\n",
    "#             bin_number = int(histograms_indexes[i,a,b])\n",
    "\n",
    "#             grad_input3[i,a,b] = grad_output.data[i,bin_number]*weight[bin_number]/histograms[i,bin_number] #TODO: divide by number of element in the bin\n",
    "#             print(values[i,bin_number]==grad_input3[i,a,b])\n",
    "        \n",
    "\n",
    "# indices: same size as output; the corresponding index value in x \n",
    "# x : the values to be assign\n",
    "# index_select: given x (value to be drawn from), dim, vector of indexes\n",
    "#               return a vector of values( x[indices[i] for i in indexes])\n",
    "\n",
    "# 1d case\n",
    "bs, seq, fs, bn = 3, 5, 4, 4\n",
    "indices = torch.LongTensor(bs, seq, fs).random_(0,bn)\n",
    "indices_orig = indices.clone()\n",
    "# np equivalent\n",
    "i3 = torch.from_numpy(np.repeat(np.arange(bs),seq*fs)*bn).view(indices.shape)\n",
    "# torch version\n",
    "i2 = torch.arange(0, bs*bn, step=bn).unsqueeze(-1).expand(-1,seq*fs)\n",
    "\n",
    "# print(i3)#.repeat(seq*fs)*bn\n",
    "i2 = i2.view(indices.shape).long()\n",
    "\n",
    "indices = indices + i2\n",
    "grad_output = torch.randn(bs,bn)*10\n",
    "grad_output = grad_output.int()\n",
    "\n",
    "ans = torch.index_select(grad_output.view(-1), 0, indices.view(-1)).view(bs,seq,fs)\n",
    "\n",
    "for i in range(bs):\n",
    "    for x in range(fs):\n",
    "        for y in range(seq):\n",
    "            bin_number = indices_orig[i,y,x]\n",
    "\n",
    "            assert(ans[i,y,x] == grad_output[i,bin_number])\n",
    "            \n",
    "\n",
    "# 2d case\n",
    "# batch x feature x seq\n",
    "bs, seq, fs, bn = 3,5,3,4\n",
    "indices = torch.LongTensor(bs, seq, fs).random_(0,bn)\n",
    "indices_orig = indices.clone()\n",
    "\n",
    "indices = indices.transpose(2,1)\n",
    "# np version\n",
    "i2 = torch.from_numpy(np.repeat(np.arange(bs*fs),seq)*bn).view(indices.shape)\n",
    "# torch version\n",
    "i2 = torch.arange(0, bs*fs*bn, step=bn).unsqueeze(-1).expand(-1,seq)\n",
    "i2 = i2.view(indices.shape).long()\n",
    "# batch x feature x bins\n",
    "grad_output = torch.randn(bs,bn,fs).transpose(2,1)*10\n",
    "grad_output = grad_output.int()\n",
    "weight = (torch.randn(bn,fs)*10).int()\n",
    "indices = indices + i2\n",
    "\n",
    "ans = torch.index_select(grad_output.view(-1), 0, indices.view(-1)).view(indices.shape)\n",
    "\n",
    "grad_output = grad_output.transpose(2,1)\n",
    "ans = ans.transpose(2,1)\n",
    "\n",
    "\n",
    "for i in range(bs):\n",
    "    for x in range(fs):\n",
    "        for y in range(seq):\n",
    "            bin_number = indices_orig[i,y,x]\n",
    "            assert(ans[i,y,x]==grad_output[i,bin_number,x])\n",
    "            \n",
    "\n",
    "# psudo code for grad output\n",
    "# for i in range(batch_size):\n",
    "#     for x in range(feature_size):\n",
    "#         for y in range(seq):\n",
    "#             bin_number = hist_index[i,y,x]\n",
    "#             grad_input[i,y,x] = grad_output[i,bin_number,x]*weight[bin_number,x]/histogram2d[i,bin_number,x] \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram 1d Pytorch Function and Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistogramFunction(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    assume input is already normalized from 0 to 1\n",
    "    assume input size is batch_size, num_points, feature_size\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    PARAM: input, weight, bins\n",
    "    \n",
    "    input: batch_size, num_points, feature_size\n",
    "    weight:  (TBD) size of input OR size of bins\n",
    "    bins: 10 by defaulted\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, bins=10):\n",
    "        batch_size = input.size()[0]\n",
    "        histograms = torch.zeros(batch_size, bins).cuda()\n",
    "        histograms_indexes = torch.zeros(input.size()).cuda()\n",
    "        \n",
    "        histograms, histograms_indexes = histogram_vectorize(input, bins=bins)\n",
    "        histograms = histograms.cuda()\n",
    "        histograms_indexes = histograms_indexes.cuda()\n",
    "        \n",
    "        histograms = histograms* weight\n",
    "        \n",
    "        #non-vec version\n",
    "#         for i in range(batch_size):\n",
    "#             # assert sum is the same as the count\n",
    "#             histograms[i,:], histograms_values[i,:] = histogram(input[i], bins=bins)\n",
    "#             histograms[i,:] = histograms[i,:] * weight\n",
    "        \n",
    "        ctx.save_for_backward(input, weight)\n",
    "        ctx.histograms_indexes = histograms_indexes\n",
    "        ctx.histograms = histograms\n",
    "        return histograms\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, weight= ctx.saved_tensors\n",
    "        histograms_indexes = ctx.histograms_indexes\n",
    "        histograms = ctx.histograms\n",
    "        grad_input = torch.zeros(input.size()).cuda()\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        the gradient at batch item i, point a, element b:\n",
    "          grad_output[i, bin_number]*weight[bin_number]/(histogram[i,bin_number])\n",
    "        \"\"\"\n",
    "#         values = grad_output.data*weight/histograms\n",
    "\n",
    "        \n",
    "        for i in range(input.size()[0]): #batch_size\n",
    "#             grad_input[i] = torch.index_select(values[i], 0, histograms_indexes[i].long().view(-1)).view(grad_input[i].size())\n",
    "            for a in range(input.size()[1]): #num_of_points\n",
    "                for b in range(input.size()[2]):\n",
    "                    bin_number = int(histograms_indexes[i,a,b])\n",
    "                    grad_input[i,a,b] = grad_output.data[i,bin_number]*weight[bin_number]/histograms[i,bin_number]\n",
    "#                     if (values[i,bin_number] - grad_output.data[i,bin_number]*weight[bin_number]/histograms[i,bin_number])>1e-5:\n",
    "#                         print(\"HI\")\n",
    "#             assert(np.all(grad_input[i]-grad_input3[i]<=1e-4))\n",
    "        \n",
    "    \n",
    "        # vectorize version:: ######################\n",
    "        # values is the grad_output\n",
    "        # histograms_values are the indices\n",
    "#         bs, seq, fs, bn = input.size()[0], input.size()[1], input.size()[2], weight.size()[0]\n",
    "#         # np equivalent\n",
    "# #         idx_cat = torch.from_numpy(np.repeat(np.arange(bs),seq*fs)*bn).view(indices.shape)\n",
    "#         # torch version\n",
    "#         idx_cat = torch.arange(0, bs*bn, step=bn).cuda().unsqueeze(-1).expand(-1,seq*fs)\n",
    "\n",
    "#         idx_cat = idx_cat.view(histograms_indexes.shape).int()\n",
    "\n",
    "#         histograms_indexes = histograms_indexes + idx_cat\n",
    "\n",
    "#         grad_input = torch.index_select(values.view(-1), 0, histograms_indexes.long().view(-1)).view(bs,seq,fs)\n",
    "        ####################\n",
    "        \n",
    "        grad_weight = (grad_output.data*histograms).sum(0)/bs\n",
    "        \n",
    "#         print(grad_input)\n",
    "        \n",
    "        return Variable(grad_input), Variable(grad_weight), None\n",
    "        \n",
    "\n",
    "class Histogram(nn.Module):\n",
    "    def __init__(self, bins=10):\n",
    "        super(Histogram, self).__init__()\n",
    "        self.bins = bins\n",
    "\n",
    "        # nn.Parameter is a special kind of Variable, that will get\n",
    "        # automatically registered as Module's parameter once it's assigned\n",
    "        # as an attribute. Parameters and buffers need to be registered, or\n",
    "        # they won't appear in .parameters() (doesn't apply to buffers), and\n",
    "        # won't be converted when e.g. .cuda() is called. You can use\n",
    "        # .register_buffer() to register buffers.\n",
    "        # nn.Parameters require gradients by default.\n",
    "        self.weights = nn.Parameter(torch.Tensor(bins).cuda())\n",
    "\n",
    "        # Not a very smart way to initialize weights\n",
    "        self.weights.data.uniform_(0, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        return HistogramFunction.apply(input, self.weights, self.bins)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        # (Optional)Set the extra information about this module. You can test\n",
    "        # it by printing an object of this class.\n",
    "        return 'bins={}'.format(self.bins)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1d Gradcheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(0 ,.,.) = \n",
      "  0  0\n",
      "  0  0\n",
      "  0  1\n",
      "\n",
      "(1 ,.,.) = \n",
      "  0  0\n",
      "  0  0\n",
      "  0  0\n",
      "[torch.cuda.FloatTensor of size 2x3x2 (GPU 0)]\n",
      "\n",
      "\n",
      "(0 ,.,.) = \n",
      "  0  0\n",
      "  0  0\n",
      "  0  1\n",
      "\n",
      "(1 ,.,.) = \n",
      "  0  0\n",
      "  0  0\n",
      "  0  0\n",
      "[torch.cuda.FloatTensor of size 2x3x2 (GPU 0)]\n",
      "\n",
      "\n",
      "(0 ,.,.) = \n",
      "  0  0\n",
      "  0  0\n",
      "  0  0\n",
      "\n",
      "(1 ,.,.) = \n",
      "  0  0\n",
      "  0  0\n",
      "  0  0\n",
      "[torch.cuda.FloatTensor of size 2x3x2 (GPU 0)]\n",
      "\n",
      "\n",
      "(0 ,.,.) = \n",
      "  0  0\n",
      "  0  0\n",
      "  0  0\n",
      "\n",
      "(1 ,.,.) = \n",
      "  0  0\n",
      "  0  0\n",
      "  0  0\n",
      "[torch.cuda.FloatTensor of size 2x3x2 (GPU 0)]\n",
      "\n",
      "\n",
      "(0 ,.,.) = \n",
      "  0.2500  0.2500\n",
      "  0.2500  0.0000\n",
      "  0.2500  0.0000\n",
      "\n",
      "(1 ,.,.) = \n",
      "  0.0000  0.0000\n",
      "  0.0000  0.0000\n",
      "  0.0000  0.0000\n",
      "[torch.cuda.FloatTensor of size 2x3x2 (GPU 0)]\n",
      "\n",
      "\n",
      "(0 ,.,.) = \n",
      "  0.2500  0.2500\n",
      "  0.2500  0.0000\n",
      "  0.2500  0.0000\n",
      "\n",
      "(1 ,.,.) = \n",
      "  0.0000  0.0000\n",
      "  0.0000  0.0000\n",
      "  0.0000  0.0000\n",
      "[torch.cuda.FloatTensor of size 2x3x2 (GPU 0)]\n",
      "\n",
      "\n",
      "(0 ,.,.) = \n",
      "  0  0\n",
      "  0  1\n",
      "  0  0\n",
      "\n",
      "(1 ,.,.) = \n",
      "  0  0\n",
      "  0  0\n",
      "  0  0\n",
      "[torch.cuda.FloatTensor of size 2x3x2 (GPU 0)]\n",
      "\n",
      "\n",
      "(0 ,.,.) = \n",
      "  0  0\n",
      "  0  1\n",
      "  0  0\n",
      "\n",
      "(1 ,.,.) = \n",
      "  0  0\n",
      "  0  0\n",
      "  0  0\n",
      "[torch.cuda.FloatTensor of size 2x3x2 (GPU 0)]\n",
      "\n",
      "\n",
      "(0 ,.,.) = \n",
      "  0  0\n",
      "  0  0\n",
      "  0  0\n",
      "\n",
      "(1 ,.,.) = \n",
      "  0  0\n",
      "  0  1\n",
      "  0  0\n",
      "[torch.cuda.FloatTensor of size 2x3x2 (GPU 0)]\n",
      "\n",
      "\n",
      "(0 ,.,.) = \n",
      "  0  0\n",
      "  0  0\n",
      "  0  0\n",
      "\n",
      "(1 ,.,.) = \n",
      "  0  0\n",
      "  0  1\n",
      "  0  0\n",
      "[torch.cuda.FloatTensor of size 2x3x2 (GPU 0)]\n",
      "\n",
      "\n",
      "(0 ,.,.) = \n",
      "  0.0000  0.0000\n",
      "  0.0000  0.0000\n",
      "  0.0000  0.0000\n",
      "\n",
      "(1 ,.,.) = \n",
      "  0.3333  0.0000\n",
      "  0.0000  0.0000\n",
      "  0.3333  0.3333\n",
      "[torch.cuda.FloatTensor of size 2x3x2 (GPU 0)]\n",
      "\n",
      "\n",
      "(0 ,.,.) = \n",
      "  0.0000  0.0000\n",
      "  0.0000  0.0000\n",
      "  0.0000  0.0000\n",
      "\n",
      "(1 ,.,.) = \n",
      "  0.3333  0.0000\n",
      "  0.0000  0.0000\n",
      "  0.3333  0.3333\n",
      "[torch.cuda.FloatTensor of size 2x3x2 (GPU 0)]\n",
      "\n",
      "\n",
      "(0 ,.,.) = \n",
      "  0  0\n",
      "  0  0\n",
      "  0  0\n",
      "\n",
      "(1 ,.,.) = \n",
      "  0  1\n",
      "  0  0\n",
      "  0  0\n",
      "[torch.cuda.FloatTensor of size 2x3x2 (GPU 0)]\n",
      "\n",
      "\n",
      "(0 ,.,.) = \n",
      "  0  0\n",
      "  0  0\n",
      "  0  0\n",
      "\n",
      "(1 ,.,.) = \n",
      "  0  1\n",
      "  0  0\n",
      "  0  0\n",
      "[torch.cuda.FloatTensor of size 2x3x2 (GPU 0)]\n",
      "\n",
      "\n",
      "(0 ,.,.) = \n",
      "  0  0\n",
      "  0  0\n",
      "  0  0\n",
      "\n",
      "(1 ,.,.) = \n",
      "  0  0\n",
      "  1  0\n",
      "  0  0\n",
      "[torch.cuda.FloatTensor of size 2x3x2 (GPU 0)]\n",
      "\n",
      "\n",
      "(0 ,.,.) = \n",
      "  0  0\n",
      "  0  0\n",
      "  0  0\n",
      "\n",
      "(1 ,.,.) = \n",
      "  0  0\n",
      "  1  0\n",
      "  0  0\n",
      "[torch.cuda.FloatTensor of size 2x3x2 (GPU 0)]\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "for output no. 1,\n numerical:(\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n[torch.FloatTensor of size 12x8]\n, \n 0.9835  0.0000  0.0000  0.0000  0.9835  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  2.9802  0.0000  0.0000\n 0.0000  0.0000  3.9339  0.0000  0.0000  0.0000  0.9835  0.0000\n 0.0000  0.0000  0.0000  0.9835  0.0000  0.0000  0.0000  0.9835\n[torch.FloatTensor of size 4x8]\n)\nanalytical:(\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n[torch.FloatTensor of size 12x8]\n, \n 0.3333  0.0000  0.0000  0.0000  0.3333  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  1.0000  0.0000  0.0000\n 0.0000  0.0000  1.3333  0.0000  0.0000  0.0000  0.3333  0.0000\n 0.0000  0.0000  0.0000  0.3333  0.0000  0.0000  0.0000  0.3333\n[torch.FloatTensor of size 4x8]\n)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-fac960206c45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHistogramFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3/lib/python3.5/site-packages/torch/autograd/gradcheck.py\u001b[0m in \u001b[0;36mgradcheck\u001b[0;34m(func, inputs, eps, atol, rtol, raise_exception)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0matol\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrtol\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfail_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'for output no. %d,\\n numerical:%s\\nanalytical:%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumerical\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalytical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreentrant\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3/lib/python3.5/site-packages/torch/autograd/gradcheck.py\u001b[0m in \u001b[0;36mfail_test\u001b[0;34m(msg)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfail_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: for output no. 1,\n numerical:(\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n[torch.FloatTensor of size 12x8]\n, \n 0.9835  0.0000  0.0000  0.0000  0.9835  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  2.9802  0.0000  0.0000\n 0.0000  0.0000  3.9339  0.0000  0.0000  0.0000  0.9835  0.0000\n 0.0000  0.0000  0.0000  0.9835  0.0000  0.0000  0.0000  0.9835\n[torch.FloatTensor of size 4x8]\n)\nanalytical:(\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0\n[torch.FloatTensor of size 12x8]\n, \n 0.3333  0.0000  0.0000  0.0000  0.3333  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  1.0000  0.0000  0.0000\n 0.0000  0.0000  1.3333  0.0000  0.0000  0.0000  0.3333  0.0000\n 0.0000  0.0000  0.0000  0.3333  0.0000  0.0000  0.0000  0.3333\n[torch.FloatTensor of size 4x8]\n)\n"
     ]
    }
   ],
   "source": [
    "input = Variable(rangeNormalize(torch.randn(2,3,2).cuda().float()),requires_grad=True)\n",
    "\n",
    "bins = 4\n",
    "weights = Variable(torch.randn(bins).cuda().float(),requires_grad=True)\n",
    "weights.data.fill_(1.)\n",
    "res = gradcheck(HistogramFunction.apply, (input, weights, bins), eps=1e-6, atol=1e-4)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram 2d Pytorch Function and Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Histogram2dFunction(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    assume input is already normalized from 0 to 1\n",
    "    assume input size is batch_size, num_points, feature_size\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    PARAM: input, weight, bins\n",
    "    \n",
    "    input: batch_size, num_points, feature_size\n",
    "    weight:  (TBD) size of input OR size of bins\n",
    "    bins: 10 by defaulted\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, bins=10):\n",
    "#         batch_size = input.size()[0]\n",
    "#         feature_size = input.size()[-1]\n",
    "#         histograms = torch.zeros(batch_size, feature_size, bins).cuda()\n",
    "#         histograms_indexes = torch.zeros(input.size()).cuda()\n",
    "        \n",
    "        histograms, histograms_indexes = histogram2d_vectorize(input, bins=bins)\n",
    "        \n",
    "        histograms = histograms.cuda().double()\n",
    "        histograms_values = histograms_indexes.cuda().double()\n",
    "        # hist size: batch, bins, feature; weight size: bins, feature\n",
    "        histograms = histograms*weight\n",
    "        \n",
    "        ctx.save_for_backward(input, weight)\n",
    "        ctx.histograms_indexes = histograms_indexes\n",
    "        ctx.histograms = histograms\n",
    "        return histograms\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, weight= ctx.saved_tensors\n",
    "        histograms_indexes = ctx.histograms_indexes\n",
    "        histograms = ctx.histograms\n",
    "\n",
    "        grad_input = torch.zeros(input.size()).cuda()\n",
    "\n",
    "        \"\"\"\n",
    "        the gradient at batch item i, point a, element b:\n",
    "          grad_output[i, bin_number]*weight[bin_number]/(LEN(histogram[i,bin_number]))\n",
    "        \"\"\"\n",
    "        values = grad_output.data*weight/histograms\n",
    "#         for i in range(input.size()[0]): #batch_size\n",
    "#             grad_input[i] = torch.index_select(values[i], 0, histograms_values[i].long().view(-1)).view(grad_input[i].size())\n",
    "\n",
    "#             for a in range(input.size()[1]): #num_of_points\n",
    "#                 for b in range(input.size()[2]):\n",
    "#                     bin_number = int(histograms_values[i,a,b])\n",
    "                    \n",
    "#                     grad_input[i,a,b] = grad_output.data[i,bin_number]*weight[bin_number]/histograms[i,bin_number] #TODO: divide by number of element in the bin\n",
    "        # values = grad_output\n",
    "        # histogram_indexes = indices\n",
    "        bs, seq, fs, bn = input.size()[0], input.size()[1], input.size()[2], weight.size()[0]\n",
    "        \n",
    "        histograms_indexes = histograms_indexes.transpose(2,1)\n",
    "        # np version\n",
    "#         i2 = torch.from_numpy(np.repeat(np.arange(bs*fs),seq)*bn).view(indices.shape)\n",
    "        # torch version\n",
    "        idx_cat = torch.arange(0, bs*fs*bn, step=bn).cuda().unsqueeze(-1).expand(-1,seq)\n",
    "        idx_cat = idx_cat.view(histograms_indexes.shape).int()\n",
    "        # batch x feature x bins\n",
    "        values = values.transpose(2,1)\n",
    "        histograms_indexes = histograms_indexes + idx_cat\n",
    "\n",
    "        grad_input = torch.index_select(values.contiguous().view(-1), 0, histograms_indexes.long().contiguous().view(-1)).view(histograms_indexes.shape)\n",
    "        \n",
    "        grad_input = grad_input.transpose(2,1)\n",
    "        \n",
    "        grad_weight = (grad_output.data*histograms).sum(0)/bs\n",
    "        \n",
    "        \n",
    "        return Variable(grad_input), Variable(grad_weight), None\n",
    "        \n",
    "\n",
    "class Histogram2d(nn.Module):\n",
    "    def __init__(self, feature_size, bins=10):\n",
    "        super(Histogram2d, self).__init__()\n",
    "        self.bins = bins\n",
    "        self.feature_size = feature_size\n",
    "\n",
    "        # nn.Parameter is a special kind of Variable, that will get\n",
    "        # automatically registered as Module's parameter once it's assigned\n",
    "        # as an attribute. Parameters and buffers need to be registered, or\n",
    "        # they won't appear in .parameters() (doesn't apply to buffers), and\n",
    "        # won't be converted when e.g. .cuda() is called. You can use\n",
    "        # .register_buffer() to register buffers.\n",
    "        # nn.Parameters require gradients by default.\n",
    "        self.weights = nn.Parameter(torch.Tensor(bins, feature_size).cuda())\n",
    "\n",
    "        # Not a very smart way to initialize weights\n",
    "        self.weights.data.uniform_(0, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        return Histogram2dFunction.apply(input, self.weights, self.bins)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        # (Optional)Set the extra information about this module. You can test\n",
    "        # it by printing an object of this class.\n",
    "        return 'bins={}'.format(self.bins)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2d Gradcheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "for output no. 0,\n numerical:(\n    0     0     0  ...      0     0     0\n    0     0     0  ...      0     0     0\n    0     0     0  ...      0     0     0\n       ...          ⋱          ...       \n    0     0     0  ...      0     0     0\n    0     0     0  ...      0     0     0\n    0     0     0  ...      0     0     0\n[torch.FloatTensor of size 150x45]\n, \n\nColumns 0 to 12 \n    3     0     0     0     0     0     0     0     0     0     0     0     0\n    0     1     0     0     0     0     0     0     0     0     0     0     0\n    0     0     3     0     0     0     0     0     0     0     0     0     0\n    0     0     0     4     0     0     0     0     0     0     0     0     0\n    0     0     0     0     1     0     0     0     0     0     0     0     0\n    0     0     0     0     0     6     0     0     0     0     0     0     0\n    0     0     0     0     0     0     7     0     0     0     0     0     0\n    0     0     0     0     0     0     0     7     0     0     0     0     0\n    0     0     0     0     0     0     0     0     5     0     0     0     0\n    0     0     0     0     0     0     0     0     0     9     0     0     0\n    0     0     0     0     0     0     0     0     0     0     1     0     0\n    0     0     0     0     0     0     0     0     0     0     0     2     0\n    0     0     0     0     0     0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0     0     0     0     0     0\n\nColumns 13 to 25 \n    0     0     2     0     0     0     0     0     0     0     0     0     0\n    0     0     0     2     0     0     0     0     0     0     0     0     0\n    0     0     0     0     1     0     0     0     0     0     0     0     0\n    0     0     0     0     0     1     0     0     0     0     0     0     0\n    0     0     0     0     0     0     3     0     0     0     0     0     0\n    0     0     0     0     0     0     0     8     0     0     0     0     0\n    0     0     0     0     0     0     0     0     8     0     0     0     0\n    0     0     0     0     0     0     0     0     0     8     0     0     0\n    0     0     0     0     0     0     0     0     0     0     7     0     0\n    0     0     0     0     0     0     0     0     0     0     0     6     0\n    0     0     0     0     0     0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0     0     0     0     0     0\n    1     0     0     0     0     0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0     0     0     0     0     0\n\nColumns 26 to 38 \n    0     0     0     0     1     0     0     0     0     0     0     0     0\n    0     0     0     0     0     1     0     0     0     0     0     0     0\n    0     0     0     0     0     0     2     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0     1     0     0     0     0\n    0     0     0     0     0     0     0     0     0     9     0     0     0\n    0     0     0     0     0     0     0     0     0     0     6     0     0\n    0     0     0     0     0     0     0     0     0     0     0     7     0\n    0     0     0     0     0     0     0     0     0     0     0     0     9\n    0     0     0     0     0     0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0     0     0     0     0     0\n    0     1     0     0     0     0     0     0     0     0     0     0     0\n    0     0     2     0     0     0     0     0     0     0     0     0     0\n    0     0     0     1     0     0     0     0     0     0     0     0     0\n\nColumns 39 to 44 \n    0     0     0     0     0     0\n    0     0     0     0     0     0\n    0     0     0     0     0     0\n    0     0     0     0     0     0\n    0     0     0     0     0     0\n    0     0     0     0     0     0\n    0     0     0     0     0     0\n    0     0     0     0     0     0\n    0     0     0     0     0     0\n    9     0     0     0     0     0\n    0     0     0     0     0     0\n    0     0     3     0     0     0\n    0     0     0     1     0     0\n    0     0     0     0     1     0\n    0     0     0     0     0     0\n[torch.FloatTensor of size 15x45]\n)\nanalytical:(\n 0.3333  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n          ...             ⋱             ...          \n 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n[torch.FloatTensor of size 150x45]\n, \n\nColumns 0 to 9 \n 1.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.3333  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  1.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  1.3333  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.3333  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  2.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  2.3333  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  2.3333  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  1.6667  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  3.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n\nColumns 10 to 19 \n 0.0000  0.0000  0.0000  0.0000  0.0000  0.6667  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.6667  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.3333  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.3333  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  1.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.3333  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.6667  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.3333  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n\nColumns 20 to 29 \n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 2.6667  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  2.6667  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  2.6667  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  2.3333  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  2.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.3333  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.6667  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.3333\n\nColumns 30 to 39 \n 0.3333  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.3333  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.6667  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.3333  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  3.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  2.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  2.3333  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  3.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  3.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n\nColumns 40 to 44 \n 0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  1.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.3333  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.3333  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000\n[torch.FloatTensor of size 15x45]\n)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-cdc4bf53e726>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHistogram2dFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3/lib/python3.5/site-packages/torch/autograd/gradcheck.py\u001b[0m in \u001b[0;36mgradcheck\u001b[0;34m(func, inputs, eps, atol, rtol, raise_exception)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0matol\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrtol\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfail_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'for output no. %d,\\n numerical:%s\\nanalytical:%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumerical\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalytical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreentrant\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3/lib/python3.5/site-packages/torch/autograd/gradcheck.py\u001b[0m in \u001b[0;36mfail_test\u001b[0;34m(msg)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfail_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: for output no. 0,\n numerical:(\n    0     0     0  ...      0     0     0\n    0     0     0  ...      0     0     0\n    0     0     0  ...      0     0     0\n       ...          ⋱          ...       \n    0     0     0  ...      0     0     0\n    0     0     0  ...      0     0     0\n    0     0     0  ...      0     0     0\n[torch.FloatTensor of size 150x45]\n, \n\nColumns 0 to 12 \n    3     0     0     0     0     0     0     0     0     0     0     0     0\n    0     1     0     0     0     0     0     0     0     0     0     0     0\n    0     0     3     0     0     0     0     0     0     0     0     0     0\n    0     0     0     4     0     0     0     0     0     0     0     0     0\n    0     0     0     0     1     0     0     0     0     0     0     0     0\n    0     0     0     0     0     6     0     0     0     0     0     0     0\n    0     0     0     0     0     0     7     0     0     0     0     0     0\n    0     0     0     0     0     0     0     7     0     0     0     0     0\n    0     0     0     0     0     0     0     0     5     0     0     0     0\n    0     0     0     0     0     0     0     0     0     9     0     0     0\n    0     0     0     0     0     0     0     0     0     0     1     0     0\n    0     0     0     0     0     0     0     0     0     0     0     2     0\n    0     0     0     0     0     0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0     0     0     0     0     0\n\nColumns 13 to 25 \n    0     0     2     0     0     0     0     0     0     0     0     0     0\n    0     0     0     2     0     0     0     0     0     0     0     0     0\n    0     0     0     0     1     0     0     0     0     0     0     0     0\n    0     0     0     0     0     1     0     0     0     0     0     0     0\n    0     0     0     0     0     0     3     0     0     0     0     0     0\n    0     0     0     0     0     0     0     8     0     0     0     0     0\n    0     0     0     0     0     0     0     0     8     0     0     0     0\n    0     0     0     0     0     0     0     0     0     8     0     0     0\n    0     0     0     0     0     0     0     0     0     0     7     0     0\n    0     0     0     0     0     0     0     0     0     0     0     6     0\n    0     0     0     0     0     0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0     0     0     0     0     0\n    1     0     0     0     0     0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0     0     0     0     0     0\n\nColumns 26 to 38 \n    0     0     0     0     1     0     0     0     0     0     0     0     0\n    0     0     0     0     0     1     0     0     0     0     0     0     0\n    0     0     0     0     0     0     2     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0     1     0     0     0     0\n    0     0     0     0     0     0     0     0     0     9     0     0     0\n    0     0     0     0     0     0     0     0     0     0     6     0     0\n    0     0     0     0     0     0     0     0     0     0     0     7     0\n    0     0     0     0     0     0     0     0     0     0     0     0     9\n    0     0     0     0     0     0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0     0     0     0     0     0\n    0     0     0     0     0     0     0     0     0     0     0     0     0\n    0     1     0     0     0     0     0     0     0     0     0     0     0\n    0     0     2     0     0     0     0     0     0     0     0     0     0\n    0     0     0     1     0     0     0     0     0     0     0     0     0\n\nColumns 39 to 44 \n    0     0     0     0     0     0\n    0     0     0     0     0     0\n    0     0     0     0     0     0\n    0     0     0     0     0     0\n    0     0     0     0     0     0\n    0     0     0     0     0     0\n    0     0     0     0     0     0\n    0     0     0     0     0     0\n    0     0     0     0     0     0\n    9     0     0     0     0     0\n    0     0     0     0     0     0\n    0     0     3     0     0     0\n    0     0     0     1     0     0\n    0     0     0     0     1     0\n    0     0     0     0     0     0\n[torch.FloatTensor of size 15x45]\n)\nanalytical:(\n 0.3333  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n          ...             ⋱             ...          \n 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n[torch.FloatTensor of size 150x45]\n, \n\nColumns 0 to 9 \n 1.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.3333  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  1.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  1.3333  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.3333  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  2.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  2.3333  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  2.3333  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  1.6667  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  3.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n\nColumns 10 to 19 \n 0.0000  0.0000  0.0000  0.0000  0.0000  0.6667  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.6667  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.3333  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.3333  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  1.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.3333  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.6667  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.3333  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n\nColumns 20 to 29 \n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 2.6667  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  2.6667  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  2.6667  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  2.3333  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  2.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.3333  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.6667  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.3333\n\nColumns 30 to 39 \n 0.3333  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.3333  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.6667  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.3333  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  3.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  2.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  2.3333  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  3.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  3.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n\nColumns 40 to 44 \n 0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  1.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.3333  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.3333  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000\n[torch.FloatTensor of size 15x45]\n)\n"
     ]
    }
   ],
   "source": [
    "input = Variable(rangeNormalize(torch.randn(3,10,5).cuda().double()),requires_grad=True)\n",
    "\n",
    "bins = 3\n",
    "weights = nn.Parameter(torch.randn(bins, 5).cuda().double())\n",
    "weights.data.fill_(1.)\n",
    "res = gradcheck(Histogram2dFunction.apply, (input, weights, bins), eps=1e-6, atol=1e-4)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myNet(nn.Module):\n",
    "    def __init__(self, in_features, H, bins=10):\n",
    "        super(myNet, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv1d(3, 3, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(3)\n",
    "        self.bins = bins\n",
    "        self.his = Histogram(bins=bins)\n",
    "        self.fc1 = nn.Linear(bins, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(2,1)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        x = x.transpose(2,1)\n",
    "        x = rangeNormalize(x)\n",
    "        x = self.his(x)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "class myNet2d(nn.Module):\n",
    "    def __init__(self, in_features, H, bins=10):\n",
    "        super(myNet2d, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv1d(3, 5, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(5)\n",
    "        self.bins = bins\n",
    "        self.his = Histogram2d(5, bins=bins)\n",
    "        self.fc1 = nn.Linear(bins*5, 2)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(2,1)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        x = x.transpose(2,1)\n",
    "        x = rangeNormalize(x)\n",
    "        x = self.his(x)\n",
    "#         print(x.size())\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running network with dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t = time.time()\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "# dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, num_points, label_size, input_dim, num_bins= 32, 3, 2, 3, 3\n",
    "\n",
    "# Create random Tensors to hold input and outputs, and wrap them in Variables.\n",
    "x = Variable(torch.randn(N, num_points, input_dim).type(dtype), requires_grad=False)\n",
    "y = Variable(torch.randn(N, label_size).type(dtype).cuda(), requires_grad=False)\n",
    "\n",
    "# Create random Tensors for weights, and wrap them in Variables.\n",
    "# w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True)\n",
    "# w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)\n",
    "\n",
    "model = myNet2d(num_points, input_dim, bins=num_bins).cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 1e-6, momentum=0.8)\n",
    "\n",
    "\n",
    "for t in range(500):\n",
    "    # To apply our Function, we use Function.apply method. We alias this as 'relu'.\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass: compute predicted y using operations on Variables; we compute\n",
    "    # ReLU using our custom autograd operation.\n",
    "    \n",
    "    y_pred = model(x.cuda())\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.data[0])\n",
    "    if(loss.data[0]==float('inf')):\n",
    "        break\n",
    "\n",
    "    # Use autograd to compute the backward pass.\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "\n",
    "print(time.time() - t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
