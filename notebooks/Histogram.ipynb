{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "from torchsample.transforms import RangeNormalize\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rangeNormalize(data):\n",
    "    return 0.99*(data-torch.min(data))/(torch.max(data)-torch.min(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.71617081  0.98976674  0.73646462]\n",
      "  [ 0.01718258  0.46821872  0.81540721]\n",
      "  [ 0.55790953  0.          0.82096578]\n",
      "  [ 0.28922225  0.27323034  0.99      ]\n",
      "  [ 0.70544068  0.50970571  0.06189024]]\n",
      "\n",
      " [[ 0.44434247  0.89019627  0.3302703 ]\n",
      "  [ 0.68024678  0.52657746  0.67441619]\n",
      "  [ 0.92950561  0.12590988  0.29057328]\n",
      "  [ 0.98422319  0.92039091  0.62042439]\n",
      "  [ 0.45922686  0.32178543  0.77193556]]\n",
      "\n",
      " [[ 0.28701469  0.56075156  0.81797657]\n",
      "  [ 0.16574765  0.43410609  0.55908609]\n",
      "  [ 0.87702813  0.00962035  0.40635735]\n",
      "  [ 0.55832478  0.66140762  0.41252884]\n",
      "  [ 0.86176751  0.77467071  0.07867537]]]\n"
     ]
    }
   ],
   "source": [
    "# data size: batch x num_points x feature\n",
    "nbins = 4\n",
    "batch_size = 3\n",
    "num_points = 5\n",
    "feature_size = 3\n",
    "test_data = np.random.random_sample((batch_size, num_points, feature_size))\n",
    "\n",
    "test_data = rangeNormalize(torch.from_numpy(test_data)).numpy()\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram 1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram(data, bins, range=(0,1)):\n",
    "    \"\"\"\n",
    "    find the histogram of given data\n",
    "    Current range, (0,1), others haven't been tested\n",
    "    \n",
    "    return histogram, TODO: ret_index, ret_value\n",
    "    \n",
    "    \"\"\"\n",
    "    ret_histogram = torch.zeros(bins)\n",
    "    ret_indexes = torch.zeros(data.size())\n",
    "    for idx, d in enumerate(data.view(data.numel())):\n",
    "        if d == 1.0:\n",
    "            bin_number = bins - 1\n",
    "        else:\n",
    "            bin_number = int(bins*d/(range[1]-range[0]))\n",
    "        if bin_number == bins:\n",
    "            bin_number = bin_number - 1\n",
    "        \n",
    "        ret_histogram[bin_number] += 1\n",
    "        if len(data.size()) == 1:\n",
    "            ret_indexes[idx] = bin_number\n",
    "        else:\n",
    "            idx_a = int(idx/data.size()[-1])\n",
    "            idx_b = idx%data.size()[-1]\n",
    "            ret_indexes[idx_a, idx_b] = bin_number\n",
    "    \n",
    "    return ret_histogram, ret_indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram1d Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       "  3  3  5  4\n",
       "  1  5  4  5\n",
       "  3  4  4  4\n",
       " [torch.FloatTensor of size 3x4], \n",
       " (0 ,.,.) = \n",
       "   2  3  2\n",
       "   0  1  3\n",
       "   2  0  3\n",
       "   1  1  3\n",
       "   2  2  0\n",
       " \n",
       " (1 ,.,.) = \n",
       "   1  3  1\n",
       "   2  2  2\n",
       "   3  0  1\n",
       "   3  3  2\n",
       "   1  1  3\n",
       " \n",
       " (2 ,.,.) = \n",
       "   1  2  3\n",
       "   0  1  2\n",
       "   3  0  1\n",
       "   2  2  1\n",
       "   3  3  0\n",
       " [torch.IntTensor of size 3x5x3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def histogram_vectorize(data, bins, range=(0,1)):\n",
    "    \"\"\"\n",
    "    find the histogram of given data: batch x seq x feature\n",
    "    Current range, (0,1), others haven't been tested\n",
    "    \n",
    "    return histogram, TODO: ret_index, ret_value\n",
    "    \n",
    "    \"\"\"\n",
    "    ret_histogram = torch.zeros(bins)\n",
    "    ret_indexes = torch.zeros(data.size())\n",
    "    \n",
    "    # calculate the the corresponding bin number \n",
    "    temp = (data*bins).int()\n",
    "    \n",
    "    # transform into one-hot vector and then sum\n",
    "    y_tensor = temp\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(data.size()[0],-1, 1)\n",
    "    # given a list of numbers, we transform each number {i} to one_hot vector(nbins x 1) where index i is 1\n",
    "    # scatter(dim, index, val)\n",
    "    y_one_hot = torch.zeros(data.size()[0],y_tensor.size()[1], bins).scatter_(2, y_tensor, 1)\n",
    "    \n",
    "    ret_indexes = temp.view(data.size())\n",
    "    ret_histogram = y_one_hot.sum(1)\n",
    "\n",
    "    return ret_histogram, ret_indexes\n",
    "histogram_vectorize(torch.from_numpy(test_data), nbins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing output (with numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# mirror output\n",
    "\n",
    "\n",
    "def np_solution(x,nbins):\n",
    "    histograms = np.zeros((batch_size, nbins))\n",
    "    for i in range(batch_size):\n",
    "        # assert sum is the same as the count\n",
    "        histograms[i,:] = np.histogram(x[i], bins=nbins, range=(0,1))[0]\n",
    "    return histograms\n",
    "\n",
    "def my_histogram(x,nbins):\n",
    "    histograms = torch.zeros((batch_size, nbins))\n",
    "    histograms_indexes = torch.zeros(x.shape)\n",
    "    for i in range(batch_size):\n",
    "        histograms[i,:], histograms_indexes[i,:] = histogram(torch.from_numpy(x[i]), bins=nbins)\n",
    "    return histograms, histograms_indexes\n",
    "\n",
    "def my_histogram_vec(x,nbins):\n",
    "    my_histogram_vec, my_indexes_vec = histogram_vectorize(torch.from_numpy(x), bins=nbins, range=(0,1))\n",
    "    return my_histogram_vec, my_indexes_vec\n",
    "\n",
    "np_hist = np_solution(test_data,nbins)\n",
    "my_hist, my_idx = my_histogram(test_data,nbins)\n",
    "my_hist_v, my_idx_vec = my_histogram_vec(test_data,nbins)\n",
    "\n",
    "assert(np.all(np_hist==my_hist))\n",
    "assert(np.all(np_hist==my_hist_v))\n",
    "assert(np.all(my_idx.float() == my_idx_vec.float()))\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np solution time\n",
      "0.0007479190826416016\n",
      "my hist time\n",
      "0.0010182857513427734\n",
      "my hist vec time\n",
      "0.00031113624572753906\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(\"np solution time\")\n",
    "t = time.time()\n",
    "np_hist = np_solution(test_data, nbins)\n",
    "print(time.time() - t)\n",
    "\n",
    "print(\"my hist time\")\n",
    "t = time.time()\n",
    "my_hist = my_histogram(test_data, nbins)\n",
    "print(time.time() - t)\n",
    "\n",
    "print(\"my hist vec time\")\n",
    "t = time.time()\n",
    "my_hist_v = my_histogram_vec(test_data, nbins)\n",
    "print(time.time() - t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram 2d Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram2d_vectorize(data, bins, range=(0,1)):\n",
    "    \"\"\"\n",
    "    find the histogram of given data: batch x seq x feature\n",
    "    Current range, (0,1), others haven't been tested\n",
    "    \n",
    "    return histogram along the second column. eg : batch x bins x feature\n",
    "    \n",
    "    \"\"\"\n",
    "    ret_histogram = torch.zeros(bins)\n",
    "    ret_indexes = torch.zeros(data.size())\n",
    "    \n",
    "    # calculate the the corresponding bin number \n",
    "    temp = (data*bins).int()\n",
    "    temp = temp.transpose(2,1)\n",
    "    # transform into one-hot vector and then sum\n",
    "    y_tensor = temp\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(data.size()[0], data.size()[-1],-1, 1)\n",
    "\n",
    "    # given a list of numbers, we transform each number {i} to one_hot vector(nbins x 1) where index i is 1\n",
    "    # scatter(dim, index, val)\n",
    "    y_one_hot = torch.zeros(data.size()[0], data.size()[-1], data.size()[1], bins).scatter_(3, y_tensor.long(), 1)\n",
    "\n",
    "    ret_indexes = temp.transpose(2,1)\n",
    "    ret_histogram = y_one_hot.sum(2).transpose(2,1)\n",
    "\n",
    "    return ret_histogram, ret_indexes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_solution2d(x, nbins):\n",
    "    histograms2d = np.zeros((batch_size, nbins, feature_size))\n",
    "    for i in range(batch_size):\n",
    "        for j in range(feature_size):\n",
    "            histograms2d[i,:,j] = np.histogram(x[i,:,j], bins=nbins, range=(0,1))[0]\n",
    "    return histograms2d\n",
    "\n",
    "def my_histogram2d(x, nbins):\n",
    "    histograms2d = torch.zeros((batch_size, nbins, feature_size))\n",
    "    histograms2d_idx = torch.zeros(x.shape)\n",
    "    for i in range(batch_size):\n",
    "        for j in range(feature_size):\n",
    "            histograms2d[i,:,j],histograms2d_idx[i,:,j] = histogram(x[i,:,j],bins=nbins)\n",
    "            \n",
    "    return histograms2d, histograms2d_idx\n",
    "def my_histogram2d_vec(x, nbins):\n",
    "    histogram2d, histogram2d_indexes = histogram2d_vectorize(x, nbins)\n",
    "    return histogram2d, histogram2d_indexes\n",
    "\n",
    "np_hist2d = np_solution2d(test_data, nbins)\n",
    "my_hist2d, my_hist2d_idx = my_histogram2d(torch.from_numpy(test_data), nbins)\n",
    "my_hist2d_vec, my_hist2d_vec_idx = my_histogram2d_vec(torch.from_numpy(test_data), nbins)\n",
    "\n",
    "assert(np.all(np_hist2d==my_hist2d))\n",
    "assert(np.all(np_hist2d==my_hist2d_vec))\n",
    "assert(np.all(my_hist2d_idx.float()==my_hist2d_vec_idx.float()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Vectorizing Backward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# shows pytorch can boardcast\n",
    "bs, bn = 5, 6\n",
    "idx = torch.zeros(bs,bn).cuda()+0.1\n",
    "w = torch.ones(bn).cuda()\n",
    "hist = torch.zeros(bs,bn).cuda()+2\n",
    "# print(I*y/z)\n",
    "# values = grad_output.data*weight/histograms\n",
    "v = idx*w/hist\n",
    "for i in range(bs):\n",
    "    for j in range(bn):\n",
    "        print(v[i,j]==(idx[i,j]*w[j]/hist[i,j]))\n",
    "\n",
    "# for i in range(input.size()[0]): #batch_size\n",
    "#     grad_input[i] = torch.index_select(values[i], 0, histograms_indexes[i].long().view(-1)).view(grad_input[i].size())\n",
    "#     for a in range(input.size()[1]): #num_of_points\n",
    "#         for b in range(input.size()[2]):\n",
    "#             bin_number = int(histograms_indexes[i,a,b])\n",
    "\n",
    "#             grad_input3[i,a,b] = grad_output.data[i,bin_number]*weight[bin_number]/histograms[i,bin_number] #TODO: divide by number of element in the bin\n",
    "#             print(values[i,bin_number]==grad_input3[i,a,b])\n",
    "        \n",
    "\n",
    "# indices: same size as output; the corresponding index value in x \n",
    "# x : the values to be assign\n",
    "# index_select: given x (value to be drawn from), dim, vector of indexes\n",
    "#               return a vector of values( x[indices[i] for i in indexes])\n",
    "\n",
    "# 1d case\n",
    "bs, seq, fs, bn = 3, 5, 4, 4\n",
    "indices = torch.LongTensor(bs, seq, fs).random_(0,bn)\n",
    "indices_orig = indices.clone()\n",
    "# np equivalent\n",
    "i3 = torch.from_numpy(np.repeat(np.arange(bs),seq*fs)*bn).view(indices.shape)\n",
    "# torch version\n",
    "i2 = torch.arange(0, bs*bn, step=bn).unsqueeze(-1).expand(-1,seq*fs)\n",
    "\n",
    "# print(i3)#.repeat(seq*fs)*bn\n",
    "i2 = i2.view(indices.shape).long()\n",
    "\n",
    "indices = indices + i2\n",
    "grad_output = torch.randn(bs,bn)*10\n",
    "grad_output = grad_output.int()\n",
    "\n",
    "ans = torch.index_select(grad_output.view(-1), 0, indices.view(-1)).view(bs,seq,fs)\n",
    "\n",
    "for i in range(bs):\n",
    "    for x in range(fs):\n",
    "        for y in range(seq):\n",
    "            bin_number = indices_orig[i,y,x]\n",
    "\n",
    "            assert(ans[i,y,x] == grad_output[i,bin_number])\n",
    "            \n",
    "\n",
    "# 2d case\n",
    "# batch x feature x seq\n",
    "bs, seq, fs, bn = 3,5,3,4\n",
    "indices = torch.LongTensor(bs, seq, fs).random_(0,bn)\n",
    "indices_orig = indices.clone()\n",
    "\n",
    "indices = indices.transpose(2,1)\n",
    "# np version\n",
    "i2 = torch.from_numpy(np.repeat(np.arange(bs*fs),seq)*bn).view(indices.shape)\n",
    "# torch version\n",
    "i2 = torch.arange(0, bs*fs*bn, step=bn).unsqueeze(-1).expand(-1,seq)\n",
    "i2 = i2.view(indices.shape).long()\n",
    "# batch x feature x bins\n",
    "grad_output = torch.randn(bs,bn,fs).transpose(2,1)*10\n",
    "grad_output = grad_output.int()\n",
    "weight = (torch.randn(bn,fs)*10).int()\n",
    "indices = indices + i2\n",
    "\n",
    "ans = torch.index_select(grad_output.view(-1), 0, indices.view(-1)).view(indices.shape)\n",
    "\n",
    "grad_output = grad_output.transpose(2,1)\n",
    "ans = ans.transpose(2,1)\n",
    "\n",
    "\n",
    "for i in range(bs):\n",
    "    for x in range(fs):\n",
    "        for y in range(seq):\n",
    "            bin_number = indices_orig[i,y,x]\n",
    "            assert(ans[i,y,x]==grad_output[i,bin_number,x])\n",
    "            \n",
    "\n",
    "# psudo code for grad output\n",
    "# for i in range(batch_size):\n",
    "#     for x in range(feature_size):\n",
    "#         for y in range(seq):\n",
    "#             bin_number = hist_index[i,y,x]\n",
    "#             grad_input[i,y,x] = grad_output[i,bin_number,x]*weight[bin_number,x]/histogram2d[i,bin_number,x] \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myNet(nn.Module):\n",
    "    def __init__(self, in_features, H, bins=10):\n",
    "        super(myNet, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv1d(3, 3, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(3)\n",
    "        self.bins = bins\n",
    "        self.his = Histogram(bins=bins)\n",
    "        self.fc1 = nn.Linear(bins, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(2,1)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        x = x.transpose(2,1)\n",
    "        x = rangeNormalize(x)\n",
    "        x = self.his(x)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "class myNet2d(nn.Module):\n",
    "    def __init__(self, in_features, H, bins=10):\n",
    "        super(myNet2d, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv1d(3, 5, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(5)\n",
    "        self.bins = bins\n",
    "        self.his = Histogram2d(5, bins=bins)\n",
    "        self.fc1 = nn.Linear(bins*5, 2)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(2,1)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        x = x.transpose(2,1)\n",
    "        x = rangeNormalize(x)\n",
    "        x = self.his(x)\n",
    "#         print(x.size())\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistogramFunction(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    assume input is already normalized from 0 to 1\n",
    "    assume input size is batch_size, num_points, feature_size\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    PARAM: input, weight, bins\n",
    "    \n",
    "    input: batch_size, num_points, feature_size\n",
    "    weight:  (TBD) size of input OR size of bins\n",
    "    bins: 10 by defaulted\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, bins=10):\n",
    "        batch_size = input.size()[0]\n",
    "        histograms = torch.zeros(batch_size, bins).cuda()\n",
    "        histograms_indexes = torch.zeros(input.size()).cuda()\n",
    "        \n",
    "        histograms, histograms_indexes = histogram_vectorize(input, bins=bins)\n",
    "        histograms = histograms.cuda()\n",
    "        histograms_indexes = histograms_indexes.cuda()\n",
    "        histograms = histograms* weight\n",
    "        \n",
    "        #non-vec version\n",
    "#         for i in range(batch_size):\n",
    "#             # assert sum is the same as the count\n",
    "#             histograms[i,:], histograms_values[i,:] = histogram(input[i], bins=bins)\n",
    "#             histograms[i,:] = histograms[i,:] * weight\n",
    "        \n",
    "        ctx.save_for_backward(input, weight)\n",
    "        ctx.histograms_indexes = histograms_indexes\n",
    "        ctx.histograms = histograms\n",
    "        return histograms\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, weight= ctx.saved_tensors\n",
    "        histograms_indexes = ctx.histograms_indexes\n",
    "        histograms = ctx.histograms\n",
    "        grad_input = torch.zeros(input.size()).cuda()\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        the gradient at batch item i, point a, element b:\n",
    "          grad_output[i, bin_number]*weight[bin_number]/(histogram[i,bin_number])\n",
    "        \"\"\"\n",
    "        values = grad_output.data*weight/histograms\n",
    "#         print(histograms.size())\n",
    "#         print(grad_output.data.size())\n",
    "        \n",
    "#         for i in range(input.size()[0]): #batch_size\n",
    "#             grad_input[i] = torch.index_select(values[i], 0, histograms_indexes[i].long().view(-1)).view(grad_input[i].size())\n",
    "#             for a in range(input.size()[1]): #num_of_points\n",
    "#                 for b in range(input.size()[2]):\n",
    "#                     bin_number = int(histograms_indexes[i,a,b])\n",
    "#                     grad_input3[i,a,b] = grad_output.data[i,bin_number]*weight[bin_number]/histograms[i,bin_number]\n",
    "#                     if (values[i,bin_number] - grad_output.data[i,bin_number]*weight[bin_number]/histograms[i,bin_number])>1e-5:\n",
    "#                         print(\"HI\")\n",
    "#             assert(np.all(grad_input[i]-grad_input3[i]<=1e-4))\n",
    "                    \n",
    "        # values is the grad_output\n",
    "        # histograms_values are the indices\n",
    "        bs, seq, fs, bn = input.size()[0], input.size()[1], input.size()[2], weight.size()[0]\n",
    "        # np equivalent\n",
    "#         idx_cat = torch.from_numpy(np.repeat(np.arange(bs),seq*fs)*bn).view(indices.shape)\n",
    "        # torch version\n",
    "        idx_cat = torch.arange(0, bs*bn, step=bn).cuda().unsqueeze(-1).expand(-1,seq*fs)\n",
    "\n",
    "        idx_cat = idx_cat.view(histograms_indexes.shape).int()\n",
    "\n",
    "        histograms_indexes = histograms_indexes + idx_cat\n",
    "\n",
    "        grad_input = torch.index_select(values.view(-1), 0, histograms_indexes.long().view(-1)).view(bs,seq,fs)\n",
    "        \n",
    "        \n",
    "        grad_weight = (grad_output.data*histograms).sum(0)/bs\n",
    "        \n",
    "    \n",
    "        \n",
    "        return Variable(grad_input), Variable(grad_weight), None\n",
    "        \n",
    "\n",
    "class Histogram(nn.Module):\n",
    "    def __init__(self, bins=10):\n",
    "        super(Histogram, self).__init__()\n",
    "        self.bins = bins\n",
    "\n",
    "        # nn.Parameter is a special kind of Variable, that will get\n",
    "        # automatically registered as Module's parameter once it's assigned\n",
    "        # as an attribute. Parameters and buffers need to be registered, or\n",
    "        # they won't appear in .parameters() (doesn't apply to buffers), and\n",
    "        # won't be converted when e.g. .cuda() is called. You can use\n",
    "        # .register_buffer() to register buffers.\n",
    "        # nn.Parameters require gradients by default.\n",
    "        self.weights = nn.Parameter(torch.Tensor(bins).cuda())\n",
    "\n",
    "        # Not a very smart way to initialize weights\n",
    "        self.weights.data.uniform_(0, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        return HistogramFunction.apply(input, self.weights, self.bins)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        # (Optional)Set the extra information about this module. You can test\n",
    "        # it by printing an object of this class.\n",
    "        return 'bins={}'.format(self.bins)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Histogram2dFunction(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    assume input is already normalized from 0 to 1\n",
    "    assume input size is batch_size, num_points, feature_size\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    PARAM: input, weight, bins\n",
    "    \n",
    "    input: batch_size, num_points, feature_size\n",
    "    weight:  (TBD) size of input OR size of bins\n",
    "    bins: 10 by defaulted\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, bins=10):\n",
    "#         batch_size = input.size()[0]\n",
    "#         feature_size = input.size()[-1]\n",
    "#         histograms = torch.zeros(batch_size, feature_size, bins).cuda()\n",
    "#         histograms_indexes = torch.zeros(input.size()).cuda()\n",
    "        \n",
    "        histograms, histograms_indexes = histogram2d_vectorize(input, bins=bins)\n",
    "        \n",
    "        histograms = histograms.cuda()\n",
    "        histograms_values = histograms_indexes.cuda()\n",
    "        # hist size: batch, bins, feature; weight size: bins, feature\n",
    "        histograms = histograms*weight\n",
    "        \n",
    "        ctx.save_for_backward(input, weight)\n",
    "        ctx.histograms_indexes = histograms_indexes\n",
    "        ctx.histograms = histograms\n",
    "        return histograms\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, weight= ctx.saved_tensors\n",
    "        histograms_indexes = ctx.histograms_indexes\n",
    "        histograms = ctx.histograms\n",
    "\n",
    "        grad_input = torch.zeros(input.size()).cuda()\n",
    "\n",
    "        \"\"\"\n",
    "        the gradient at batch item i, point a, element b:\n",
    "          grad_output[i, bin_number]*weight[bin_number]/(LEN(histogram[i,bin_number]))\n",
    "        \"\"\"\n",
    "        values = grad_output.data*weight/histograms\n",
    "#         for i in range(input.size()[0]): #batch_size\n",
    "#             grad_input[i] = torch.index_select(values[i], 0, histograms_values[i].long().view(-1)).view(grad_input[i].size())\n",
    "\n",
    "#             for a in range(input.size()[1]): #num_of_points\n",
    "#                 for b in range(input.size()[2]):\n",
    "#                     bin_number = int(histograms_values[i,a,b])\n",
    "                    \n",
    "#                     grad_input[i,a,b] = grad_output.data[i,bin_number]*weight[bin_number]/histograms[i,bin_number] #TODO: divide by number of element in the bin\n",
    "        # values = grad_output\n",
    "        # histogram_indexes = indices\n",
    "        bs, seq, fs, bn = input.size()[0], input.size()[1], input.size()[2], weight.size()[0]\n",
    "        \n",
    "        histograms_indexes = histograms_indexes.transpose(2,1)\n",
    "        # np version\n",
    "#         i2 = torch.from_numpy(np.repeat(np.arange(bs*fs),seq)*bn).view(indices.shape)\n",
    "        # torch version\n",
    "        idx_cat = torch.arange(0, bs*fs*bn, step=bn).cuda().unsqueeze(-1).expand(-1,seq)\n",
    "        idx_cat = idx_cat.view(histograms_indexes.shape).int()\n",
    "        # batch x feature x bins\n",
    "        values = values.transpose(2,1)\n",
    "        histograms_indexes = histograms_indexes + idx_cat\n",
    "\n",
    "        grad_input = torch.index_select(values.contiguous().view(-1), 0, histograms_indexes.long().contiguous().view(-1)).view(histograms_indexes.shape)\n",
    "        \n",
    "        grad_input = grad_input.transpose(2,1)\n",
    "        \n",
    "        grad_weight = (grad_output.data*histograms).sum(0)/bs\n",
    "        \n",
    "        \n",
    "        return Variable(grad_input), Variable(grad_weight), None\n",
    "        \n",
    "\n",
    "class Histogram2d(nn.Module):\n",
    "    def __init__(self, feature_size, bins=10):\n",
    "        super(Histogram2d, self).__init__()\n",
    "        self.bins = bins\n",
    "        self.feature_size = feature_size\n",
    "\n",
    "        # nn.Parameter is a special kind of Variable, that will get\n",
    "        # automatically registered as Module's parameter once it's assigned\n",
    "        # as an attribute. Parameters and buffers need to be registered, or\n",
    "        # they won't appear in .parameters() (doesn't apply to buffers), and\n",
    "        # won't be converted when e.g. .cuda() is called. You can use\n",
    "        # .register_buffer() to register buffers.\n",
    "        # nn.Parameters require gradients by default.\n",
    "        self.weights = nn.Parameter(torch.Tensor(bins, feature_size).cuda())\n",
    "\n",
    "        # Not a very smart way to initialize weights\n",
    "        self.weights.data.uniform_(0, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        return Histogram2dFunction.apply(input, self.weights, self.bins)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        # (Optional)Set the extra information about this module. You can test\n",
    "        # it by printing an object of this class.\n",
    "        return 'bins={}'.format(self.bins)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 55.40248489379883\n",
      "1 55.39984893798828\n",
      "2 55.39511489868164\n",
      "3 55.38871383666992\n",
      "4 55.38096237182617\n",
      "5 55.37217712402344\n",
      "6 55.3625602722168\n",
      "7 55.35227584838867\n",
      "8 55.34148406982422\n",
      "9 55.330291748046875\n",
      "10 55.318790435791016\n",
      "11 55.30706024169922\n",
      "12 55.29515838623047\n",
      "13 55.28312683105469\n",
      "14 55.27100372314453\n",
      "15 55.25882339477539\n",
      "16 55.24660110473633\n",
      "17 55.234352111816406\n",
      "18 55.22211456298828\n",
      "19 55.20988464355469\n",
      "20 55.197669982910156\n",
      "21 55.185489654541016\n",
      "22 55.17332458496094\n",
      "23 55.16120910644531\n",
      "24 55.149147033691406\n",
      "25 55.137107849121094\n",
      "26 55.125118255615234\n",
      "27 55.113197326660156\n",
      "28 55.10130310058594\n",
      "29 55.08946228027344\n",
      "30 55.077674865722656\n",
      "31 55.06594467163086\n",
      "32 55.05427551269531\n",
      "33 55.042640686035156\n",
      "34 55.03105926513672\n",
      "35 55.01954650878906\n",
      "36 55.00807189941406\n",
      "37 54.99666213989258\n",
      "38 54.985294342041016\n",
      "39 54.97399139404297\n",
      "40 54.96272659301758\n",
      "41 54.951534271240234\n",
      "42 54.94037628173828\n",
      "43 54.92927551269531\n",
      "44 54.91823196411133\n",
      "45 54.90723419189453\n",
      "46 54.896278381347656\n",
      "47 54.88539505004883\n",
      "48 54.87455749511719\n",
      "49 54.86376953125\n",
      "50 54.853023529052734\n",
      "51 54.84232711791992\n",
      "52 54.83169174194336\n",
      "53 54.82110595703125\n",
      "54 54.81056594848633\n",
      "55 54.80008316040039\n",
      "56 54.78963088989258\n",
      "57 54.77923583984375\n",
      "58 54.73561477661133\n",
      "59 54.72533416748047\n",
      "60 54.7150993347168\n",
      "61 54.704925537109375\n",
      "62 54.69478988647461\n",
      "63 54.6846923828125\n",
      "64 54.67466735839844\n",
      "65 54.6646728515625\n",
      "66 54.65472412109375\n",
      "67 54.64482498168945\n",
      "68 54.63497543334961\n",
      "69 54.62516784667969\n",
      "70 54.61540985107422\n",
      "71 54.60569381713867\n",
      "72 54.59602355957031\n",
      "73 54.586395263671875\n",
      "74 54.576820373535156\n",
      "75 54.567283630371094\n",
      "76 54.55780029296875\n",
      "77 54.54833984375\n",
      "78 54.53894805908203\n",
      "79 54.52958679199219\n",
      "80 54.520263671875\n",
      "81 54.51099395751953\n",
      "82 54.501766204833984\n",
      "83 54.49257278442383\n",
      "84 54.483428955078125\n",
      "85 54.47431945800781\n",
      "86 54.46525955200195\n",
      "87 54.456241607666016\n",
      "88 54.447265625\n",
      "89 54.438331604003906\n",
      "90 54.42943572998047\n",
      "91 54.42058181762695\n",
      "92 54.411773681640625\n",
      "93 54.40299606323242\n",
      "94 54.39426040649414\n",
      "95 54.38557434082031\n",
      "96 54.376914978027344\n",
      "97 54.368316650390625\n",
      "98 54.35974884033203\n",
      "99 54.351219177246094\n",
      "100 54.342708587646484\n",
      "101 54.334266662597656\n",
      "102 54.32585144042969\n",
      "103 54.317481994628906\n",
      "104 54.30913543701172\n",
      "105 54.30083465576172\n",
      "106 54.292579650878906\n",
      "107 54.28436279296875\n",
      "108 54.27617645263672\n",
      "109 54.26802444458008\n",
      "110 54.259925842285156\n",
      "111 54.25184631347656\n",
      "112 54.243812561035156\n",
      "113 54.23582458496094\n",
      "114 54.22785568237305\n",
      "115 54.219932556152344\n",
      "116 54.2120475769043\n",
      "117 54.20420455932617\n",
      "118 54.19639205932617\n",
      "119 54.18860626220703\n",
      "120 54.18085479736328\n",
      "121 54.17316436767578\n",
      "122 54.16548156738281\n",
      "123 54.1578483581543\n",
      "124 54.15024948120117\n",
      "125 54.14268112182617\n",
      "126 54.13513946533203\n",
      "127 54.12765884399414\n",
      "128 54.12019348144531\n",
      "129 54.11275863647461\n",
      "130 54.105369567871094\n",
      "131 54.09800720214844\n",
      "132 54.09067916870117\n",
      "133 54.08339309692383\n",
      "134 54.076133728027344\n",
      "135 54.068904876708984\n",
      "136 54.06170654296875\n",
      "137 54.0545539855957\n",
      "138 54.04742431640625\n",
      "139 54.04031753540039\n",
      "140 54.033267974853516\n",
      "141 54.026241302490234\n",
      "142 54.01922607421875\n",
      "143 54.01227569580078\n",
      "144 54.00533676147461\n",
      "145 53.998435974121094\n",
      "146 53.99156951904297\n",
      "147 53.98472595214844\n",
      "148 53.97792053222656\n",
      "149 53.97113037109375\n",
      "150 53.964393615722656\n",
      "151 53.95768737792969\n",
      "152 53.95099639892578\n",
      "153 53.944339752197266\n",
      "154 53.937721252441406\n",
      "155 53.931129455566406\n",
      "156 53.924560546875\n",
      "157 53.918033599853516\n",
      "158 53.91153335571289\n",
      "159 53.90504837036133\n",
      "160 53.89860916137695\n",
      "161 53.89219284057617\n",
      "162 53.88580322265625\n",
      "163 53.879451751708984\n",
      "164 53.87313461303711\n",
      "165 53.8668327331543\n",
      "166 53.860557556152344\n",
      "167 53.85432052612305\n",
      "168 53.848106384277344\n",
      "169 53.8419189453125\n",
      "170 53.835758209228516\n",
      "171 53.82963562011719\n",
      "172 53.823543548583984\n",
      "173 53.81745910644531\n",
      "174 53.81142807006836\n",
      "175 53.805397033691406\n",
      "176 53.79940414428711\n",
      "177 53.793453216552734\n",
      "178 53.787506103515625\n",
      "179 53.7816047668457\n",
      "180 53.77572250366211\n",
      "181 53.769859313964844\n",
      "182 53.7640380859375\n",
      "183 53.758235931396484\n",
      "184 53.7524528503418\n",
      "185 53.746707916259766\n",
      "186 53.7409782409668\n",
      "187 53.73527908325195\n",
      "188 53.72960662841797\n",
      "189 53.72396469116211\n",
      "190 53.71834182739258\n",
      "191 53.71274948120117\n",
      "192 53.70717239379883\n",
      "193 53.70163345336914\n",
      "194 53.69610595703125\n",
      "195 53.690616607666016\n",
      "196 53.685150146484375\n",
      "197 53.6796989440918\n",
      "198 53.674278259277344\n",
      "199 53.66888427734375\n",
      "200 53.663509368896484\n",
      "201 53.65817642211914\n",
      "202 53.652828216552734\n",
      "203 53.64754104614258\n",
      "204 53.64226531982422\n",
      "205 53.63700866699219\n",
      "206 53.63179016113281\n",
      "207 53.6265869140625\n",
      "208 53.62139129638672\n",
      "209 53.61624526977539\n",
      "210 53.611106872558594\n",
      "211 53.605987548828125\n",
      "212 53.60090637207031\n",
      "213 53.5958366394043\n",
      "214 53.59079360961914\n",
      "215 53.585777282714844\n",
      "216 53.58077621459961\n",
      "217 53.5758056640625\n",
      "218 53.57085037231445\n",
      "219 53.56592559814453\n",
      "220 53.56101608276367\n",
      "221 53.55613708496094\n",
      "222 53.5512580871582\n",
      "223 53.546417236328125\n",
      "224 53.54159164428711\n",
      "225 53.53679275512695\n",
      "226 53.532020568847656\n",
      "227 53.52726364135742\n",
      "228 53.52252197265625\n",
      "229 53.51780319213867\n",
      "230 53.513118743896484\n",
      "231 53.50844192504883\n",
      "232 53.50379180908203\n",
      "233 53.49916458129883\n",
      "234 53.49455261230469\n",
      "235 53.489959716796875\n",
      "236 53.48540496826172\n",
      "237 53.4808464050293\n",
      "238 53.476322174072266\n",
      "239 53.471824645996094\n",
      "240 53.46731948852539\n",
      "241 53.462860107421875\n",
      "242 53.458404541015625\n",
      "243 53.4539794921875\n",
      "244 53.4495735168457\n",
      "245 53.4451904296875\n",
      "246 53.44081497192383\n",
      "247 53.43647003173828\n",
      "248 53.43213653564453\n",
      "249 53.42782211303711\n",
      "250 53.42353820800781\n",
      "251 53.41926193237305\n",
      "252 53.415008544921875\n",
      "253 53.410770416259766\n",
      "254 53.40654754638672\n",
      "255 53.40235900878906\n",
      "256 53.39817810058594\n",
      "257 53.394012451171875\n",
      "258 53.389869689941406\n",
      "259 53.3857536315918\n",
      "260 53.381656646728516\n",
      "261 53.3775634765625\n",
      "262 53.37350082397461\n",
      "263 53.369441986083984\n",
      "264 53.36540985107422\n",
      "265 53.36138916015625\n",
      "266 53.35739517211914\n",
      "267 53.35340881347656\n",
      "268 53.34946060180664\n",
      "269 53.34550857543945\n",
      "270 53.341583251953125\n",
      "271 53.33768081665039\n",
      "272 53.333778381347656\n",
      "273 53.32990264892578\n",
      "274 53.3260383605957\n",
      "275 53.32219696044922\n",
      "276 53.31837463378906\n",
      "277 53.314571380615234\n",
      "278 53.31078338623047\n",
      "279 53.3070068359375\n",
      "280 53.303253173828125\n",
      "281 53.29950714111328\n",
      "282 53.2957878112793\n",
      "283 53.292083740234375\n",
      "284 53.288387298583984\n",
      "285 53.28470993041992\n",
      "286 53.281044006347656\n",
      "287 53.27741622924805\n",
      "288 53.273780822753906\n",
      "289 53.270172119140625\n",
      "290 53.266571044921875\n",
      "291 53.26298904418945\n",
      "292 53.25942611694336\n",
      "293 53.25587844848633\n",
      "294 53.252342224121094\n",
      "295 53.24883270263672\n",
      "296 53.24532699584961\n",
      "297 53.2418327331543\n",
      "298 53.23835754394531\n",
      "299 53.23490905761719\n",
      "300 53.23146057128906\n",
      "301 53.22804260253906\n",
      "302 53.22462844848633\n",
      "303 53.221229553222656\n",
      "304 53.21784591674805\n",
      "305 53.21448516845703\n",
      "306 53.21113204956055\n",
      "307 53.20779037475586\n",
      "308 53.204463958740234\n",
      "309 53.20115661621094\n",
      "310 53.19786071777344\n",
      "311 53.194580078125\n",
      "312 53.191314697265625\n",
      "313 53.18806457519531\n",
      "314 53.18482208251953\n",
      "315 53.18159866333008\n",
      "316 53.17838668823242\n",
      "317 53.17518997192383\n",
      "318 53.17201232910156\n",
      "319 53.1688346862793\n",
      "320 53.16569519042969\n",
      "321 53.16254425048828\n",
      "322 53.159420013427734\n",
      "323 53.15629577636719\n",
      "324 53.1531982421875\n",
      "325 53.150115966796875\n",
      "326 53.14704132080078\n",
      "327 53.14397430419922\n",
      "328 53.14093017578125\n",
      "329 53.13789367675781\n",
      "330 53.13487243652344\n",
      "331 53.13187026977539\n",
      "332 53.12887191772461\n",
      "333 53.125885009765625\n",
      "334 53.1229133605957\n",
      "335 53.11996078491211\n",
      "336 53.117008209228516\n",
      "337 53.114078521728516\n",
      "338 53.11116409301758\n",
      "339 53.10825729370117\n",
      "340 53.10536193847656\n",
      "341 53.10247802734375\n",
      "342 53.099605560302734\n",
      "343 53.096744537353516\n",
      "344 53.093902587890625\n",
      "345 53.0910758972168\n",
      "346 53.088260650634766\n",
      "347 53.08544158935547\n",
      "348 53.082645416259766\n",
      "349 53.07986831665039\n",
      "350 53.077083587646484\n",
      "351 53.07432556152344\n",
      "352 53.07157897949219\n",
      "353 53.06884002685547\n",
      "354 53.06611633300781\n",
      "355 53.06340026855469\n",
      "356 53.06069564819336\n",
      "357 53.058006286621094\n",
      "358 53.055328369140625\n",
      "359 53.05266189575195\n",
      "360 53.04999542236328\n",
      "361 53.04734802246094\n",
      "362 53.04471206665039\n",
      "363 53.042091369628906\n",
      "364 53.03947448730469\n",
      "365 53.03687286376953\n",
      "366 53.03429412841797\n",
      "367 53.03169631958008\n",
      "368 53.029136657714844\n",
      "369 53.02658462524414\n",
      "370 53.02402877807617\n",
      "371 53.02149200439453\n",
      "372 53.01896667480469\n",
      "373 53.01645278930664\n",
      "374 53.013938903808594\n",
      "375 53.01145553588867\n",
      "376 53.008968353271484\n",
      "377 53.00648880004883\n",
      "378 53.004032135009766\n",
      "379 53.00157928466797\n",
      "380 52.9991340637207\n",
      "381 52.99671173095703\n",
      "382 52.9942741394043\n",
      "383 52.99187469482422\n",
      "384 52.989471435546875\n",
      "385 52.98707580566406\n",
      "386 52.98469161987305\n",
      "387 52.982322692871094\n",
      "388 52.97996139526367\n",
      "389 52.977603912353516\n",
      "390 52.97526550292969\n",
      "391 52.97293472290039\n",
      "392 52.970611572265625\n",
      "393 52.96828842163086\n",
      "394 52.96599197387695\n",
      "395 52.96369934082031\n",
      "396 52.96141052246094\n",
      "397 52.95914077758789\n",
      "398 52.956878662109375\n",
      "399 52.95461654663086\n",
      "400 52.95236587524414\n",
      "401 52.950130462646484\n",
      "402 52.94790267944336\n",
      "403 52.945674896240234\n",
      "404 52.94346618652344\n",
      "405 52.9412727355957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406 52.9390754699707\n",
      "407 52.93689727783203\n",
      "408 52.93471908569336\n",
      "409 52.93256378173828\n",
      "410 52.93040466308594\n",
      "411 52.928245544433594\n",
      "412 52.92610549926758\n",
      "413 52.923980712890625\n",
      "414 52.9218635559082\n",
      "415 52.91975021362305\n",
      "416 52.91764450073242\n",
      "417 52.91554260253906\n",
      "418 52.91346740722656\n",
      "419 52.91138458251953\n",
      "420 52.909324645996094\n",
      "421 52.907257080078125\n",
      "422 52.905208587646484\n",
      "423 52.903160095214844\n",
      "424 52.901119232177734\n",
      "425 52.89909362792969\n",
      "426 52.89707946777344\n",
      "427 52.89506149291992\n",
      "428 52.89305877685547\n",
      "429 52.891056060791016\n",
      "430 52.88908767700195\n",
      "431 52.8870964050293\n",
      "432 52.88512420654297\n",
      "433 52.88317108154297\n",
      "434 52.88121032714844\n",
      "435 52.879268646240234\n",
      "436 52.87732696533203\n",
      "437 52.875396728515625\n",
      "438 52.87347412109375\n",
      "439 52.87155532836914\n",
      "440 52.8696403503418\n",
      "441 52.86775588989258\n",
      "442 52.8658561706543\n",
      "443 52.86397171020508\n",
      "444 52.86209487915039\n",
      "445 52.8602180480957\n",
      "446 52.85835647583008\n",
      "447 52.85650634765625\n",
      "448 52.854652404785156\n",
      "449 52.852813720703125\n",
      "450 52.850982666015625\n",
      "451 52.849159240722656\n",
      "452 52.84734344482422\n",
      "453 52.84553527832031\n",
      "454 52.843719482421875\n",
      "455 52.84192657470703\n",
      "456 52.84013748168945\n",
      "457 52.83835220336914\n",
      "458 52.83658218383789\n",
      "459 52.834808349609375\n",
      "460 52.833038330078125\n",
      "461 52.831295013427734\n",
      "462 52.82954788208008\n",
      "463 52.82780075073242\n",
      "464 52.82606887817383\n",
      "465 52.8243408203125\n",
      "466 52.82261657714844\n",
      "467 52.82090759277344\n",
      "468 52.81920623779297\n",
      "469 52.817501068115234\n",
      "470 52.8158073425293\n",
      "471 52.814125061035156\n",
      "472 52.812442779541016\n",
      "473 52.810768127441406\n",
      "474 52.80909729003906\n",
      "475 52.80744171142578\n",
      "476 52.805782318115234\n",
      "477 52.80414581298828\n",
      "478 52.8025016784668\n",
      "479 52.80086135864258\n",
      "480 52.799224853515625\n",
      "481 52.797611236572266\n",
      "482 52.795997619628906\n",
      "483 52.79439163208008\n",
      "484 52.79279327392578\n",
      "485 52.79118728637695\n",
      "486 52.78959655761719\n",
      "487 52.78801727294922\n",
      "488 52.786434173583984\n",
      "489 52.78485870361328\n",
      "490 52.78329086303711\n",
      "491 52.78173828125\n",
      "492 52.78018569946289\n",
      "493 52.77863693237305\n",
      "494 52.7770881652832\n",
      "495 52.77554702758789\n",
      "496 52.774017333984375\n",
      "497 52.77250671386719\n",
      "498 52.77097702026367\n",
      "499 52.76946258544922\n",
      "1523851368.013908\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t = time.time()\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "# dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, num_points, label_size, input_dim, num_bins= 32, 3, 2, 3, 3\n",
    "\n",
    "# Create random Tensors to hold input and outputs, and wrap them in Variables.\n",
    "x = Variable(torch.randn(N, num_points, input_dim).type(dtype), requires_grad=False)\n",
    "y = Variable(torch.randn(N, label_size).type(dtype).cuda(), requires_grad=False)\n",
    "\n",
    "# Create random Tensors for weights, and wrap them in Variables.\n",
    "# w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True)\n",
    "# w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)\n",
    "\n",
    "model = myNet2d(num_points, input_dim, bins=num_bins).cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 1e-6, momentum=0.8)\n",
    "\n",
    "\n",
    "for t in range(500):\n",
    "    # To apply our Function, we use Function.apply method. We alias this as 'relu'.\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass: compute predicted y using operations on Variables; we compute\n",
    "    # ReLU using our custom autograd operation.\n",
    "    \n",
    "    y_pred = model(x.cuda())\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.data[0])\n",
    "    if(loss.data[0]==float('inf')):\n",
    "        break\n",
    "\n",
    "    # Use autograd to compute the backward pass.\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "\n",
    "print(time.time() - t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
