{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import argparse\n",
    "import h5py\n",
    "import json\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE_LIST = \"data/modelnet40_ply_hdf5_2048/train_files.txt\"\n",
    "TRAIN_FILE_ID_LIST = \"data/modelnet40_ply_hdf5_2048/train_files_id.txt\"\n",
    "TEST_FILE_LIST  = \"data/modelnet40_ply_hdf5_2048/test_files.txt\"\n",
    "TEST_FILE_ID_LIST = \"data/modelnet40_ply_hdf5_2048/test_files_id.txt\"\n",
    "SHAPE_LIST = \"data/modelnet40_ply_hdf5_2048/shape_names.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelNetDataset(Dataset):\n",
    "    \n",
    "    \n",
    "    def __init__(self, root, train=True, npoints=2048, mesh=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root (string): Directory of data, default = data/modelnet40_ply_hdf5_2048\n",
    "            train (boolean): return Training data if true, Testing data o.w.\n",
    "            npoints (int): 512, 1024, 2048; default: 2048\n",
    "\n",
    "        Returns:\n",
    "            data (n, npoints, 3): ndarray image arrays\n",
    "            labels (n, 1): ndarray, label of image\n",
    "            ###label_names (n,1): str ndarray label names \n",
    "            ###mesh_paths (str): director of its corresponding mesh\n",
    "        \"\"\"\n",
    "        \n",
    "        def load_mesh_file(filename, train=True):\n",
    "            \"\"\"\n",
    "            parse the list of mesh directories to match '.off' file in data/ModelNet40 folder\n",
    "            \"\"\"\n",
    "            with open(filename) as file:\n",
    "                data = json.load(file)\n",
    "            # original: eg, sofa/sofa_0037.ply, output: ['sofa', 'sofa_0037]\n",
    "            a = [data[i].split('.')[0].split('/') for i in range(len(data))]\n",
    "            # output: \"data/ModelNet40/sofa/train/sofa_0037.off\"\n",
    "            f = [os.path.join(\"data/ModelNet40\",a[i][0], folder,a[i][1]+'.off')  for i in range(len(a))]\n",
    "            return f\n",
    "        \n",
    "        train_txt = os.path.join(root, \"train_files.txt\")\n",
    "        mesh_train_txt = os.path.join(root, \"train_files_id.txt\")\n",
    "        test_txt = os.path.join(root, \"test_files.txt\")\n",
    "        mesh_test_txt = os.path.join(root, \"test_files_id.txt\")\n",
    "        label_names_txt = os.path.join(root, \"shape_names.txt\")\n",
    "        \n",
    "        label_names = [line.rstrip() for line in open(label_names_txt)]\n",
    "        if train:\n",
    "            h5_list = [line.rstrip() for line in open(train_txt)]\n",
    "            mesh_list = [line.rstrip() for line in open(mesh_train_txt)]\n",
    "        else:\n",
    "            h5_list = [line.rstrip() for line in open(test_txt)]\n",
    "            mesh_list = [line.rstrip() for line in open(mesh_test_txt)]\n",
    "        \n",
    "        h5_file = h5py.File(h5_list[0])\n",
    "        self.npoints = npoints\n",
    "        self.mesh = mesh\n",
    "        self.data = h5_file['data'][:,0:npoints,:]\n",
    "        self.labels = h5_file['label']\n",
    "        self.length = len(self.data)\n",
    "        self.mesh_paths = load_mesh_file(mesh_list[0])\n",
    "        print(len(self.mesh_paths))\n",
    "        \n",
    "        for i in range(1, len(h5_list)):\n",
    "            h5_file = h5py.File(h5_list[i])\n",
    "            datum = h5_file['data'][:,0:npoints,:]\n",
    "            label = h5_file['label']\n",
    "            self.data = np.concatenate((self.data, datum), axis=0)\n",
    "            self.labels = np.concatenate((self.labels, label), axis=0)\n",
    "            self.length += len(datum)\n",
    "            mesh_path = load_mesh_file(mesh_list[i])\n",
    "            self.mesh_paths += mesh_path\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        if self.mesh:\n",
    "            return self.data[index], self.labels[index], self.mesh_paths[index]\n",
    "        else:\n",
    "            return self.data[index], self.labels[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = True\n",
    "if train:\n",
    "    folder = 'train'\n",
    "\n",
    "root = \"data/modelnet40_ply_hdf5_2048/\"\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n",
      "2048\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(ModelNetDataset(root, train=True), batch_size=4,\n",
    "                       shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(ModelNetDataset(root, train=False), batch_size=4,\n",
    "                       shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression (\n",
       "  (linear): Linear (6144 -> 40)\n",
       ")"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return F.log_softmax(out)\n",
    "\n",
    "model = LogisticRegression(2048*3, 40)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperParamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "learning_rate = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.long().cuda()\n",
    "        data, target = Variable(data.view(-1, 2048*3)), Variable(target[:,0])\n",
    "        \n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "            \n",
    "    print(\"Training Accuracy: {:.0f}%\".format(100.*correct/len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    for (data, target) in test_loader:\n",
    "        data, target = data.cuda(), target.long().cuda()\n",
    "        data, target = Variable(data.view(-1, 2048*3)), Variable(target[:,0])\n",
    "        output = model(data)\n",
    "\n",
    "        # sum up batch loss\n",
    "        test_loss += criterion(output, target).data[0]\n",
    "        # get the index of the max log-probability\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'\n",
    "          .format(test_loss, correct, len(test_loader.dataset),\n",
    "                  100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/9840 (0%)]\tLoss: 3.821280\t\n",
      "Train Epoch: 0 [400/9840 (4%)]\tLoss: 3.747535\t\n",
      "Train Epoch: 0 [800/9840 (8%)]\tLoss: 3.521755\t\n",
      "Train Epoch: 0 [1200/9840 (12%)]\tLoss: 6.514562\t\n",
      "Train Epoch: 0 [1600/9840 (16%)]\tLoss: 4.011637\t\n",
      "Train Epoch: 0 [2000/9840 (20%)]\tLoss: 4.876408\t\n",
      "Train Epoch: 0 [2400/9840 (24%)]\tLoss: 8.352939\t\n",
      "Train Epoch: 0 [2800/9840 (28%)]\tLoss: 4.886254\t\n",
      "Train Epoch: 0 [3200/9840 (33%)]\tLoss: 8.478621\t\n",
      "Train Epoch: 0 [3600/9840 (37%)]\tLoss: 5.168748\t\n",
      "Train Epoch: 0 [4000/9840 (41%)]\tLoss: 10.292203\t\n",
      "Train Epoch: 0 [4400/9840 (45%)]\tLoss: 6.986529\t\n",
      "Train Epoch: 0 [4800/9840 (49%)]\tLoss: 5.648674\t\n",
      "Train Epoch: 0 [5200/9840 (53%)]\tLoss: 9.356028\t\n",
      "Train Epoch: 0 [5600/9840 (57%)]\tLoss: 11.209589\t\n",
      "Train Epoch: 0 [6000/9840 (61%)]\tLoss: 10.116699\t\n",
      "Train Epoch: 0 [6400/9840 (65%)]\tLoss: 8.483124\t\n",
      "Train Epoch: 0 [6800/9840 (69%)]\tLoss: 7.749393\t\n",
      "Train Epoch: 0 [7200/9840 (73%)]\tLoss: 12.292482\t\n",
      "Train Epoch: 0 [7600/9840 (77%)]\tLoss: 9.736151\t\n",
      "Train Epoch: 0 [8000/9840 (81%)]\tLoss: 7.285369\t\n",
      "Train Epoch: 0 [8400/9840 (85%)]\tLoss: 14.590692\t\n",
      "Train Epoch: 0 [8800/9840 (89%)]\tLoss: 11.782335\t\n",
      "Train Epoch: 0 [9200/9840 (93%)]\tLoss: 10.834974\t\n",
      "Train Epoch: 0 [9600/9840 (98%)]\tLoss: 7.899850\t\n",
      "Training Accuracy: 6%\n",
      "\n",
      "Test set: Average loss: 2.8183, Accuracy: 124/2468 (5%)\n",
      "\n",
      "Train Epoch: 1 [0/9840 (0%)]\tLoss: 2.104418\t\n",
      "Train Epoch: 1 [400/9840 (4%)]\tLoss: 2.144869\t\n",
      "Train Epoch: 1 [800/9840 (8%)]\tLoss: 2.551943\t\n",
      "Train Epoch: 1 [1200/9840 (12%)]\tLoss: 0.424684\t\n",
      "Train Epoch: 1 [1600/9840 (16%)]\tLoss: 0.008782\t\n",
      "Train Epoch: 1 [2000/9840 (20%)]\tLoss: 0.000530\t\n",
      "Train Epoch: 1 [2400/9840 (24%)]\tLoss: 0.012398\t\n",
      "Train Epoch: 1 [2800/9840 (28%)]\tLoss: 0.018736\t\n",
      "Train Epoch: 1 [3200/9840 (33%)]\tLoss: 1.308565\t\n",
      "Train Epoch: 1 [3600/9840 (37%)]\tLoss: 1.161222\t\n",
      "Train Epoch: 1 [4000/9840 (41%)]\tLoss: 1.744505\t\n",
      "Train Epoch: 1 [4400/9840 (45%)]\tLoss: 2.397575\t\n",
      "Train Epoch: 1 [4800/9840 (49%)]\tLoss: 0.050684\t\n",
      "Train Epoch: 1 [5200/9840 (53%)]\tLoss: 1.805948\t\n",
      "Train Epoch: 1 [5600/9840 (57%)]\tLoss: 0.007061\t\n",
      "Train Epoch: 1 [6000/9840 (61%)]\tLoss: 0.691636\t\n",
      "Train Epoch: 1 [6400/9840 (65%)]\tLoss: 0.095162\t\n",
      "Train Epoch: 1 [6800/9840 (69%)]\tLoss: 3.153286\t\n",
      "Train Epoch: 1 [7200/9840 (73%)]\tLoss: 5.721632\t\n",
      "Train Epoch: 1 [7600/9840 (77%)]\tLoss: 4.572390\t\n",
      "Train Epoch: 1 [8000/9840 (81%)]\tLoss: 1.889847\t\n",
      "Train Epoch: 1 [8400/9840 (85%)]\tLoss: 0.164027\t\n",
      "Train Epoch: 1 [8800/9840 (89%)]\tLoss: 1.610430\t\n",
      "Train Epoch: 1 [9200/9840 (93%)]\tLoss: 0.793704\t\n",
      "Train Epoch: 1 [9600/9840 (98%)]\tLoss: 3.225275\t\n",
      "Training Accuracy: 80%\n",
      "\n",
      "Test set: Average loss: 3.5168, Accuracy: 162/2468 (7%)\n",
      "\n",
      "Train Epoch: 2 [0/9840 (0%)]\tLoss: 0.000111\t\n",
      "Train Epoch: 2 [400/9840 (4%)]\tLoss: 0.000001\t\n",
      "Train Epoch: 2 [800/9840 (8%)]\tLoss: 0.000789\t\n",
      "Train Epoch: 2 [1200/9840 (12%)]\tLoss: 0.000020\t\n",
      "Train Epoch: 2 [1600/9840 (16%)]\tLoss: 0.005221\t\n",
      "Train Epoch: 2 [2000/9840 (20%)]\tLoss: 2.005869\t\n",
      "Train Epoch: 2 [2400/9840 (24%)]\tLoss: 0.000309\t\n",
      "Train Epoch: 2 [2800/9840 (28%)]\tLoss: 0.533249\t\n",
      "Train Epoch: 2 [3200/9840 (33%)]\tLoss: 1.855158\t\n",
      "Train Epoch: 2 [3600/9840 (37%)]\tLoss: 0.018081\t\n",
      "Train Epoch: 2 [4000/9840 (41%)]\tLoss: 0.297742\t\n",
      "Train Epoch: 2 [4400/9840 (45%)]\tLoss: 0.000048\t\n",
      "Train Epoch: 2 [4800/9840 (49%)]\tLoss: 2.313211\t\n",
      "Train Epoch: 2 [5200/9840 (53%)]\tLoss: 2.185441\t\n",
      "Train Epoch: 2 [5600/9840 (57%)]\tLoss: 0.039319\t\n",
      "Train Epoch: 2 [6000/9840 (61%)]\tLoss: 0.021657\t\n",
      "Train Epoch: 2 [6400/9840 (65%)]\tLoss: 0.033950\t\n",
      "Train Epoch: 2 [6800/9840 (69%)]\tLoss: 1.678782\t\n",
      "Train Epoch: 2 [7200/9840 (73%)]\tLoss: 1.599214\t\n",
      "Train Epoch: 2 [7600/9840 (77%)]\tLoss: 0.045796\t\n",
      "Train Epoch: 2 [8000/9840 (81%)]\tLoss: 6.395164\t\n",
      "Train Epoch: 2 [8400/9840 (85%)]\tLoss: 1.702628\t\n",
      "Train Epoch: 2 [8800/9840 (89%)]\tLoss: 0.020120\t\n",
      "Train Epoch: 2 [9200/9840 (93%)]\tLoss: 0.671739\t\n",
      "Train Epoch: 2 [9600/9840 (98%)]\tLoss: 5.687624\t\n",
      "Training Accuracy: 77%\n",
      "\n",
      "Test set: Average loss: 4.6728, Accuracy: 147/2468 (6%)\n",
      "\n",
      "Train Epoch: 3 [0/9840 (0%)]\tLoss: 0.052698\t\n",
      "Train Epoch: 3 [400/9840 (4%)]\tLoss: 0.068208\t\n",
      "Train Epoch: 3 [800/9840 (8%)]\tLoss: 2.517727\t\n",
      "Train Epoch: 3 [1200/9840 (12%)]\tLoss: 0.000295\t\n",
      "Train Epoch: 3 [1600/9840 (16%)]\tLoss: 0.339498\t\n",
      "Train Epoch: 3 [2000/9840 (20%)]\tLoss: 2.968589\t\n",
      "Train Epoch: 3 [2400/9840 (24%)]\tLoss: 0.410282\t\n",
      "Train Epoch: 3 [2800/9840 (28%)]\tLoss: 0.000071\t\n",
      "Train Epoch: 3 [3200/9840 (33%)]\tLoss: 0.354643\t\n",
      "Train Epoch: 3 [3600/9840 (37%)]\tLoss: 1.830212\t\n",
      "Train Epoch: 3 [4000/9840 (41%)]\tLoss: 2.126753\t\n",
      "Train Epoch: 3 [4400/9840 (45%)]\tLoss: 0.250268\t\n",
      "Train Epoch: 3 [4800/9840 (49%)]\tLoss: 0.742419\t\n",
      "Train Epoch: 3 [5200/9840 (53%)]\tLoss: 3.777781\t\n",
      "Train Epoch: 3 [5600/9840 (57%)]\tLoss: 2.565589\t\n",
      "Train Epoch: 3 [6000/9840 (61%)]\tLoss: 0.512125\t\n",
      "Train Epoch: 3 [6400/9840 (65%)]\tLoss: 2.820523\t\n",
      "Train Epoch: 3 [6800/9840 (69%)]\tLoss: 1.244468\t\n",
      "Train Epoch: 3 [7200/9840 (73%)]\tLoss: 2.880475\t\n",
      "Train Epoch: 3 [7600/9840 (77%)]\tLoss: 2.833293\t\n",
      "Train Epoch: 3 [8000/9840 (81%)]\tLoss: 2.579592\t\n",
      "Train Epoch: 3 [8400/9840 (85%)]\tLoss: 0.061707\t\n",
      "Train Epoch: 3 [8800/9840 (89%)]\tLoss: 0.000001\t\n",
      "Train Epoch: 3 [9200/9840 (93%)]\tLoss: 2.676719\t\n",
      "Train Epoch: 3 [9600/9840 (98%)]\tLoss: 1.794690\t\n",
      "Training Accuracy: 78%\n",
      "\n",
      "Test set: Average loss: 5.5428, Accuracy: 145/2468 (6%)\n",
      "\n",
      "Train Epoch: 4 [0/9840 (0%)]\tLoss: 0.003990\t\n",
      "Train Epoch: 4 [400/9840 (4%)]\tLoss: 0.104725\t\n",
      "Train Epoch: 4 [800/9840 (8%)]\tLoss: 0.000047\t\n",
      "Train Epoch: 4 [1200/9840 (12%)]\tLoss: 0.007360\t\n",
      "Train Epoch: 4 [1600/9840 (16%)]\tLoss: 0.786008\t\n",
      "Train Epoch: 4 [2000/9840 (20%)]\tLoss: 0.000469\t\n",
      "Train Epoch: 4 [2400/9840 (24%)]\tLoss: 2.096397\t\n",
      "Train Epoch: 4 [2800/9840 (28%)]\tLoss: 1.846045\t\n",
      "Train Epoch: 4 [3200/9840 (33%)]\tLoss: 1.887135\t\n",
      "Train Epoch: 4 [3600/9840 (37%)]\tLoss: 0.473072\t\n",
      "Train Epoch: 4 [4000/9840 (41%)]\tLoss: 2.266591\t\n",
      "Train Epoch: 4 [4400/9840 (45%)]\tLoss: 0.359526\t\n",
      "Train Epoch: 4 [4800/9840 (49%)]\tLoss: 4.354502\t\n",
      "Train Epoch: 4 [5200/9840 (53%)]\tLoss: 0.000008\t\n",
      "Train Epoch: 4 [5600/9840 (57%)]\tLoss: 0.026022\t\n",
      "Train Epoch: 4 [6000/9840 (61%)]\tLoss: 0.080419\t\n",
      "Train Epoch: 4 [6400/9840 (65%)]\tLoss: 0.032498\t\n",
      "Train Epoch: 4 [6800/9840 (69%)]\tLoss: 1.771681\t\n",
      "Train Epoch: 4 [7200/9840 (73%)]\tLoss: 0.152277\t\n",
      "Train Epoch: 4 [7600/9840 (77%)]\tLoss: 0.046748\t\n",
      "Train Epoch: 4 [8000/9840 (81%)]\tLoss: 5.003686\t\n",
      "Train Epoch: 4 [8400/9840 (85%)]\tLoss: 2.346901\t\n",
      "Train Epoch: 4 [8800/9840 (89%)]\tLoss: 0.619955\t\n",
      "Train Epoch: 4 [9200/9840 (93%)]\tLoss: 0.447113\t\n",
      "Train Epoch: 4 [9600/9840 (98%)]\tLoss: 0.025596\t\n",
      "Training Accuracy: 88%\n",
      "\n",
      "Test set: Average loss: 6.2831, Accuracy: 149/2468 (6%)\n",
      "\n",
      "Train Epoch: 5 [0/9840 (0%)]\tLoss: 0.000000\t\n",
      "Train Epoch: 5 [400/9840 (4%)]\tLoss: 0.000000\t\n",
      "Train Epoch: 5 [800/9840 (8%)]\tLoss: 0.000000\t\n",
      "Train Epoch: 5 [1200/9840 (12%)]\tLoss: 0.027472\t\n",
      "Train Epoch: 5 [1600/9840 (16%)]\tLoss: 2.223791\t\n",
      "Train Epoch: 5 [2000/9840 (20%)]\tLoss: 1.145313\t\n",
      "Train Epoch: 5 [2400/9840 (24%)]\tLoss: 0.971261\t\n",
      "Train Epoch: 5 [2800/9840 (28%)]\tLoss: 0.000000\t\n",
      "Train Epoch: 5 [3200/9840 (33%)]\tLoss: 0.003319\t\n",
      "Train Epoch: 5 [3600/9840 (37%)]\tLoss: 0.024913\t\n",
      "Train Epoch: 5 [4000/9840 (41%)]\tLoss: 0.000000\t\n",
      "Train Epoch: 5 [4400/9840 (45%)]\tLoss: 0.000137\t\n",
      "Train Epoch: 5 [4800/9840 (49%)]\tLoss: 0.000666\t\n",
      "Train Epoch: 5 [5200/9840 (53%)]\tLoss: 2.040824\t\n",
      "Train Epoch: 5 [5600/9840 (57%)]\tLoss: 0.008305\t\n",
      "Train Epoch: 5 [6000/9840 (61%)]\tLoss: 0.000031\t\n",
      "Train Epoch: 5 [6400/9840 (65%)]\tLoss: 0.000677\t\n",
      "Train Epoch: 5 [6800/9840 (69%)]\tLoss: 0.308330\t\n",
      "Train Epoch: 5 [7200/9840 (73%)]\tLoss: 2.147460\t\n",
      "Train Epoch: 5 [7600/9840 (77%)]\tLoss: 1.658568\t\n",
      "Train Epoch: 5 [8000/9840 (81%)]\tLoss: 0.000033\t\n",
      "Train Epoch: 5 [8400/9840 (85%)]\tLoss: 0.069314\t\n",
      "Train Epoch: 5 [8800/9840 (89%)]\tLoss: 0.992058\t\n",
      "Train Epoch: 5 [9200/9840 (93%)]\tLoss: 1.285491\t\n",
      "Train Epoch: 5 [9600/9840 (98%)]\tLoss: 2.976044\t\n",
      "Training Accuracy: 91%\n",
      "\n",
      "Test set: Average loss: 6.8248, Accuracy: 154/2468 (6%)\n",
      "\n",
      "Train Epoch: 6 [0/9840 (0%)]\tLoss: 0.000005\t\n",
      "Train Epoch: 6 [400/9840 (4%)]\tLoss: 0.001638\t\n",
      "Train Epoch: 6 [800/9840 (8%)]\tLoss: 0.526441\t\n",
      "Train Epoch: 6 [1200/9840 (12%)]\tLoss: 3.332598\t\n",
      "Train Epoch: 6 [1600/9840 (16%)]\tLoss: 0.026769\t\n",
      "Train Epoch: 6 [2000/9840 (20%)]\tLoss: 0.000945\t\n",
      "Train Epoch: 6 [2400/9840 (24%)]\tLoss: 1.646418\t\n",
      "Train Epoch: 6 [2800/9840 (28%)]\tLoss: 0.770725\t\n",
      "Train Epoch: 6 [3200/9840 (33%)]\tLoss: 0.052805\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [3600/9840 (37%)]\tLoss: 2.221018\t\n",
      "Train Epoch: 6 [4000/9840 (41%)]\tLoss: 0.503623\t\n",
      "Train Epoch: 6 [4400/9840 (45%)]\tLoss: 0.000047\t\n",
      "Train Epoch: 6 [4800/9840 (49%)]\tLoss: 0.001026\t\n",
      "Train Epoch: 6 [5200/9840 (53%)]\tLoss: 0.000335\t\n",
      "Train Epoch: 6 [5600/9840 (57%)]\tLoss: 0.372912\t\n",
      "Train Epoch: 6 [6000/9840 (61%)]\tLoss: 0.000057\t\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionWithStn (\n",
       "  (stn): STN (\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv3): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (mp1): MaxPool2d (size=(2048, 1), stride=(2048, 1), dilation=(1, 1))\n",
       "    (fc1): Linear (1024 -> 512)\n",
       "    (fc2): Linear (512 -> 256)\n",
       "    (fc3): Linear (256 -> 9)\n",
       "    (relu): ReLU ()\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (bn4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (bn5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "  )\n",
       "  (linear): Linear (6144 -> 40)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class STN(nn.Module):\n",
    "    def __init__(self, num_points = 2048):\n",
    "        super(STN, self).__init__()\n",
    "        self.num_points = num_points\n",
    "        self.conv1 = torch.nn.Conv2d(3, 64, 1)\n",
    "        self.conv2 = torch.nn.Conv2d(64, 128, 1)\n",
    "        self.conv3 = torch.nn.Conv2d(128, 1024, 1)\n",
    "        self.mp1 = torch.nn.MaxPool2d((num_points,1))\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 9)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.bn3 = nn.BatchNorm2d(1024)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.size()[0]\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.mp1(x)\n",
    "        x = x.view(-1, 1024)\n",
    "\n",
    "        x = F.relu(self.bn4(self.fc1(x)))\n",
    "        x = F.relu(self.bn5(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        iden = Variable(torch.from_numpy(np.array([1,0,0,0,1,0,0,0,1]).astype(np.float32)))#.view(1,9).repeat(batchsize,1)\n",
    "        if x.is_cuda:\n",
    "            iden = iden.cuda()\n",
    "        x = x + iden\n",
    "        x = x.view(-1, 3, 3)\n",
    "        return x\n",
    "    \n",
    "class LogisticRegressionWithStn(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LogisticRegressionWithStn, self).__init__()\n",
    "        self.stn = STN()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        trans = self.stn(x)\n",
    "        x = x.transpose(2,1)\n",
    "        x = x.view(4, 2048, 3)\n",
    "        x = torch.bmm(x, trans)\n",
    "        \n",
    "        \n",
    "        #print(x.view(4,2048*3).size())\n",
    "        out = self.linear(x.view(-1, 2048*3))\n",
    "        return out\n",
    "    \n",
    "model = LogisticRegressionWithStn(2048*3, 40)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/9840 (0%)]\tLoss: 3.676184\n",
      "Train Epoch: 0 [400/9840 (4%)]\tLoss: 3.595488\n",
      "Train Epoch: 0 [800/9840 (8%)]\tLoss: 3.223452\n",
      "Train Epoch: 0 [1200/9840 (12%)]\tLoss: 3.374255\n",
      "Train Epoch: 0 [1600/9840 (16%)]\tLoss: 2.792691\n",
      "Train Epoch: 0 [2000/9840 (20%)]\tLoss: 3.412310\n",
      "Train Epoch: 0 [2400/9840 (24%)]\tLoss: 3.635794\n",
      "Train Epoch: 0 [2800/9840 (28%)]\tLoss: 3.584598\n",
      "Train Epoch: 0 [3200/9840 (33%)]\tLoss: 3.436437\n",
      "Train Epoch: 0 [3600/9840 (37%)]\tLoss: 3.173114\n",
      "Train Epoch: 0 [4000/9840 (41%)]\tLoss: 3.093855\n",
      "Train Epoch: 0 [4400/9840 (45%)]\tLoss: 4.137153\n",
      "Train Epoch: 0 [4800/9840 (49%)]\tLoss: 3.945675\n",
      "Train Epoch: 0 [5200/9840 (53%)]\tLoss: 3.586825\n",
      "Train Epoch: 0 [5600/9840 (57%)]\tLoss: 3.161856\n",
      "Train Epoch: 0 [6000/9840 (61%)]\tLoss: 3.682771\n",
      "Train Epoch: 0 [6400/9840 (65%)]\tLoss: 3.633793\n",
      "Train Epoch: 0 [6800/9840 (69%)]\tLoss: 3.172957\n",
      "Train Epoch: 0 [7200/9840 (73%)]\tLoss: 3.403653\n",
      "Train Epoch: 0 [7600/9840 (77%)]\tLoss: 2.732123\n",
      "Train Epoch: 0 [8000/9840 (81%)]\tLoss: 3.928093\n",
      "Train Epoch: 0 [8400/9840 (85%)]\tLoss: 3.661375\n",
      "Train Epoch: 0 [8800/9840 (89%)]\tLoss: 2.785387\n",
      "Train Epoch: 0 [9200/9840 (93%)]\tLoss: 3.254175\n",
      "Train Epoch: 0 [9600/9840 (98%)]\tLoss: 3.575047\n",
      "\n",
      "Test set: Average loss: 0.9264, Accuracy: 115/2468 (5%)\n",
      "\n",
      "Train Epoch: 1 [0/9840 (0%)]\tLoss: 3.829392\n",
      "Train Epoch: 1 [400/9840 (4%)]\tLoss: 3.988680\n",
      "Train Epoch: 1 [800/9840 (8%)]\tLoss: 2.965750\n",
      "Train Epoch: 1 [1200/9840 (12%)]\tLoss: 4.219913\n",
      "Train Epoch: 1 [1600/9840 (16%)]\tLoss: 5.215450\n",
      "Train Epoch: 1 [2000/9840 (20%)]\tLoss: 2.758666\n",
      "Train Epoch: 1 [2400/9840 (24%)]\tLoss: 4.090724\n",
      "Train Epoch: 1 [2800/9840 (28%)]\tLoss: 3.106959\n",
      "Train Epoch: 1 [3200/9840 (33%)]\tLoss: 2.490380\n",
      "Train Epoch: 1 [3600/9840 (37%)]\tLoss: 2.739561\n",
      "Train Epoch: 1 [4000/9840 (41%)]\tLoss: 3.557896\n",
      "Train Epoch: 1 [4400/9840 (45%)]\tLoss: 4.606931\n",
      "Train Epoch: 1 [4800/9840 (49%)]\tLoss: 3.767533\n",
      "Train Epoch: 1 [5200/9840 (53%)]\tLoss: 3.208633\n",
      "Train Epoch: 1 [5600/9840 (57%)]\tLoss: 4.178072\n",
      "Train Epoch: 1 [6000/9840 (61%)]\tLoss: 2.803749\n",
      "Train Epoch: 1 [6400/9840 (65%)]\tLoss: 3.469045\n",
      "Train Epoch: 1 [6800/9840 (69%)]\tLoss: 3.137521\n",
      "Train Epoch: 1 [7200/9840 (73%)]\tLoss: 3.524803\n",
      "Train Epoch: 1 [7600/9840 (77%)]\tLoss: 3.475086\n",
      "Train Epoch: 1 [8000/9840 (81%)]\tLoss: 3.332287\n",
      "Train Epoch: 1 [8400/9840 (85%)]\tLoss: 3.451556\n",
      "Train Epoch: 1 [8800/9840 (89%)]\tLoss: 3.527097\n",
      "Train Epoch: 1 [9200/9840 (93%)]\tLoss: 3.181416\n",
      "Train Epoch: 1 [9600/9840 (98%)]\tLoss: 4.169991\n",
      "\n",
      "Test set: Average loss: 0.9417, Accuracy: 136/2468 (6%)\n",
      "\n",
      "Train Epoch: 2 [0/9840 (0%)]\tLoss: 1.662801\n",
      "Train Epoch: 2 [400/9840 (4%)]\tLoss: 2.926518\n",
      "Train Epoch: 2 [800/9840 (8%)]\tLoss: 0.677931\n",
      "Train Epoch: 2 [1200/9840 (12%)]\tLoss: 1.440907\n",
      "Train Epoch: 2 [1600/9840 (16%)]\tLoss: 2.648376\n",
      "Train Epoch: 2 [2000/9840 (20%)]\tLoss: 0.230882\n",
      "Train Epoch: 2 [2400/9840 (24%)]\tLoss: 2.137877\n",
      "Train Epoch: 2 [2800/9840 (28%)]\tLoss: 3.199646\n",
      "Train Epoch: 2 [3200/9840 (33%)]\tLoss: 1.835345\n",
      "Train Epoch: 2 [3600/9840 (37%)]\tLoss: 2.861719\n",
      "Train Epoch: 2 [4000/9840 (41%)]\tLoss: 2.412724\n",
      "Train Epoch: 2 [4400/9840 (45%)]\tLoss: 3.083322\n",
      "Train Epoch: 2 [4800/9840 (49%)]\tLoss: 1.497071\n",
      "Train Epoch: 2 [5200/9840 (53%)]\tLoss: 3.028152\n",
      "Train Epoch: 2 [5600/9840 (57%)]\tLoss: 2.760790\n",
      "Train Epoch: 2 [6000/9840 (61%)]\tLoss: 1.441670\n",
      "Train Epoch: 2 [6400/9840 (65%)]\tLoss: 0.572408\n",
      "Train Epoch: 2 [6800/9840 (69%)]\tLoss: 1.783822\n",
      "Train Epoch: 2 [7200/9840 (73%)]\tLoss: 1.079026\n",
      "Train Epoch: 2 [7600/9840 (77%)]\tLoss: 2.596330\n",
      "Train Epoch: 2 [8000/9840 (81%)]\tLoss: 2.243579\n",
      "Train Epoch: 2 [8400/9840 (85%)]\tLoss: 1.678124\n",
      "Train Epoch: 2 [8800/9840 (89%)]\tLoss: 2.586241\n",
      "Train Epoch: 2 [9200/9840 (93%)]\tLoss: 3.247309\n",
      "Train Epoch: 2 [9600/9840 (98%)]\tLoss: 1.312427\n",
      "\n",
      "Test set: Average loss: 1.1364, Accuracy: 197/2468 (8%)\n",
      "\n",
      "Train Epoch: 3 [0/9840 (0%)]\tLoss: 1.013259\n",
      "Train Epoch: 3 [400/9840 (4%)]\tLoss: 0.019325\n",
      "Train Epoch: 3 [800/9840 (8%)]\tLoss: 0.036328\n",
      "Train Epoch: 3 [1200/9840 (12%)]\tLoss: 0.143814\n",
      "Train Epoch: 3 [1600/9840 (16%)]\tLoss: 0.638831\n",
      "Train Epoch: 3 [2000/9840 (20%)]\tLoss: 0.517823\n",
      "Train Epoch: 3 [2400/9840 (24%)]\tLoss: 0.581997\n",
      "Train Epoch: 3 [2800/9840 (28%)]\tLoss: 0.227508\n",
      "Train Epoch: 3 [3200/9840 (33%)]\tLoss: 2.413717\n",
      "Train Epoch: 3 [3600/9840 (37%)]\tLoss: 1.040717\n",
      "Train Epoch: 3 [4000/9840 (41%)]\tLoss: 0.827958\n",
      "Train Epoch: 3 [4400/9840 (45%)]\tLoss: 0.720935\n",
      "Train Epoch: 3 [4800/9840 (49%)]\tLoss: 0.382576\n",
      "Train Epoch: 3 [5200/9840 (53%)]\tLoss: 0.111456\n",
      "Train Epoch: 3 [5600/9840 (57%)]\tLoss: 2.084683\n",
      "Train Epoch: 3 [6000/9840 (61%)]\tLoss: 0.733064\n",
      "Train Epoch: 3 [6400/9840 (65%)]\tLoss: 2.414142\n",
      "Train Epoch: 3 [6800/9840 (69%)]\tLoss: 3.254696\n",
      "Train Epoch: 3 [7200/9840 (73%)]\tLoss: 2.290812\n",
      "Train Epoch: 3 [7600/9840 (77%)]\tLoss: 1.725958\n",
      "Train Epoch: 3 [8000/9840 (81%)]\tLoss: 1.720195\n",
      "Train Epoch: 3 [8400/9840 (85%)]\tLoss: 0.517879\n",
      "Train Epoch: 3 [8800/9840 (89%)]\tLoss: 0.331006\n",
      "Train Epoch: 3 [9200/9840 (93%)]\tLoss: 1.973166\n",
      "Train Epoch: 3 [9600/9840 (98%)]\tLoss: 0.817871\n",
      "\n",
      "Test set: Average loss: 1.4952, Accuracy: 182/2468 (7%)\n",
      "\n",
      "Train Epoch: 4 [0/9840 (0%)]\tLoss: 0.047376\n",
      "Train Epoch: 4 [400/9840 (4%)]\tLoss: 0.255328\n",
      "Train Epoch: 4 [800/9840 (8%)]\tLoss: 0.010292\n",
      "Train Epoch: 4 [1200/9840 (12%)]\tLoss: 0.261344\n",
      "Train Epoch: 4 [1600/9840 (16%)]\tLoss: 0.043619\n",
      "Train Epoch: 4 [2000/9840 (20%)]\tLoss: 0.001087\n",
      "Train Epoch: 4 [2400/9840 (24%)]\tLoss: 0.085868\n",
      "Train Epoch: 4 [2800/9840 (28%)]\tLoss: 0.000450\n",
      "Train Epoch: 4 [3200/9840 (33%)]\tLoss: 0.102548\n",
      "Train Epoch: 4 [3600/9840 (37%)]\tLoss: 0.002788\n",
      "Train Epoch: 4 [4000/9840 (41%)]\tLoss: 0.005393\n",
      "Train Epoch: 4 [4400/9840 (45%)]\tLoss: 0.002332\n",
      "Train Epoch: 4 [4800/9840 (49%)]\tLoss: 0.214080\n",
      "Train Epoch: 4 [5200/9840 (53%)]\tLoss: 0.002932\n",
      "Train Epoch: 4 [5600/9840 (57%)]\tLoss: 0.024100\n",
      "Train Epoch: 4 [6000/9840 (61%)]\tLoss: 0.223278\n",
      "Train Epoch: 4 [6400/9840 (65%)]\tLoss: 0.936311\n",
      "Train Epoch: 4 [6800/9840 (69%)]\tLoss: 0.812895\n",
      "Train Epoch: 4 [7200/9840 (73%)]\tLoss: 0.330184\n",
      "Train Epoch: 4 [7600/9840 (77%)]\tLoss: 0.295322\n",
      "Train Epoch: 4 [8000/9840 (81%)]\tLoss: 0.430583\n",
      "Train Epoch: 4 [8400/9840 (85%)]\tLoss: 0.146673\n",
      "Train Epoch: 4 [8800/9840 (89%)]\tLoss: 0.785374\n",
      "Train Epoch: 4 [9200/9840 (93%)]\tLoss: 0.040571\n",
      "Train Epoch: 4 [9600/9840 (98%)]\tLoss: 0.213950\n",
      "\n",
      "Test set: Average loss: 2.7230, Accuracy: 183/2468 (7%)\n",
      "\n",
      "Train Epoch: 5 [0/9840 (0%)]\tLoss: 0.302925\n",
      "Train Epoch: 5 [400/9840 (4%)]\tLoss: 0.109720\n",
      "Train Epoch: 5 [800/9840 (8%)]\tLoss: 0.000042\n",
      "Train Epoch: 5 [1200/9840 (12%)]\tLoss: 0.020072\n",
      "Train Epoch: 5 [1600/9840 (16%)]\tLoss: 1.816741\n",
      "Train Epoch: 5 [2000/9840 (20%)]\tLoss: 0.000058\n",
      "Train Epoch: 5 [2400/9840 (24%)]\tLoss: 0.001220\n",
      "Train Epoch: 5 [2800/9840 (28%)]\tLoss: 0.430518\n",
      "Train Epoch: 5 [3200/9840 (33%)]\tLoss: 0.045810\n",
      "Train Epoch: 5 [3600/9840 (37%)]\tLoss: 0.584908\n",
      "Train Epoch: 5 [4000/9840 (41%)]\tLoss: 0.001657\n",
      "Train Epoch: 5 [4400/9840 (45%)]\tLoss: 0.050913\n",
      "Train Epoch: 5 [4800/9840 (49%)]\tLoss: 0.142812\n",
      "Train Epoch: 5 [5200/9840 (53%)]\tLoss: 0.115149\n",
      "Train Epoch: 5 [5600/9840 (57%)]\tLoss: 0.146699\n",
      "Train Epoch: 5 [6000/9840 (61%)]\tLoss: 0.066909\n",
      "Train Epoch: 5 [6400/9840 (65%)]\tLoss: 0.121332\n",
      "Train Epoch: 5 [6800/9840 (69%)]\tLoss: 0.047956\n",
      "Train Epoch: 5 [7200/9840 (73%)]\tLoss: 0.413088\n",
      "Train Epoch: 5 [7600/9840 (77%)]\tLoss: 1.432918\n",
      "Train Epoch: 5 [8000/9840 (81%)]\tLoss: 0.010489\n",
      "Train Epoch: 5 [8400/9840 (85%)]\tLoss: 0.006426\n",
      "Train Epoch: 5 [8800/9840 (89%)]\tLoss: 1.958133\n",
      "Train Epoch: 5 [9200/9840 (93%)]\tLoss: 2.474503\n",
      "Train Epoch: 5 [9600/9840 (98%)]\tLoss: 1.218796\n",
      "\n",
      "Test set: Average loss: 3.0594, Accuracy: 177/2468 (7%)\n",
      "\n",
      "Train Epoch: 6 [0/9840 (0%)]\tLoss: 0.054442\n",
      "Train Epoch: 6 [400/9840 (4%)]\tLoss: 0.007145\n",
      "Train Epoch: 6 [800/9840 (8%)]\tLoss: 0.000187\n",
      "Train Epoch: 6 [1200/9840 (12%)]\tLoss: 0.000133\n",
      "Train Epoch: 6 [1600/9840 (16%)]\tLoss: 0.014893\n",
      "Train Epoch: 6 [2000/9840 (20%)]\tLoss: 0.002165\n",
      "Train Epoch: 6 [2400/9840 (24%)]\tLoss: 0.045743\n",
      "Train Epoch: 6 [2800/9840 (28%)]\tLoss: 0.000103\n",
      "Train Epoch: 6 [3200/9840 (33%)]\tLoss: 0.000975\n",
      "Train Epoch: 6 [3600/9840 (37%)]\tLoss: 0.013986\n",
      "Train Epoch: 6 [4000/9840 (41%)]\tLoss: 0.061710\n",
      "Train Epoch: 6 [4400/9840 (45%)]\tLoss: 0.000069\n",
      "Train Epoch: 6 [4800/9840 (49%)]\tLoss: 0.034791\n",
      "Train Epoch: 6 [5200/9840 (53%)]\tLoss: 0.005051\n",
      "Train Epoch: 6 [5600/9840 (57%)]\tLoss: 0.077483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [6000/9840 (61%)]\tLoss: 0.000492\n",
      "Train Epoch: 6 [6400/9840 (65%)]\tLoss: 0.003031\n",
      "Train Epoch: 6 [6800/9840 (69%)]\tLoss: 0.002721\n",
      "Train Epoch: 6 [7200/9840 (73%)]\tLoss: 0.006145\n",
      "Train Epoch: 6 [7600/9840 (77%)]\tLoss: 0.099600\n",
      "Train Epoch: 6 [8000/9840 (81%)]\tLoss: 0.025441\n",
      "Train Epoch: 6 [8400/9840 (85%)]\tLoss: 0.275865\n",
      "Train Epoch: 6 [8800/9840 (89%)]\tLoss: 0.012437\n",
      "Train Epoch: 6 [9200/9840 (93%)]\tLoss: 3.388949\n",
      "Train Epoch: 6 [9600/9840 (98%)]\tLoss: 0.000032\n",
      "\n",
      "Test set: Average loss: 3.5247, Accuracy: 166/2468 (7%)\n",
      "\n",
      "Train Epoch: 7 [0/9840 (0%)]\tLoss: 0.566533\n",
      "Train Epoch: 7 [400/9840 (4%)]\tLoss: 0.014467\n",
      "Train Epoch: 7 [800/9840 (8%)]\tLoss: 0.000450\n",
      "Train Epoch: 7 [1200/9840 (12%)]\tLoss: 0.080107\n",
      "Train Epoch: 7 [1600/9840 (16%)]\tLoss: 0.094023\n",
      "Train Epoch: 7 [2000/9840 (20%)]\tLoss: 0.000142\n",
      "Train Epoch: 7 [2400/9840 (24%)]\tLoss: 0.119184\n",
      "Train Epoch: 7 [2800/9840 (28%)]\tLoss: 0.000146\n",
      "Train Epoch: 7 [3200/9840 (33%)]\tLoss: 0.034632\n",
      "Train Epoch: 7 [3600/9840 (37%)]\tLoss: 0.003894\n",
      "Train Epoch: 7 [4000/9840 (41%)]\tLoss: 0.118138\n",
      "Train Epoch: 7 [4400/9840 (45%)]\tLoss: 0.034148\n",
      "Train Epoch: 7 [4800/9840 (49%)]\tLoss: 0.396774\n",
      "Train Epoch: 7 [5200/9840 (53%)]\tLoss: 0.428601\n",
      "Train Epoch: 7 [5600/9840 (57%)]\tLoss: 0.057765\n",
      "Train Epoch: 7 [6000/9840 (61%)]\tLoss: 0.000001\n",
      "Train Epoch: 7 [6400/9840 (65%)]\tLoss: 0.825894\n",
      "Train Epoch: 7 [6800/9840 (69%)]\tLoss: 0.057965\n",
      "Train Epoch: 7 [7200/9840 (73%)]\tLoss: 0.108014\n",
      "Train Epoch: 7 [7600/9840 (77%)]\tLoss: 0.122787\n",
      "Train Epoch: 7 [8000/9840 (81%)]\tLoss: 0.000001\n",
      "Train Epoch: 7 [8400/9840 (85%)]\tLoss: 0.000387\n",
      "Train Epoch: 7 [8800/9840 (89%)]\tLoss: 0.005009\n",
      "Train Epoch: 7 [9200/9840 (93%)]\tLoss: 0.014272\n",
      "Train Epoch: 7 [9600/9840 (98%)]\tLoss: 0.288085\n",
      "\n",
      "Test set: Average loss: 3.9593, Accuracy: 177/2468 (7%)\n",
      "\n",
      "Train Epoch: 8 [0/9840 (0%)]\tLoss: 0.030950\n",
      "Train Epoch: 8 [400/9840 (4%)]\tLoss: 0.054698\n",
      "Train Epoch: 8 [800/9840 (8%)]\tLoss: 0.005370\n",
      "Train Epoch: 8 [1200/9840 (12%)]\tLoss: 0.003253\n",
      "Train Epoch: 8 [1600/9840 (16%)]\tLoss: 0.000112\n",
      "Train Epoch: 8 [2000/9840 (20%)]\tLoss: 0.000772\n",
      "Train Epoch: 8 [2400/9840 (24%)]\tLoss: 0.600557\n",
      "Train Epoch: 8 [2800/9840 (28%)]\tLoss: 0.119878\n",
      "Train Epoch: 8 [3200/9840 (33%)]\tLoss: 0.002328\n",
      "Train Epoch: 8 [3600/9840 (37%)]\tLoss: 0.002870\n",
      "Train Epoch: 8 [4000/9840 (41%)]\tLoss: 0.000156\n",
      "Train Epoch: 8 [4400/9840 (45%)]\tLoss: 0.000049\n",
      "Train Epoch: 8 [4800/9840 (49%)]\tLoss: 3.030005\n",
      "Train Epoch: 8 [5200/9840 (53%)]\tLoss: 0.001224\n",
      "Train Epoch: 8 [5600/9840 (57%)]\tLoss: 0.002414\n",
      "Train Epoch: 8 [6000/9840 (61%)]\tLoss: 0.135157\n",
      "Train Epoch: 8 [6400/9840 (65%)]\tLoss: 0.072763\n",
      "Train Epoch: 8 [6800/9840 (69%)]\tLoss: 1.889407\n",
      "Train Epoch: 8 [7200/9840 (73%)]\tLoss: 0.000075\n",
      "Train Epoch: 8 [7600/9840 (77%)]\tLoss: 0.137808\n",
      "Train Epoch: 8 [8000/9840 (81%)]\tLoss: 0.866388\n",
      "Train Epoch: 8 [8400/9840 (85%)]\tLoss: 0.048233\n",
      "Train Epoch: 8 [8800/9840 (89%)]\tLoss: 0.664804\n",
      "Train Epoch: 8 [9200/9840 (93%)]\tLoss: 0.006322\n",
      "Train Epoch: 8 [9600/9840 (98%)]\tLoss: 1.633465\n",
      "\n",
      "Test set: Average loss: 4.1692, Accuracy: 201/2468 (8%)\n",
      "\n",
      "Train Epoch: 9 [0/9840 (0%)]\tLoss: 0.021559\n",
      "Train Epoch: 9 [400/9840 (4%)]\tLoss: 0.453420\n",
      "Train Epoch: 9 [800/9840 (8%)]\tLoss: 0.014864\n",
      "Train Epoch: 9 [1200/9840 (12%)]\tLoss: 0.581477\n",
      "Train Epoch: 9 [1600/9840 (16%)]\tLoss: 0.000001\n",
      "Train Epoch: 9 [2000/9840 (20%)]\tLoss: 0.035530\n",
      "Train Epoch: 9 [2400/9840 (24%)]\tLoss: 0.000490\n",
      "Train Epoch: 9 [2800/9840 (28%)]\tLoss: 0.545708\n",
      "Train Epoch: 9 [3200/9840 (33%)]\tLoss: 0.000340\n",
      "Train Epoch: 9 [3600/9840 (37%)]\tLoss: 0.019742\n",
      "Train Epoch: 9 [4000/9840 (41%)]\tLoss: 0.000215\n",
      "Train Epoch: 9 [4400/9840 (45%)]\tLoss: 1.453248\n",
      "Train Epoch: 9 [4800/9840 (49%)]\tLoss: 0.018907\n",
      "Train Epoch: 9 [5200/9840 (53%)]\tLoss: 0.036440\n",
      "Train Epoch: 9 [5600/9840 (57%)]\tLoss: 0.011005\n",
      "Train Epoch: 9 [6000/9840 (61%)]\tLoss: 0.145800\n",
      "Train Epoch: 9 [6400/9840 (65%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [6800/9840 (69%)]\tLoss: 0.025544\n",
      "Train Epoch: 9 [7200/9840 (73%)]\tLoss: 0.021619\n",
      "Train Epoch: 9 [7600/9840 (77%)]\tLoss: 0.000030\n",
      "Train Epoch: 9 [8000/9840 (81%)]\tLoss: 0.047469\n",
      "Train Epoch: 9 [8400/9840 (85%)]\tLoss: 0.001717\n",
      "Train Epoch: 9 [8800/9840 (89%)]\tLoss: 1.425011\n",
      "Train Epoch: 9 [9200/9840 (93%)]\tLoss: 0.301414\n",
      "Train Epoch: 9 [9600/9840 (98%)]\tLoss: 0.032760\n",
      "\n",
      "Test set: Average loss: 4.9370, Accuracy: 194/2468 (8%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.long().cuda()\n",
    "        data, target = Variable(data), Variable(target[:,0])\n",
    "        \n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        ##print(\"huehue\",data.size())\n",
    "        data = data.transpose(2,1)\n",
    "        data = data.unsqueeze(-1)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "def test():\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    for (data, target) in test_loader:\n",
    "        data, target = data.cuda(), target.long().cuda()\n",
    "        data, target = Variable(data), Variable(target[:,0])\n",
    "        data = data.transpose(2,1)\n",
    "        data = data.unsqueeze(-1)\n",
    "        output = model(data)\n",
    "\n",
    "        # sum up batch loss\n",
    "        test_loss += criterion(output, target).data[0]\n",
    "        # get the index of the max log-probability\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        #print(pred.size())\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'\n",
    "          .format(test_loss, correct, len(test_loader.dataset),\n",
    "                  100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_step = 2048\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM (\n",
       "  (rnn): LSTM(3, 64, batch_first=True)\n",
       "  (out): Linear (64 -> 40)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size=3, hidden_size=64, num_layers=1, batch_first=True)\n",
    "        self.out = nn.Linear(64, 40)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r_out, (h_n, h_c) = self.rnn(x, None)\n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return F.log_softmax(out)\n",
    "        \n",
    "    \n",
    "model = LSTM()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)   # optimize all cnn parameters\n",
    "criterion = nn.NLLLoss()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/9840 (0%)]\tLoss: 3.710415\n",
      "Train Epoch: 0 [400/9840 (4%)]\tLoss: 3.680993\n",
      "Train Epoch: 0 [800/9840 (8%)]\tLoss: 3.583914\n",
      "Train Epoch: 0 [1200/9840 (12%)]\tLoss: 3.236986\n",
      "Train Epoch: 0 [1600/9840 (16%)]\tLoss: 3.318634\n",
      "Train Epoch: 0 [2000/9840 (20%)]\tLoss: 3.513550\n",
      "Train Epoch: 0 [2400/9840 (24%)]\tLoss: 4.066520\n",
      "Train Epoch: 0 [2800/9840 (28%)]\tLoss: 2.599865\n",
      "Train Epoch: 0 [3200/9840 (33%)]\tLoss: 2.493284\n",
      "Train Epoch: 0 [3600/9840 (37%)]\tLoss: 2.988086\n",
      "Train Epoch: 0 [4000/9840 (41%)]\tLoss: 2.910098\n",
      "Train Epoch: 0 [4400/9840 (45%)]\tLoss: 2.931051\n",
      "Train Epoch: 0 [4800/9840 (49%)]\tLoss: 2.433187\n",
      "Train Epoch: 0 [5200/9840 (53%)]\tLoss: 1.857828\n",
      "Train Epoch: 0 [5600/9840 (57%)]\tLoss: 3.723701\n",
      "Train Epoch: 0 [6000/9840 (61%)]\tLoss: 3.176331\n",
      "Train Epoch: 0 [6400/9840 (65%)]\tLoss: 2.354869\n",
      "Train Epoch: 0 [6800/9840 (69%)]\tLoss: 2.569828\n",
      "Train Epoch: 0 [7200/9840 (73%)]\tLoss: 2.638167\n",
      "Train Epoch: 0 [7600/9840 (77%)]\tLoss: 2.530609\n",
      "Train Epoch: 0 [8000/9840 (81%)]\tLoss: 2.514326\n",
      "Train Epoch: 0 [8400/9840 (85%)]\tLoss: 2.567731\n",
      "Train Epoch: 0 [8800/9840 (89%)]\tLoss: 1.868136\n",
      "Train Epoch: 0 [9200/9840 (93%)]\tLoss: 2.891818\n",
      "Train Epoch: 0 [9600/9840 (98%)]\tLoss: 2.200479\n",
      "\n",
      "Test set: Average loss: 0.6537, Accuracy: 578/2468 (23%)\n",
      "\n",
      "Train Epoch: 1 [0/9840 (0%)]\tLoss: 3.122776\n",
      "Train Epoch: 1 [400/9840 (4%)]\tLoss: 2.095708\n",
      "Train Epoch: 1 [800/9840 (8%)]\tLoss: 2.971859\n",
      "Train Epoch: 1 [1200/9840 (12%)]\tLoss: 3.205961\n",
      "Train Epoch: 1 [1600/9840 (16%)]\tLoss: 1.735864\n",
      "Train Epoch: 1 [2000/9840 (20%)]\tLoss: 1.861357\n",
      "Train Epoch: 1 [2400/9840 (24%)]\tLoss: 0.929954\n",
      "Train Epoch: 1 [2800/9840 (28%)]\tLoss: 2.473444\n",
      "Train Epoch: 1 [3200/9840 (33%)]\tLoss: 2.287755\n",
      "Train Epoch: 1 [3600/9840 (37%)]\tLoss: 2.658996\n",
      "Train Epoch: 1 [4000/9840 (41%)]\tLoss: 2.953175\n",
      "Train Epoch: 1 [4400/9840 (45%)]\tLoss: 1.778105\n",
      "Train Epoch: 1 [4800/9840 (49%)]\tLoss: 1.458012\n",
      "Train Epoch: 1 [5200/9840 (53%)]\tLoss: 1.859324\n",
      "Train Epoch: 1 [5600/9840 (57%)]\tLoss: 3.305872\n",
      "Train Epoch: 1 [6000/9840 (61%)]\tLoss: 2.104292\n",
      "Train Epoch: 1 [6400/9840 (65%)]\tLoss: 1.670657\n",
      "Train Epoch: 1 [6800/9840 (69%)]\tLoss: 2.456334\n",
      "Train Epoch: 1 [7200/9840 (73%)]\tLoss: 1.676484\n",
      "Train Epoch: 1 [7600/9840 (77%)]\tLoss: 2.056806\n",
      "Train Epoch: 1 [8000/9840 (81%)]\tLoss: 0.919594\n",
      "Train Epoch: 1 [8400/9840 (85%)]\tLoss: 1.250952\n",
      "Train Epoch: 1 [8800/9840 (89%)]\tLoss: 1.950279\n",
      "Train Epoch: 1 [9200/9840 (93%)]\tLoss: 2.089021\n",
      "Train Epoch: 1 [9600/9840 (98%)]\tLoss: 1.831378\n",
      "\n",
      "Test set: Average loss: 0.4864, Accuracy: 941/2468 (38%)\n",
      "\n",
      "Train Epoch: 2 [0/9840 (0%)]\tLoss: 2.698986\n",
      "Train Epoch: 2 [400/9840 (4%)]\tLoss: 2.341683\n",
      "Train Epoch: 2 [800/9840 (8%)]\tLoss: 2.700778\n",
      "Train Epoch: 2 [1200/9840 (12%)]\tLoss: 1.228403\n",
      "Train Epoch: 2 [1600/9840 (16%)]\tLoss: 1.404209\n",
      "Train Epoch: 2 [2000/9840 (20%)]\tLoss: 2.181923\n",
      "Train Epoch: 2 [2400/9840 (24%)]\tLoss: 0.584850\n",
      "Train Epoch: 2 [2800/9840 (28%)]\tLoss: 2.549317\n",
      "Train Epoch: 2 [3200/9840 (33%)]\tLoss: 3.718342\n",
      "Train Epoch: 2 [3600/9840 (37%)]\tLoss: 2.710080\n",
      "Train Epoch: 2 [4000/9840 (41%)]\tLoss: 2.278351\n",
      "Train Epoch: 2 [4400/9840 (45%)]\tLoss: 3.288534\n",
      "Train Epoch: 2 [4800/9840 (49%)]\tLoss: 2.186642\n",
      "Train Epoch: 2 [5200/9840 (53%)]\tLoss: 2.306821\n",
      "Train Epoch: 2 [5600/9840 (57%)]\tLoss: 3.025812\n",
      "Train Epoch: 2 [6000/9840 (61%)]\tLoss: 2.345599\n",
      "Train Epoch: 2 [6400/9840 (65%)]\tLoss: 3.563036\n",
      "Train Epoch: 2 [6800/9840 (69%)]\tLoss: 2.137435\n",
      "Train Epoch: 2 [7200/9840 (73%)]\tLoss: 2.141102\n",
      "Train Epoch: 2 [7600/9840 (77%)]\tLoss: 2.367947\n",
      "Train Epoch: 2 [8000/9840 (81%)]\tLoss: 2.968701\n",
      "Train Epoch: 2 [8400/9840 (85%)]\tLoss: 3.066408\n",
      "Train Epoch: 2 [8800/9840 (89%)]\tLoss: 2.161100\n",
      "Train Epoch: 2 [9200/9840 (93%)]\tLoss: 1.555487\n",
      "Train Epoch: 2 [9600/9840 (98%)]\tLoss: 2.693460\n",
      "\n",
      "Test set: Average loss: 0.6285, Accuracy: 697/2468 (28%)\n",
      "\n",
      "Train Epoch: 3 [0/9840 (0%)]\tLoss: 1.827100\n",
      "Train Epoch: 3 [400/9840 (4%)]\tLoss: 4.104517\n",
      "Train Epoch: 3 [800/9840 (8%)]\tLoss: 1.306008\n",
      "Train Epoch: 3 [1200/9840 (12%)]\tLoss: 2.127055\n",
      "Train Epoch: 3 [1600/9840 (16%)]\tLoss: 2.459862\n",
      "Train Epoch: 3 [2000/9840 (20%)]\tLoss: 2.960475\n",
      "Train Epoch: 3 [2400/9840 (24%)]\tLoss: 2.854628\n",
      "Train Epoch: 3 [2800/9840 (28%)]\tLoss: 2.287960\n",
      "Train Epoch: 3 [3200/9840 (33%)]\tLoss: 2.702871\n",
      "Train Epoch: 3 [3600/9840 (37%)]\tLoss: 2.447825\n",
      "Train Epoch: 3 [4000/9840 (41%)]\tLoss: 2.114807\n",
      "Train Epoch: 3 [4400/9840 (45%)]\tLoss: 3.215996\n",
      "Train Epoch: 3 [4800/9840 (49%)]\tLoss: 1.109339\n",
      "Train Epoch: 3 [5200/9840 (53%)]\tLoss: 3.270292\n",
      "Train Epoch: 3 [5600/9840 (57%)]\tLoss: 2.974523\n",
      "Train Epoch: 3 [6000/9840 (61%)]\tLoss: 2.123827\n",
      "Train Epoch: 3 [6400/9840 (65%)]\tLoss: 1.828563\n",
      "Train Epoch: 3 [6800/9840 (69%)]\tLoss: 2.231901\n",
      "Train Epoch: 3 [7200/9840 (73%)]\tLoss: 3.387448\n",
      "Train Epoch: 3 [7600/9840 (77%)]\tLoss: 0.914380\n",
      "Train Epoch: 3 [8000/9840 (81%)]\tLoss: 2.872948\n",
      "Train Epoch: 3 [8400/9840 (85%)]\tLoss: 2.372087\n",
      "Train Epoch: 3 [8800/9840 (89%)]\tLoss: 2.331889\n",
      "Train Epoch: 3 [9200/9840 (93%)]\tLoss: 0.807865\n",
      "Train Epoch: 3 [9600/9840 (98%)]\tLoss: 1.883619\n",
      "\n",
      "Test set: Average loss: 0.6078, Accuracy: 740/2468 (30%)\n",
      "\n",
      "Train Epoch: 4 [0/9840 (0%)]\tLoss: 2.010170\n",
      "Train Epoch: 4 [400/9840 (4%)]\tLoss: 1.761695\n",
      "Train Epoch: 4 [800/9840 (8%)]\tLoss: 2.342198\n",
      "Train Epoch: 4 [1200/9840 (12%)]\tLoss: 2.501784\n",
      "Train Epoch: 4 [1600/9840 (16%)]\tLoss: 1.989301\n",
      "Train Epoch: 4 [2000/9840 (20%)]\tLoss: 1.600447\n",
      "Train Epoch: 4 [2400/9840 (24%)]\tLoss: 1.859843\n",
      "Train Epoch: 4 [2800/9840 (28%)]\tLoss: 1.599743\n",
      "Train Epoch: 4 [3200/9840 (33%)]\tLoss: 2.410343\n",
      "Train Epoch: 4 [3600/9840 (37%)]\tLoss: 3.477241\n",
      "Train Epoch: 4 [4000/9840 (41%)]\tLoss: 2.393615\n",
      "Train Epoch: 4 [4400/9840 (45%)]\tLoss: 3.710113\n",
      "Train Epoch: 4 [4800/9840 (49%)]\tLoss: 1.821848\n",
      "Train Epoch: 4 [5200/9840 (53%)]\tLoss: 2.116510\n",
      "Train Epoch: 4 [5600/9840 (57%)]\tLoss: 1.670255\n",
      "Train Epoch: 4 [6000/9840 (61%)]\tLoss: 2.404164\n",
      "Train Epoch: 4 [6400/9840 (65%)]\tLoss: 2.665862\n",
      "Train Epoch: 4 [6800/9840 (69%)]\tLoss: 1.904288\n",
      "Train Epoch: 4 [7200/9840 (73%)]\tLoss: 2.539305\n",
      "Train Epoch: 4 [7600/9840 (77%)]\tLoss: 2.029421\n",
      "Train Epoch: 4 [8000/9840 (81%)]\tLoss: 1.333324\n",
      "Train Epoch: 4 [8400/9840 (85%)]\tLoss: 2.245947\n",
      "Train Epoch: 4 [8800/9840 (89%)]\tLoss: 1.865675\n",
      "Train Epoch: 4 [9200/9840 (93%)]\tLoss: 2.135742\n",
      "Train Epoch: 4 [9600/9840 (98%)]\tLoss: 1.293257\n",
      "\n",
      "Test set: Average loss: 0.5985, Accuracy: 761/2468 (31%)\n",
      "\n",
      "Train Epoch: 5 [0/9840 (0%)]\tLoss: 1.282178\n",
      "Train Epoch: 5 [400/9840 (4%)]\tLoss: 2.208113\n",
      "Train Epoch: 5 [800/9840 (8%)]\tLoss: 2.078395\n",
      "Train Epoch: 5 [1200/9840 (12%)]\tLoss: 2.115404\n",
      "Train Epoch: 5 [1600/9840 (16%)]\tLoss: 3.511914\n",
      "Train Epoch: 5 [2000/9840 (20%)]\tLoss: 2.707716\n",
      "Train Epoch: 5 [2400/9840 (24%)]\tLoss: 2.760960\n",
      "Train Epoch: 5 [2800/9840 (28%)]\tLoss: 3.247578\n",
      "Train Epoch: 5 [3200/9840 (33%)]\tLoss: 2.151618\n",
      "Train Epoch: 5 [3600/9840 (37%)]\tLoss: 1.964882\n",
      "Train Epoch: 5 [4000/9840 (41%)]\tLoss: 3.826498\n",
      "Train Epoch: 5 [4400/9840 (45%)]\tLoss: 2.560539\n",
      "Train Epoch: 5 [4800/9840 (49%)]\tLoss: 2.032029\n",
      "Train Epoch: 5 [5200/9840 (53%)]\tLoss: 3.058692\n",
      "Train Epoch: 5 [5600/9840 (57%)]\tLoss: 1.859829\n",
      "Train Epoch: 5 [6000/9840 (61%)]\tLoss: 2.308237\n",
      "Train Epoch: 5 [6400/9840 (65%)]\tLoss: 3.098623\n",
      "Train Epoch: 5 [6800/9840 (69%)]\tLoss: 2.449922\n",
      "Train Epoch: 5 [7200/9840 (73%)]\tLoss: 1.609068\n",
      "Train Epoch: 5 [7600/9840 (77%)]\tLoss: 1.851973\n",
      "Train Epoch: 5 [8000/9840 (81%)]\tLoss: 2.492771\n",
      "Train Epoch: 5 [8400/9840 (85%)]\tLoss: 2.148572\n",
      "Train Epoch: 5 [8800/9840 (89%)]\tLoss: 2.857072\n",
      "Train Epoch: 5 [9200/9840 (93%)]\tLoss: 2.227253\n",
      "Train Epoch: 5 [9600/9840 (98%)]\tLoss: 1.866922\n",
      "\n",
      "Test set: Average loss: 0.5790, Accuracy: 785/2468 (32%)\n",
      "\n",
      "Train Epoch: 6 [0/9840 (0%)]\tLoss: 2.358742\n",
      "Train Epoch: 6 [400/9840 (4%)]\tLoss: 1.744653\n",
      "Train Epoch: 6 [800/9840 (8%)]\tLoss: 1.826258\n",
      "Train Epoch: 6 [1200/9840 (12%)]\tLoss: 2.061321\n",
      "Train Epoch: 6 [1600/9840 (16%)]\tLoss: 3.302157\n",
      "Train Epoch: 6 [2000/9840 (20%)]\tLoss: 1.736300\n",
      "Train Epoch: 6 [2400/9840 (24%)]\tLoss: 2.282803\n",
      "Train Epoch: 6 [2800/9840 (28%)]\tLoss: 2.166819\n",
      "Train Epoch: 6 [3200/9840 (33%)]\tLoss: 1.616066\n",
      "Train Epoch: 6 [3600/9840 (37%)]\tLoss: 2.157802\n",
      "Train Epoch: 6 [4000/9840 (41%)]\tLoss: 5.467546\n",
      "Train Epoch: 6 [4400/9840 (45%)]\tLoss: 2.867323\n",
      "Train Epoch: 6 [4800/9840 (49%)]\tLoss: 1.521116\n",
      "Train Epoch: 6 [5200/9840 (53%)]\tLoss: 1.381144\n",
      "Train Epoch: 6 [5600/9840 (57%)]\tLoss: 2.418040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [6000/9840 (61%)]\tLoss: 2.673280\n",
      "Train Epoch: 6 [6400/9840 (65%)]\tLoss: 1.609842\n",
      "Train Epoch: 6 [6800/9840 (69%)]\tLoss: 2.855297\n",
      "Train Epoch: 6 [7200/9840 (73%)]\tLoss: 1.632500\n",
      "Train Epoch: 6 [7600/9840 (77%)]\tLoss: 2.266833\n",
      "Train Epoch: 6 [8000/9840 (81%)]\tLoss: 2.625758\n",
      "Train Epoch: 6 [8400/9840 (85%)]\tLoss: 1.960229\n",
      "Train Epoch: 6 [8800/9840 (89%)]\tLoss: 1.864625\n",
      "Train Epoch: 6 [9200/9840 (93%)]\tLoss: 3.488738\n",
      "Train Epoch: 6 [9600/9840 (98%)]\tLoss: 1.690265\n",
      "\n",
      "Test set: Average loss: 0.5829, Accuracy: 771/2468 (31%)\n",
      "\n",
      "Train Epoch: 7 [0/9840 (0%)]\tLoss: 1.411681\n",
      "Train Epoch: 7 [400/9840 (4%)]\tLoss: 1.761628\n",
      "Train Epoch: 7 [800/9840 (8%)]\tLoss: 0.779688\n",
      "Train Epoch: 7 [1200/9840 (12%)]\tLoss: 1.736087\n",
      "Train Epoch: 7 [1600/9840 (16%)]\tLoss: 1.861927\n",
      "Train Epoch: 7 [2000/9840 (20%)]\tLoss: 2.562926\n",
      "Train Epoch: 7 [2400/9840 (24%)]\tLoss: 2.617288\n",
      "Train Epoch: 7 [2800/9840 (28%)]\tLoss: 2.920453\n",
      "Train Epoch: 7 [3200/9840 (33%)]\tLoss: 2.274041\n",
      "Train Epoch: 7 [3600/9840 (37%)]\tLoss: 2.875514\n",
      "Train Epoch: 7 [4000/9840 (41%)]\tLoss: 2.081536\n",
      "Train Epoch: 7 [4400/9840 (45%)]\tLoss: 2.660299\n",
      "Train Epoch: 7 [4800/9840 (49%)]\tLoss: 3.082191\n",
      "Train Epoch: 7 [5200/9840 (53%)]\tLoss: 3.273811\n",
      "Train Epoch: 7 [5600/9840 (57%)]\tLoss: 2.654490\n",
      "Train Epoch: 7 [6000/9840 (61%)]\tLoss: 3.184577\n",
      "Train Epoch: 7 [6400/9840 (65%)]\tLoss: 3.024699\n",
      "Train Epoch: 7 [6800/9840 (69%)]\tLoss: 3.201207\n",
      "Train Epoch: 7 [7200/9840 (73%)]\tLoss: 2.199445\n",
      "Train Epoch: 7 [7600/9840 (77%)]\tLoss: 2.607913\n",
      "Train Epoch: 7 [8000/9840 (81%)]\tLoss: 3.595191\n",
      "Train Epoch: 7 [8400/9840 (85%)]\tLoss: 2.075035\n",
      "Train Epoch: 7 [8800/9840 (89%)]\tLoss: 4.886503\n",
      "Train Epoch: 7 [9200/9840 (93%)]\tLoss: 1.657668\n",
      "Train Epoch: 7 [9600/9840 (98%)]\tLoss: 3.217632\n",
      "\n",
      "Test set: Average loss: 0.7571, Accuracy: 337/2468 (14%)\n",
      "\n",
      "Train Epoch: 8 [0/9840 (0%)]\tLoss: 3.423094\n",
      "Train Epoch: 8 [400/9840 (4%)]\tLoss: 3.637381\n",
      "Train Epoch: 8 [800/9840 (8%)]\tLoss: 3.188504\n",
      "Train Epoch: 8 [1200/9840 (12%)]\tLoss: 2.397306\n",
      "Train Epoch: 8 [1600/9840 (16%)]\tLoss: 1.950858\n",
      "Train Epoch: 8 [2000/9840 (20%)]\tLoss: 3.022089\n",
      "Train Epoch: 8 [2400/9840 (24%)]\tLoss: 1.965888\n",
      "Train Epoch: 8 [2800/9840 (28%)]\tLoss: 2.439322\n",
      "Train Epoch: 8 [3200/9840 (33%)]\tLoss: 2.065089\n",
      "Train Epoch: 8 [3600/9840 (37%)]\tLoss: 2.360820\n",
      "Train Epoch: 8 [4000/9840 (41%)]\tLoss: 2.803564\n",
      "Train Epoch: 8 [4400/9840 (45%)]\tLoss: 2.304320\n",
      "Train Epoch: 8 [4800/9840 (49%)]\tLoss: 2.277420\n",
      "Train Epoch: 8 [5200/9840 (53%)]\tLoss: 2.573924\n",
      "Train Epoch: 8 [5600/9840 (57%)]\tLoss: 3.190134\n",
      "Train Epoch: 8 [6000/9840 (61%)]\tLoss: 3.564670\n",
      "Train Epoch: 8 [6400/9840 (65%)]\tLoss: 2.783492\n",
      "Train Epoch: 8 [6800/9840 (69%)]\tLoss: 2.427574\n",
      "Train Epoch: 8 [7200/9840 (73%)]\tLoss: 2.326292\n",
      "Train Epoch: 8 [7600/9840 (77%)]\tLoss: 2.375681\n",
      "Train Epoch: 8 [8000/9840 (81%)]\tLoss: 1.994858\n",
      "Train Epoch: 8 [8400/9840 (85%)]\tLoss: 2.697648\n",
      "Train Epoch: 8 [8800/9840 (89%)]\tLoss: 3.076374\n",
      "Train Epoch: 8 [9200/9840 (93%)]\tLoss: 2.694167\n",
      "Train Epoch: 8 [9600/9840 (98%)]\tLoss: 2.850928\n",
      "\n",
      "Test set: Average loss: 0.7226, Accuracy: 407/2468 (16%)\n",
      "\n",
      "Train Epoch: 9 [0/9840 (0%)]\tLoss: 3.047924\n",
      "Train Epoch: 9 [400/9840 (4%)]\tLoss: 3.127194\n",
      "Train Epoch: 9 [800/9840 (8%)]\tLoss: 1.792978\n",
      "Train Epoch: 9 [1200/9840 (12%)]\tLoss: 2.982589\n",
      "Train Epoch: 9 [1600/9840 (16%)]\tLoss: 1.615645\n",
      "Train Epoch: 9 [2000/9840 (20%)]\tLoss: 3.432795\n",
      "Train Epoch: 9 [2400/9840 (24%)]\tLoss: 3.255498\n",
      "Train Epoch: 9 [2800/9840 (28%)]\tLoss: 2.705926\n",
      "Train Epoch: 9 [3200/9840 (33%)]\tLoss: 3.474923\n",
      "Train Epoch: 9 [3600/9840 (37%)]\tLoss: 2.008337\n",
      "Train Epoch: 9 [4000/9840 (41%)]\tLoss: 3.291356\n",
      "Train Epoch: 9 [4400/9840 (45%)]\tLoss: 2.783389\n",
      "Train Epoch: 9 [4800/9840 (49%)]\tLoss: 2.775328\n",
      "Train Epoch: 9 [5200/9840 (53%)]\tLoss: 3.133580\n",
      "Train Epoch: 9 [5600/9840 (57%)]\tLoss: 2.299834\n",
      "Train Epoch: 9 [6000/9840 (61%)]\tLoss: 3.944486\n",
      "Train Epoch: 9 [6400/9840 (65%)]\tLoss: 1.856549\n",
      "Train Epoch: 9 [6800/9840 (69%)]\tLoss: 3.054642\n",
      "Train Epoch: 9 [7200/9840 (73%)]\tLoss: 3.647125\n",
      "Train Epoch: 9 [7600/9840 (77%)]\tLoss: 2.093240\n",
      "Train Epoch: 9 [8000/9840 (81%)]\tLoss: 2.596953\n",
      "Train Epoch: 9 [8400/9840 (85%)]\tLoss: 2.236518\n",
      "Train Epoch: 9 [8800/9840 (89%)]\tLoss: 2.765808\n",
      "Train Epoch: 9 [9200/9840 (93%)]\tLoss: 2.708870\n",
      "Train Epoch: 9 [9600/9840 (98%)]\tLoss: 3.040422\n",
      "\n",
      "Test set: Average loss: 0.7026, Accuracy: 440/2468 (18%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train(epoch):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.long().cuda()\n",
    "        data, target = Variable(data), Variable(target[:,0])\n",
    "        \n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "def test():\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    for (data, target) in test_loader:\n",
    "        data, target = data.cuda(), target.long().cuda()\n",
    "        data, target = Variable(data), Variable(target[:,0])\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        # sum up batch loss\n",
    "        test_loss += criterion(output, target).data[0]\n",
    "        # get the index of the max log-probability\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        #print(pred.size())\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'\n",
    "          .format(test_loss, correct, len(test_loader.dataset),\n",
    "                  100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "for epoch in range(10):\n",
    "    train(epoch)\n",
    "    test()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM2 (\n",
       "  (conv1): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
       "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (rnn): LSTM(64, 128, batch_first=True)\n",
       "  (out): Linear (128 -> 40)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LSTM2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM2, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv1d(3, 64, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.rnn = nn.LSTM(input_size=64, hidden_size=128, num_layers=1, batch_first=True)\n",
    "        self.out = nn.Linear(128, 40)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(2,1)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = x.transpose(2,1)\n",
    "        r_out, (h_n, h_c) = self.rnn(x, None)\n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return F.log_softmax(out)\n",
    "        \n",
    "    \n",
    "model = LSTM2()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.NLLLoss()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/9840 (0%)]\tLoss: 3.707300\n",
      "Train Epoch: 0 [400/9840 (4%)]\tLoss: 3.416899\n",
      "Train Epoch: 0 [800/9840 (8%)]\tLoss: 2.547228\n",
      "Train Epoch: 0 [1200/9840 (12%)]\tLoss: 2.810785\n",
      "Train Epoch: 0 [1600/9840 (16%)]\tLoss: 2.279734\n",
      "Train Epoch: 0 [2000/9840 (20%)]\tLoss: 4.605933\n",
      "Train Epoch: 0 [2400/9840 (24%)]\tLoss: 3.338383\n",
      "Train Epoch: 0 [2800/9840 (28%)]\tLoss: 3.449450\n",
      "Train Epoch: 0 [3200/9840 (33%)]\tLoss: 2.866663\n",
      "Train Epoch: 0 [3600/9840 (37%)]\tLoss: 3.021249\n",
      "Train Epoch: 0 [4000/9840 (41%)]\tLoss: 2.561337\n",
      "Train Epoch: 0 [4400/9840 (45%)]\tLoss: 0.869653\n",
      "Train Epoch: 0 [4800/9840 (49%)]\tLoss: 2.602786\n",
      "Train Epoch: 0 [5200/9840 (53%)]\tLoss: 3.428615\n",
      "Train Epoch: 0 [5600/9840 (57%)]\tLoss: 3.543193\n",
      "Train Epoch: 0 [6000/9840 (61%)]\tLoss: 1.435865\n",
      "Train Epoch: 0 [6400/9840 (65%)]\tLoss: 3.358827\n",
      "Train Epoch: 0 [6800/9840 (69%)]\tLoss: 3.298955\n",
      "Train Epoch: 0 [7200/9840 (73%)]\tLoss: 1.616557\n",
      "Train Epoch: 0 [7600/9840 (77%)]\tLoss: 3.492884\n",
      "Train Epoch: 0 [8000/9840 (81%)]\tLoss: 2.899663\n",
      "Train Epoch: 0 [8400/9840 (85%)]\tLoss: 2.350036\n",
      "Train Epoch: 0 [8800/9840 (89%)]\tLoss: 1.804682\n",
      "Train Epoch: 0 [9200/9840 (93%)]\tLoss: 1.385787\n",
      "Train Epoch: 0 [9600/9840 (98%)]\tLoss: 2.806057\n",
      "\n",
      "Test set: Average loss: 0.6499, Accuracy: 614/2468 (25%)\n",
      "\n",
      "Train Epoch: 1 [0/9840 (0%)]\tLoss: 1.542828\n",
      "Train Epoch: 1 [400/9840 (4%)]\tLoss: 1.415522\n",
      "Train Epoch: 1 [800/9840 (8%)]\tLoss: 0.768089\n",
      "Train Epoch: 1 [1200/9840 (12%)]\tLoss: 1.662533\n",
      "Train Epoch: 1 [1600/9840 (16%)]\tLoss: 2.683859\n",
      "Train Epoch: 1 [2000/9840 (20%)]\tLoss: 2.252082\n",
      "Train Epoch: 1 [2400/9840 (24%)]\tLoss: 1.822506\n",
      "Train Epoch: 1 [2800/9840 (28%)]\tLoss: 1.107892\n",
      "Train Epoch: 1 [3200/9840 (33%)]\tLoss: 1.321957\n",
      "Train Epoch: 1 [3600/9840 (37%)]\tLoss: 1.927244\n",
      "Train Epoch: 1 [4000/9840 (41%)]\tLoss: 2.543661\n",
      "Train Epoch: 1 [4400/9840 (45%)]\tLoss: 2.872970\n",
      "Train Epoch: 1 [4800/9840 (49%)]\tLoss: 1.242987\n",
      "Train Epoch: 1 [5200/9840 (53%)]\tLoss: 1.521046\n",
      "Train Epoch: 1 [5600/9840 (57%)]\tLoss: 3.545077\n",
      "Train Epoch: 1 [6000/9840 (61%)]\tLoss: 1.887190\n",
      "Train Epoch: 1 [6400/9840 (65%)]\tLoss: 4.284704\n",
      "Train Epoch: 1 [6800/9840 (69%)]\tLoss: 1.587757\n",
      "Train Epoch: 1 [7200/9840 (73%)]\tLoss: 2.881113\n",
      "Train Epoch: 1 [7600/9840 (77%)]\tLoss: 1.363983\n",
      "Train Epoch: 1 [8000/9840 (81%)]\tLoss: 1.680394\n",
      "Train Epoch: 1 [8400/9840 (85%)]\tLoss: 0.931978\n",
      "Train Epoch: 1 [8800/9840 (89%)]\tLoss: 2.326969\n",
      "Train Epoch: 1 [9200/9840 (93%)]\tLoss: 4.166888\n",
      "Train Epoch: 1 [9600/9840 (98%)]\tLoss: 0.381857\n",
      "\n",
      "Test set: Average loss: 0.4835, Accuracy: 1139/2468 (46%)\n",
      "\n",
      "Train Epoch: 2 [0/9840 (0%)]\tLoss: 1.665495\n",
      "Train Epoch: 2 [400/9840 (4%)]\tLoss: 2.002863\n",
      "Train Epoch: 2 [800/9840 (8%)]\tLoss: 0.796423\n",
      "Train Epoch: 2 [1200/9840 (12%)]\tLoss: 1.950508\n",
      "Train Epoch: 2 [1600/9840 (16%)]\tLoss: 1.687941\n",
      "Train Epoch: 2 [2000/9840 (20%)]\tLoss: 2.402563\n",
      "Train Epoch: 2 [2400/9840 (24%)]\tLoss: 3.039620\n",
      "Train Epoch: 2 [2800/9840 (28%)]\tLoss: 3.420200\n",
      "Train Epoch: 2 [3200/9840 (33%)]\tLoss: 2.123143\n",
      "Train Epoch: 2 [3600/9840 (37%)]\tLoss: 3.889968\n",
      "Train Epoch: 2 [4000/9840 (41%)]\tLoss: 1.849920\n",
      "Train Epoch: 2 [4400/9840 (45%)]\tLoss: 3.168936\n",
      "Train Epoch: 2 [4800/9840 (49%)]\tLoss: 2.931703\n",
      "Train Epoch: 2 [5200/9840 (53%)]\tLoss: 1.313951\n",
      "Train Epoch: 2 [5600/9840 (57%)]\tLoss: 0.630940\n",
      "Train Epoch: 2 [6000/9840 (61%)]\tLoss: 1.198259\n",
      "Train Epoch: 2 [6400/9840 (65%)]\tLoss: 3.505008\n",
      "Train Epoch: 2 [6800/9840 (69%)]\tLoss: 1.447817\n",
      "Train Epoch: 2 [7200/9840 (73%)]\tLoss: 0.440043\n",
      "Train Epoch: 2 [7600/9840 (77%)]\tLoss: 2.007213\n",
      "Train Epoch: 2 [8000/9840 (81%)]\tLoss: 2.151051\n",
      "Train Epoch: 2 [8400/9840 (85%)]\tLoss: 2.055920\n",
      "Train Epoch: 2 [8800/9840 (89%)]\tLoss: 1.087755\n",
      "Train Epoch: 2 [9200/9840 (93%)]\tLoss: 0.429868\n",
      "Train Epoch: 2 [9600/9840 (98%)]\tLoss: 2.674994\n",
      "\n",
      "Test set: Average loss: 0.4633, Accuracy: 1180/2468 (48%)\n",
      "\n",
      "Train Epoch: 3 [0/9840 (0%)]\tLoss: 1.429876\n",
      "Train Epoch: 3 [400/9840 (4%)]\tLoss: 0.932893\n",
      "Train Epoch: 3 [800/9840 (8%)]\tLoss: 1.161933\n",
      "Train Epoch: 3 [1200/9840 (12%)]\tLoss: 1.050200\n",
      "Train Epoch: 3 [1600/9840 (16%)]\tLoss: 1.812539\n",
      "Train Epoch: 3 [2000/9840 (20%)]\tLoss: 2.129634\n",
      "Train Epoch: 3 [2400/9840 (24%)]\tLoss: 2.126780\n",
      "Train Epoch: 3 [2800/9840 (28%)]\tLoss: 0.833570\n",
      "Train Epoch: 3 [3200/9840 (33%)]\tLoss: 0.443901\n",
      "Train Epoch: 3 [3600/9840 (37%)]\tLoss: 2.749547\n",
      "Train Epoch: 3 [4000/9840 (41%)]\tLoss: 1.206934\n",
      "Train Epoch: 3 [4400/9840 (45%)]\tLoss: 0.065016\n",
      "Train Epoch: 3 [4800/9840 (49%)]\tLoss: 1.978082\n",
      "Train Epoch: 3 [5200/9840 (53%)]\tLoss: 0.953240\n",
      "Train Epoch: 3 [5600/9840 (57%)]\tLoss: 1.748375\n",
      "Train Epoch: 3 [6000/9840 (61%)]\tLoss: 0.925562\n",
      "Train Epoch: 3 [6400/9840 (65%)]\tLoss: 3.300915\n",
      "Train Epoch: 3 [6800/9840 (69%)]\tLoss: 3.105612\n",
      "Train Epoch: 3 [7200/9840 (73%)]\tLoss: 1.086650\n",
      "Train Epoch: 3 [7600/9840 (77%)]\tLoss: 0.543545\n",
      "Train Epoch: 3 [8000/9840 (81%)]\tLoss: 1.398879\n",
      "Train Epoch: 3 [8400/9840 (85%)]\tLoss: 0.598680\n",
      "Train Epoch: 3 [8800/9840 (89%)]\tLoss: 2.444701\n",
      "Train Epoch: 3 [9200/9840 (93%)]\tLoss: 2.604703\n",
      "Train Epoch: 3 [9600/9840 (98%)]\tLoss: 1.768767\n",
      "\n",
      "Test set: Average loss: 0.4696, Accuracy: 1168/2468 (47%)\n",
      "\n",
      "Train Epoch: 4 [0/9840 (0%)]\tLoss: 0.771526\n",
      "Train Epoch: 4 [400/9840 (4%)]\tLoss: 2.271813\n",
      "Train Epoch: 4 [800/9840 (8%)]\tLoss: 0.434524\n",
      "Train Epoch: 4 [1200/9840 (12%)]\tLoss: 1.729899\n",
      "Train Epoch: 4 [1600/9840 (16%)]\tLoss: 3.277498\n",
      "Train Epoch: 4 [2000/9840 (20%)]\tLoss: 2.913263\n",
      "Train Epoch: 4 [2400/9840 (24%)]\tLoss: 0.811433\n",
      "Train Epoch: 4 [2800/9840 (28%)]\tLoss: 0.839897\n",
      "Train Epoch: 4 [3200/9840 (33%)]\tLoss: 1.144814\n",
      "Train Epoch: 4 [3600/9840 (37%)]\tLoss: 0.211235\n",
      "Train Epoch: 4 [4000/9840 (41%)]\tLoss: 1.040945\n",
      "Train Epoch: 4 [4400/9840 (45%)]\tLoss: 3.566014\n",
      "Train Epoch: 4 [4800/9840 (49%)]\tLoss: 1.264957\n",
      "Train Epoch: 4 [5200/9840 (53%)]\tLoss: 0.683333\n",
      "Train Epoch: 4 [5600/9840 (57%)]\tLoss: 0.194885\n",
      "Train Epoch: 4 [6000/9840 (61%)]\tLoss: 3.433679\n",
      "Train Epoch: 4 [6400/9840 (65%)]\tLoss: 1.396086\n",
      "Train Epoch: 4 [6800/9840 (69%)]\tLoss: 1.126465\n",
      "Train Epoch: 4 [7200/9840 (73%)]\tLoss: 2.211370\n",
      "Train Epoch: 4 [7600/9840 (77%)]\tLoss: 0.710586\n",
      "Train Epoch: 4 [8000/9840 (81%)]\tLoss: 2.575464\n",
      "Train Epoch: 4 [8400/9840 (85%)]\tLoss: 1.259248\n",
      "Train Epoch: 4 [8800/9840 (89%)]\tLoss: 0.171537\n",
      "Train Epoch: 4 [9200/9840 (93%)]\tLoss: 1.664928\n",
      "Train Epoch: 4 [9600/9840 (98%)]\tLoss: 0.994426\n",
      "\n",
      "Test set: Average loss: 0.3716, Accuracy: 1440/2468 (58%)\n",
      "\n",
      "Train Epoch: 5 [0/9840 (0%)]\tLoss: 0.476806\n",
      "Train Epoch: 5 [400/9840 (4%)]\tLoss: 2.197284\n",
      "Train Epoch: 5 [800/9840 (8%)]\tLoss: 0.588342\n",
      "Train Epoch: 5 [1200/9840 (12%)]\tLoss: 1.183865\n",
      "Train Epoch: 5 [1600/9840 (16%)]\tLoss: 0.525045\n",
      "Train Epoch: 5 [2000/9840 (20%)]\tLoss: 2.299746\n",
      "Train Epoch: 5 [2400/9840 (24%)]\tLoss: 1.641380\n",
      "Train Epoch: 5 [2800/9840 (28%)]\tLoss: 1.078769\n",
      "Train Epoch: 5 [3200/9840 (33%)]\tLoss: 3.497872\n",
      "Train Epoch: 5 [3600/9840 (37%)]\tLoss: 1.801535\n",
      "Train Epoch: 5 [4000/9840 (41%)]\tLoss: 0.261947\n",
      "Train Epoch: 5 [4400/9840 (45%)]\tLoss: 0.473405\n",
      "Train Epoch: 5 [4800/9840 (49%)]\tLoss: 0.773411\n",
      "Train Epoch: 5 [5200/9840 (53%)]\tLoss: 0.875909\n",
      "Train Epoch: 5 [5600/9840 (57%)]\tLoss: 3.166915\n",
      "Train Epoch: 5 [6000/9840 (61%)]\tLoss: 0.224823\n",
      "Train Epoch: 5 [6400/9840 (65%)]\tLoss: 2.692177\n",
      "Train Epoch: 5 [6800/9840 (69%)]\tLoss: 1.245573\n",
      "Train Epoch: 5 [7200/9840 (73%)]\tLoss: 2.697517\n",
      "Train Epoch: 5 [7600/9840 (77%)]\tLoss: 2.134078\n",
      "Train Epoch: 5 [8000/9840 (81%)]\tLoss: 0.608667\n",
      "Train Epoch: 5 [8400/9840 (85%)]\tLoss: 1.859190\n",
      "Train Epoch: 5 [8800/9840 (89%)]\tLoss: 0.847381\n",
      "Train Epoch: 5 [9200/9840 (93%)]\tLoss: 0.651017\n",
      "Train Epoch: 5 [9600/9840 (98%)]\tLoss: 1.104907\n",
      "\n",
      "Test set: Average loss: 0.3620, Accuracy: 1460/2468 (59%)\n",
      "\n",
      "Train Epoch: 6 [0/9840 (0%)]\tLoss: 0.526052\n",
      "Train Epoch: 6 [400/9840 (4%)]\tLoss: 1.346515\n",
      "Train Epoch: 6 [800/9840 (8%)]\tLoss: 1.087350\n",
      "Train Epoch: 6 [1200/9840 (12%)]\tLoss: 4.604789\n",
      "Train Epoch: 6 [1600/9840 (16%)]\tLoss: 0.561037\n",
      "Train Epoch: 6 [2000/9840 (20%)]\tLoss: 2.929621\n",
      "Train Epoch: 6 [2400/9840 (24%)]\tLoss: 1.932279\n",
      "Train Epoch: 6 [2800/9840 (28%)]\tLoss: 2.419311\n",
      "Train Epoch: 6 [3200/9840 (33%)]\tLoss: 0.669493\n",
      "Train Epoch: 6 [3600/9840 (37%)]\tLoss: 0.656535\n",
      "Train Epoch: 6 [4000/9840 (41%)]\tLoss: 2.792545\n",
      "Train Epoch: 6 [4400/9840 (45%)]\tLoss: 1.206428\n",
      "Train Epoch: 6 [4800/9840 (49%)]\tLoss: 1.491321\n",
      "Train Epoch: 6 [5200/9840 (53%)]\tLoss: 1.505315\n",
      "Train Epoch: 6 [5600/9840 (57%)]\tLoss: 0.936067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [6000/9840 (61%)]\tLoss: 1.300586\n",
      "Train Epoch: 6 [6400/9840 (65%)]\tLoss: 1.807060\n",
      "Train Epoch: 6 [6800/9840 (69%)]\tLoss: 0.743654\n",
      "Train Epoch: 6 [7200/9840 (73%)]\tLoss: 1.232577\n",
      "Train Epoch: 6 [7600/9840 (77%)]\tLoss: 2.813485\n",
      "Train Epoch: 6 [8000/9840 (81%)]\tLoss: 1.447519\n",
      "Train Epoch: 6 [8400/9840 (85%)]\tLoss: 0.289811\n",
      "Train Epoch: 6 [8800/9840 (89%)]\tLoss: 1.089924\n",
      "Train Epoch: 6 [9200/9840 (93%)]\tLoss: 1.721156\n",
      "Train Epoch: 6 [9600/9840 (98%)]\tLoss: 2.070853\n",
      "\n",
      "Test set: Average loss: 0.3392, Accuracy: 1525/2468 (62%)\n",
      "\n",
      "Train Epoch: 7 [0/9840 (0%)]\tLoss: 1.203844\n",
      "Train Epoch: 7 [400/9840 (4%)]\tLoss: 3.600948\n",
      "Train Epoch: 7 [800/9840 (8%)]\tLoss: 0.058671\n",
      "Train Epoch: 7 [1200/9840 (12%)]\tLoss: 1.603901\n",
      "Train Epoch: 7 [1600/9840 (16%)]\tLoss: 0.172065\n",
      "Train Epoch: 7 [2000/9840 (20%)]\tLoss: 1.913331\n",
      "Train Epoch: 7 [2400/9840 (24%)]\tLoss: 1.708518\n",
      "Train Epoch: 7 [2800/9840 (28%)]\tLoss: 2.017997\n",
      "Train Epoch: 7 [3200/9840 (33%)]\tLoss: 3.384535\n",
      "Train Epoch: 7 [3600/9840 (37%)]\tLoss: 0.707295\n",
      "Train Epoch: 7 [4000/9840 (41%)]\tLoss: 0.834031\n",
      "Train Epoch: 7 [4400/9840 (45%)]\tLoss: 1.014998\n",
      "Train Epoch: 7 [4800/9840 (49%)]\tLoss: 0.484997\n",
      "Train Epoch: 7 [5200/9840 (53%)]\tLoss: 0.097677\n",
      "Train Epoch: 7 [5600/9840 (57%)]\tLoss: 1.512904\n",
      "Train Epoch: 7 [6000/9840 (61%)]\tLoss: 1.870191\n",
      "Train Epoch: 7 [6400/9840 (65%)]\tLoss: 0.418395\n",
      "Train Epoch: 7 [6800/9840 (69%)]\tLoss: 0.769957\n",
      "Train Epoch: 7 [7200/9840 (73%)]\tLoss: 2.093945\n",
      "Train Epoch: 7 [7600/9840 (77%)]\tLoss: 1.950052\n",
      "Train Epoch: 7 [8000/9840 (81%)]\tLoss: 0.580728\n",
      "Train Epoch: 7 [8400/9840 (85%)]\tLoss: 1.258812\n",
      "Train Epoch: 7 [8800/9840 (89%)]\tLoss: 1.758295\n",
      "Train Epoch: 7 [9200/9840 (93%)]\tLoss: 0.629052\n",
      "Train Epoch: 7 [9600/9840 (98%)]\tLoss: 0.971587\n",
      "\n",
      "Test set: Average loss: 0.3613, Accuracy: 1475/2468 (60%)\n",
      "\n",
      "Train Epoch: 8 [0/9840 (0%)]\tLoss: 1.039088\n",
      "Train Epoch: 8 [400/9840 (4%)]\tLoss: 1.035279\n",
      "Train Epoch: 8 [800/9840 (8%)]\tLoss: 1.157211\n",
      "Train Epoch: 8 [1200/9840 (12%)]\tLoss: 1.659160\n",
      "Train Epoch: 8 [1600/9840 (16%)]\tLoss: 0.199990\n",
      "Train Epoch: 8 [2000/9840 (20%)]\tLoss: 2.518086\n",
      "Train Epoch: 8 [2400/9840 (24%)]\tLoss: 0.357273\n",
      "Train Epoch: 8 [2800/9840 (28%)]\tLoss: 0.590896\n",
      "Train Epoch: 8 [3200/9840 (33%)]\tLoss: 1.407695\n",
      "Train Epoch: 8 [3600/9840 (37%)]\tLoss: 1.193087\n",
      "Train Epoch: 8 [4000/9840 (41%)]\tLoss: 0.322783\n",
      "Train Epoch: 8 [4400/9840 (45%)]\tLoss: 2.446628\n",
      "Train Epoch: 8 [4800/9840 (49%)]\tLoss: 1.537954\n",
      "Train Epoch: 8 [5200/9840 (53%)]\tLoss: 2.093479\n",
      "Train Epoch: 8 [5600/9840 (57%)]\tLoss: 1.016173\n",
      "Train Epoch: 8 [6000/9840 (61%)]\tLoss: 0.767194\n",
      "Train Epoch: 8 [6400/9840 (65%)]\tLoss: 0.790644\n",
      "Train Epoch: 8 [6800/9840 (69%)]\tLoss: 0.051609\n",
      "Train Epoch: 8 [7200/9840 (73%)]\tLoss: 0.824345\n",
      "Train Epoch: 8 [7600/9840 (77%)]\tLoss: 0.069121\n",
      "Train Epoch: 8 [8000/9840 (81%)]\tLoss: 0.720231\n",
      "Train Epoch: 8 [8400/9840 (85%)]\tLoss: 0.767174\n",
      "Train Epoch: 8 [8800/9840 (89%)]\tLoss: 0.445773\n",
      "Train Epoch: 8 [9200/9840 (93%)]\tLoss: 1.078835\n",
      "Train Epoch: 8 [9600/9840 (98%)]\tLoss: 0.236319\n",
      "\n",
      "Test set: Average loss: 0.3462, Accuracy: 1516/2468 (61%)\n",
      "\n",
      "Train Epoch: 9 [0/9840 (0%)]\tLoss: 1.343574\n",
      "Train Epoch: 9 [400/9840 (4%)]\tLoss: 0.222320\n",
      "Train Epoch: 9 [800/9840 (8%)]\tLoss: 1.172756\n",
      "Train Epoch: 9 [1200/9840 (12%)]\tLoss: 1.532488\n",
      "Train Epoch: 9 [1600/9840 (16%)]\tLoss: 0.584621\n",
      "Train Epoch: 9 [2000/9840 (20%)]\tLoss: 1.835263\n",
      "Train Epoch: 9 [2400/9840 (24%)]\tLoss: 1.965885\n",
      "Train Epoch: 9 [2800/9840 (28%)]\tLoss: 0.628115\n",
      "Train Epoch: 9 [3200/9840 (33%)]\tLoss: 1.826878\n",
      "Train Epoch: 9 [3600/9840 (37%)]\tLoss: 2.069142\n",
      "Train Epoch: 9 [4000/9840 (41%)]\tLoss: 0.198498\n",
      "Train Epoch: 9 [4400/9840 (45%)]\tLoss: 0.936996\n",
      "Train Epoch: 9 [4800/9840 (49%)]\tLoss: 0.200228\n",
      "Train Epoch: 9 [5200/9840 (53%)]\tLoss: 0.459927\n",
      "Train Epoch: 9 [5600/9840 (57%)]\tLoss: 1.080793\n",
      "Train Epoch: 9 [6000/9840 (61%)]\tLoss: 1.190109\n",
      "Train Epoch: 9 [6400/9840 (65%)]\tLoss: 0.946398\n",
      "Train Epoch: 9 [6800/9840 (69%)]\tLoss: 0.307519\n",
      "Train Epoch: 9 [7200/9840 (73%)]\tLoss: 0.527752\n",
      "Train Epoch: 9 [7600/9840 (77%)]\tLoss: 0.759590\n",
      "Train Epoch: 9 [8000/9840 (81%)]\tLoss: 0.102245\n",
      "Train Epoch: 9 [8400/9840 (85%)]\tLoss: 0.645193\n",
      "Train Epoch: 9 [8800/9840 (89%)]\tLoss: 1.337009\n",
      "Train Epoch: 9 [9200/9840 (93%)]\tLoss: 1.343333\n",
      "Train Epoch: 9 [9600/9840 (98%)]\tLoss: 1.054157\n",
      "\n",
      "Test set: Average loss: 0.3253, Accuracy: 1569/2468 (64%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train(epoch):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.long().cuda()\n",
    "        data, target = Variable(data), Variable(target[:,0])\n",
    "        \n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        ##print(\"huehue\",data.size())\n",
    "        \n",
    "        \n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), 0.25)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-learning_rate, p.grad.data)\n",
    "            \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "def test():\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    for (data, target) in test_loader:\n",
    "        data, target = data.cuda(), target.long().cuda()\n",
    "        data, target = Variable(data), Variable(target[:,0])\n",
    "#         data = data.transpose(2,1)\n",
    "#         data = data.unsqueeze(-1)\n",
    "        output = model(data)\n",
    "\n",
    "        # sum up batch loss\n",
    "        test_loss += criterion(output, target).data[0]\n",
    "        # get the index of the max log-probability\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        #print(pred.size())\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'\n",
    "          .format(test_loss, correct, len(test_loader.dataset),\n",
    "                  100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "for epoch in range(10):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/9840 (0%)]\tLoss: 1.385352\n",
      "Train Epoch: 0 [400/9840 (4%)]\tLoss: 9.880922\n",
      "Train Epoch: 0 [800/9840 (8%)]\tLoss: 4.443917\n",
      "Train Epoch: 0 [1200/9840 (12%)]\tLoss: 4.223427\n",
      "Train Epoch: 0 [1600/9840 (16%)]\tLoss: 2.120834\n",
      "Train Epoch: 0 [2000/9840 (20%)]\tLoss: 1.975827\n",
      "Train Epoch: 0 [2400/9840 (24%)]\tLoss: 0.671768\n",
      "Train Epoch: 0 [2800/9840 (28%)]\tLoss: 1.639770\n",
      "Train Epoch: 0 [3200/9840 (33%)]\tLoss: 2.386112\n",
      "Train Epoch: 0 [3600/9840 (37%)]\tLoss: 3.714325\n",
      "Train Epoch: 0 [4000/9840 (41%)]\tLoss: 3.117979\n",
      "Train Epoch: 0 [4400/9840 (45%)]\tLoss: 2.262218\n",
      "Train Epoch: 0 [4800/9840 (49%)]\tLoss: 2.964238\n",
      "Train Epoch: 0 [5200/9840 (53%)]\tLoss: 1.995938\n",
      "Train Epoch: 0 [5600/9840 (57%)]\tLoss: 1.739147\n",
      "Train Epoch: 0 [6000/9840 (61%)]\tLoss: 2.958109\n",
      "Train Epoch: 0 [6400/9840 (65%)]\tLoss: 2.258731\n",
      "Train Epoch: 0 [6800/9840 (69%)]\tLoss: 2.597437\n",
      "Train Epoch: 0 [7200/9840 (73%)]\tLoss: 1.840538\n",
      "Train Epoch: 0 [7600/9840 (77%)]\tLoss: 2.114126\n",
      "Train Epoch: 0 [8000/9840 (81%)]\tLoss: 2.748051\n",
      "Train Epoch: 0 [8400/9840 (85%)]\tLoss: 1.826780\n",
      "Train Epoch: 0 [8800/9840 (89%)]\tLoss: 3.003615\n",
      "Train Epoch: 0 [9200/9840 (93%)]\tLoss: 2.127450\n",
      "Train Epoch: 0 [9600/9840 (98%)]\tLoss: 1.943026\n",
      "\n",
      "Test set: Average loss: 0.5272, Accuracy: 1023/2468 (41%)\n",
      "\n",
      "Train Epoch: 1 [0/9840 (0%)]\tLoss: 2.484677\n",
      "Train Epoch: 1 [400/9840 (4%)]\tLoss: 2.477159\n",
      "Train Epoch: 1 [800/9840 (8%)]\tLoss: 2.788670\n",
      "Train Epoch: 1 [1200/9840 (12%)]\tLoss: 2.785183\n",
      "Train Epoch: 1 [1600/9840 (16%)]\tLoss: 1.831433\n",
      "Train Epoch: 1 [2000/9840 (20%)]\tLoss: 1.496346\n",
      "Train Epoch: 1 [2400/9840 (24%)]\tLoss: 1.156349\n",
      "Train Epoch: 1 [2800/9840 (28%)]\tLoss: 2.953100\n",
      "Train Epoch: 1 [3200/9840 (33%)]\tLoss: 3.121264\n",
      "Train Epoch: 1 [3600/9840 (37%)]\tLoss: 1.496167\n",
      "Train Epoch: 1 [4000/9840 (41%)]\tLoss: 2.094278\n",
      "Train Epoch: 1 [4400/9840 (45%)]\tLoss: 1.802007\n",
      "Train Epoch: 1 [4800/9840 (49%)]\tLoss: 2.144402\n",
      "Train Epoch: 1 [5200/9840 (53%)]\tLoss: 2.217875\n",
      "Train Epoch: 1 [5600/9840 (57%)]\tLoss: 2.173882\n",
      "Train Epoch: 1 [6000/9840 (61%)]\tLoss: 0.735408\n",
      "Train Epoch: 1 [6400/9840 (65%)]\tLoss: 1.831007\n",
      "Train Epoch: 1 [6800/9840 (69%)]\tLoss: 1.985666\n",
      "Train Epoch: 1 [7200/9840 (73%)]\tLoss: 0.310935\n",
      "Train Epoch: 1 [7600/9840 (77%)]\tLoss: 1.648558\n",
      "Train Epoch: 1 [8000/9840 (81%)]\tLoss: 1.960243\n",
      "Train Epoch: 1 [8400/9840 (85%)]\tLoss: 2.511436\n",
      "Train Epoch: 1 [8800/9840 (89%)]\tLoss: 1.704683\n",
      "Train Epoch: 1 [9200/9840 (93%)]\tLoss: 1.521427\n",
      "Train Epoch: 1 [9600/9840 (98%)]\tLoss: 1.974535\n",
      "\n",
      "Test set: Average loss: 0.4543, Accuracy: 1179/2468 (48%)\n",
      "\n",
      "Train Epoch: 2 [0/9840 (0%)]\tLoss: 1.444934\n",
      "Train Epoch: 2 [400/9840 (4%)]\tLoss: 1.017542\n",
      "Train Epoch: 2 [800/9840 (8%)]\tLoss: 1.396810\n",
      "Train Epoch: 2 [1200/9840 (12%)]\tLoss: 0.197308\n",
      "Train Epoch: 2 [1600/9840 (16%)]\tLoss: 1.383450\n",
      "Train Epoch: 2 [2000/9840 (20%)]\tLoss: 1.198321\n",
      "Train Epoch: 2 [2400/9840 (24%)]\tLoss: 2.652550\n",
      "Train Epoch: 2 [2800/9840 (28%)]\tLoss: 1.229087\n",
      "Train Epoch: 2 [3200/9840 (33%)]\tLoss: 2.260607\n",
      "Train Epoch: 2 [3600/9840 (37%)]\tLoss: 1.362514\n",
      "Train Epoch: 2 [4000/9840 (41%)]\tLoss: 2.076193\n",
      "Train Epoch: 2 [4400/9840 (45%)]\tLoss: 1.529911\n",
      "Train Epoch: 2 [4800/9840 (49%)]\tLoss: 1.207494\n",
      "Train Epoch: 2 [5200/9840 (53%)]\tLoss: 1.745813\n",
      "Train Epoch: 2 [5600/9840 (57%)]\tLoss: 0.186338\n",
      "Train Epoch: 2 [6000/9840 (61%)]\tLoss: 1.408916\n",
      "Train Epoch: 2 [6400/9840 (65%)]\tLoss: 1.591078\n",
      "Train Epoch: 2 [6800/9840 (69%)]\tLoss: 1.346274\n",
      "Train Epoch: 2 [7200/9840 (73%)]\tLoss: 1.816093\n",
      "Train Epoch: 2 [7600/9840 (77%)]\tLoss: 0.551889\n",
      "Train Epoch: 2 [8000/9840 (81%)]\tLoss: 1.462777\n",
      "Train Epoch: 2 [8400/9840 (85%)]\tLoss: 2.701431\n",
      "Train Epoch: 2 [8800/9840 (89%)]\tLoss: 1.531593\n",
      "Train Epoch: 2 [9200/9840 (93%)]\tLoss: 0.885217\n",
      "Train Epoch: 2 [9600/9840 (98%)]\tLoss: 3.041683\n",
      "\n",
      "Test set: Average loss: 0.4013, Accuracy: 1305/2468 (53%)\n",
      "\n",
      "Train Epoch: 3 [0/9840 (0%)]\tLoss: 0.424563\n",
      "Train Epoch: 3 [400/9840 (4%)]\tLoss: 1.103957\n",
      "Train Epoch: 3 [800/9840 (8%)]\tLoss: 2.012105\n",
      "Train Epoch: 3 [1200/9840 (12%)]\tLoss: 0.186089\n",
      "Train Epoch: 3 [1600/9840 (16%)]\tLoss: 0.205630\n",
      "Train Epoch: 3 [2000/9840 (20%)]\tLoss: 1.563480\n",
      "Train Epoch: 3 [2400/9840 (24%)]\tLoss: 0.509103\n",
      "Train Epoch: 3 [2800/9840 (28%)]\tLoss: 0.872602\n",
      "Train Epoch: 3 [3200/9840 (33%)]\tLoss: 0.861274\n",
      "Train Epoch: 3 [3600/9840 (37%)]\tLoss: 1.428668\n",
      "Train Epoch: 3 [4000/9840 (41%)]\tLoss: 1.365250\n",
      "Train Epoch: 3 [4400/9840 (45%)]\tLoss: 1.981894\n",
      "Train Epoch: 3 [4800/9840 (49%)]\tLoss: 0.584964\n",
      "Train Epoch: 3 [5200/9840 (53%)]\tLoss: 1.509576\n",
      "Train Epoch: 3 [5600/9840 (57%)]\tLoss: 1.251673\n",
      "Train Epoch: 3 [6000/9840 (61%)]\tLoss: 0.320549\n",
      "Train Epoch: 3 [6400/9840 (65%)]\tLoss: 1.711422\n",
      "Train Epoch: 3 [6800/9840 (69%)]\tLoss: 1.379213\n",
      "Train Epoch: 3 [7200/9840 (73%)]\tLoss: 1.306445\n",
      "Train Epoch: 3 [7600/9840 (77%)]\tLoss: 4.608987\n",
      "Train Epoch: 3 [8000/9840 (81%)]\tLoss: 2.060195\n",
      "Train Epoch: 3 [8400/9840 (85%)]\tLoss: 1.256918\n",
      "Train Epoch: 3 [8800/9840 (89%)]\tLoss: 1.702050\n",
      "Train Epoch: 3 [9200/9840 (93%)]\tLoss: 0.658994\n",
      "Train Epoch: 3 [9600/9840 (98%)]\tLoss: 1.636881\n",
      "\n",
      "Test set: Average loss: 0.3758, Accuracy: 1389/2468 (56%)\n",
      "\n",
      "Train Epoch: 4 [0/9840 (0%)]\tLoss: 1.221456\n",
      "Train Epoch: 4 [400/9840 (4%)]\tLoss: 3.330194\n",
      "Train Epoch: 4 [800/9840 (8%)]\tLoss: 2.682848\n",
      "Train Epoch: 4 [1200/9840 (12%)]\tLoss: 0.558595\n",
      "Train Epoch: 4 [1600/9840 (16%)]\tLoss: 1.738355\n",
      "Train Epoch: 4 [2000/9840 (20%)]\tLoss: 2.424235\n",
      "Train Epoch: 4 [2400/9840 (24%)]\tLoss: 2.071452\n",
      "Train Epoch: 4 [2800/9840 (28%)]\tLoss: 0.662445\n",
      "Train Epoch: 4 [3200/9840 (33%)]\tLoss: 1.655947\n",
      "Train Epoch: 4 [3600/9840 (37%)]\tLoss: 2.814864\n",
      "Train Epoch: 4 [4000/9840 (41%)]\tLoss: 0.195452\n",
      "Train Epoch: 4 [4400/9840 (45%)]\tLoss: 0.550286\n",
      "Train Epoch: 4 [4800/9840 (49%)]\tLoss: 0.390055\n",
      "Train Epoch: 4 [5200/9840 (53%)]\tLoss: 1.505838\n",
      "Train Epoch: 4 [5600/9840 (57%)]\tLoss: 1.033865\n",
      "Train Epoch: 4 [6000/9840 (61%)]\tLoss: 1.005900\n",
      "Train Epoch: 4 [6400/9840 (65%)]\tLoss: 1.085066\n",
      "Train Epoch: 4 [6800/9840 (69%)]\tLoss: 3.684656\n",
      "Train Epoch: 4 [7200/9840 (73%)]\tLoss: 0.702783\n",
      "Train Epoch: 4 [7600/9840 (77%)]\tLoss: 1.085473\n",
      "Train Epoch: 4 [8000/9840 (81%)]\tLoss: 1.021225\n",
      "Train Epoch: 4 [8400/9840 (85%)]\tLoss: 0.413709\n",
      "Train Epoch: 4 [8800/9840 (89%)]\tLoss: 0.642614\n",
      "Train Epoch: 4 [9200/9840 (93%)]\tLoss: 0.854939\n",
      "Train Epoch: 4 [9600/9840 (98%)]\tLoss: 1.415302\n",
      "\n",
      "Test set: Average loss: 0.3628, Accuracy: 1413/2468 (57%)\n",
      "\n",
      "Train Epoch: 5 [0/9840 (0%)]\tLoss: 1.204682\n",
      "Train Epoch: 5 [400/9840 (4%)]\tLoss: 1.188244\n",
      "Train Epoch: 5 [800/9840 (8%)]\tLoss: 1.838517\n",
      "Train Epoch: 5 [1200/9840 (12%)]\tLoss: 0.396593\n",
      "Train Epoch: 5 [1600/9840 (16%)]\tLoss: 1.319499\n",
      "Train Epoch: 5 [2000/9840 (20%)]\tLoss: 0.284820\n",
      "Train Epoch: 5 [2400/9840 (24%)]\tLoss: 1.613966\n",
      "Train Epoch: 5 [2800/9840 (28%)]\tLoss: 2.771614\n",
      "Train Epoch: 5 [3200/9840 (33%)]\tLoss: 2.016484\n",
      "Train Epoch: 5 [3600/9840 (37%)]\tLoss: 0.598391\n",
      "Train Epoch: 5 [4000/9840 (41%)]\tLoss: 0.364424\n",
      "Train Epoch: 5 [4400/9840 (45%)]\tLoss: 0.493198\n",
      "Train Epoch: 5 [4800/9840 (49%)]\tLoss: 0.081271\n",
      "Train Epoch: 5 [5200/9840 (53%)]\tLoss: 1.547060\n",
      "Train Epoch: 5 [5600/9840 (57%)]\tLoss: 0.476358\n",
      "Train Epoch: 5 [6000/9840 (61%)]\tLoss: 2.743768\n",
      "Train Epoch: 5 [6400/9840 (65%)]\tLoss: 1.740805\n",
      "Train Epoch: 5 [6800/9840 (69%)]\tLoss: 0.578000\n",
      "Train Epoch: 5 [7200/9840 (73%)]\tLoss: 1.013943\n",
      "Train Epoch: 5 [7600/9840 (77%)]\tLoss: 1.624938\n",
      "Train Epoch: 5 [8000/9840 (81%)]\tLoss: 0.055889\n",
      "Train Epoch: 5 [8400/9840 (85%)]\tLoss: 1.268290\n",
      "Train Epoch: 5 [8800/9840 (89%)]\tLoss: 0.977020\n",
      "Train Epoch: 5 [9200/9840 (93%)]\tLoss: 0.891416\n",
      "Train Epoch: 5 [9600/9840 (98%)]\tLoss: 1.864271\n",
      "\n",
      "Test set: Average loss: 0.3331, Accuracy: 1504/2468 (61%)\n",
      "\n",
      "Train Epoch: 6 [0/9840 (0%)]\tLoss: 0.293201\n",
      "Train Epoch: 6 [400/9840 (4%)]\tLoss: 1.591384\n",
      "Train Epoch: 6 [800/9840 (8%)]\tLoss: 2.233526\n",
      "Train Epoch: 6 [1200/9840 (12%)]\tLoss: 0.912312\n",
      "Train Epoch: 6 [1600/9840 (16%)]\tLoss: 1.120644\n",
      "Train Epoch: 6 [2000/9840 (20%)]\tLoss: 1.845600\n",
      "Train Epoch: 6 [2400/9840 (24%)]\tLoss: 0.657392\n",
      "Train Epoch: 6 [2800/9840 (28%)]\tLoss: 1.605193\n",
      "Train Epoch: 6 [3200/9840 (33%)]\tLoss: 0.620421\n",
      "Train Epoch: 6 [3600/9840 (37%)]\tLoss: 1.008273\n",
      "Train Epoch: 6 [4000/9840 (41%)]\tLoss: 1.053858\n",
      "Train Epoch: 6 [4400/9840 (45%)]\tLoss: 0.624050\n",
      "Train Epoch: 6 [4800/9840 (49%)]\tLoss: 0.896990\n",
      "Train Epoch: 6 [5200/9840 (53%)]\tLoss: 0.946478\n",
      "Train Epoch: 6 [5600/9840 (57%)]\tLoss: 0.320232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [6000/9840 (61%)]\tLoss: 1.320419\n",
      "Train Epoch: 6 [6400/9840 (65%)]\tLoss: 0.896269\n",
      "Train Epoch: 6 [6800/9840 (69%)]\tLoss: 1.140698\n",
      "Train Epoch: 6 [7200/9840 (73%)]\tLoss: 1.004391\n",
      "Train Epoch: 6 [7600/9840 (77%)]\tLoss: 1.775482\n",
      "Train Epoch: 6 [8000/9840 (81%)]\tLoss: 1.723288\n",
      "Train Epoch: 6 [8400/9840 (85%)]\tLoss: 2.067725\n",
      "Train Epoch: 6 [8800/9840 (89%)]\tLoss: 1.344197\n",
      "Train Epoch: 6 [9200/9840 (93%)]\tLoss: 1.431569\n",
      "Train Epoch: 6 [9600/9840 (98%)]\tLoss: 0.538778\n",
      "\n",
      "Test set: Average loss: 0.3304, Accuracy: 1491/2468 (60%)\n",
      "\n",
      "Train Epoch: 7 [0/9840 (0%)]\tLoss: 2.074849\n",
      "Train Epoch: 7 [400/9840 (4%)]\tLoss: 1.317554\n",
      "Train Epoch: 7 [800/9840 (8%)]\tLoss: 0.874123\n",
      "Train Epoch: 7 [1200/9840 (12%)]\tLoss: 1.762091\n",
      "Train Epoch: 7 [1600/9840 (16%)]\tLoss: 0.599851\n",
      "Train Epoch: 7 [2000/9840 (20%)]\tLoss: 0.266940\n",
      "Train Epoch: 7 [2400/9840 (24%)]\tLoss: 1.586520\n",
      "Train Epoch: 7 [2800/9840 (28%)]\tLoss: 1.489334\n",
      "Train Epoch: 7 [3200/9840 (33%)]\tLoss: 0.887182\n",
      "Train Epoch: 7 [3600/9840 (37%)]\tLoss: 1.989506\n",
      "Train Epoch: 7 [4000/9840 (41%)]\tLoss: 1.941105\n",
      "Train Epoch: 7 [4400/9840 (45%)]\tLoss: 1.201492\n",
      "Train Epoch: 7 [4800/9840 (49%)]\tLoss: 1.087325\n",
      "Train Epoch: 7 [5200/9840 (53%)]\tLoss: 0.276102\n",
      "Train Epoch: 7 [5600/9840 (57%)]\tLoss: 2.034123\n",
      "Train Epoch: 7 [6000/9840 (61%)]\tLoss: 3.542970\n",
      "Train Epoch: 7 [6400/9840 (65%)]\tLoss: 2.520802\n",
      "Train Epoch: 7 [6800/9840 (69%)]\tLoss: 0.539908\n",
      "Train Epoch: 7 [7200/9840 (73%)]\tLoss: 0.482229\n",
      "Train Epoch: 7 [7600/9840 (77%)]\tLoss: 1.317610\n",
      "Train Epoch: 7 [8000/9840 (81%)]\tLoss: 0.231673\n",
      "Train Epoch: 7 [8400/9840 (85%)]\tLoss: 1.296115\n",
      "Train Epoch: 7 [8800/9840 (89%)]\tLoss: 0.691731\n",
      "Train Epoch: 7 [9200/9840 (93%)]\tLoss: 1.191509\n",
      "Train Epoch: 7 [9600/9840 (98%)]\tLoss: 1.861029\n",
      "\n",
      "Test set: Average loss: 0.3145, Accuracy: 1533/2468 (62%)\n",
      "\n",
      "Train Epoch: 8 [0/9840 (0%)]\tLoss: 1.734957\n",
      "Train Epoch: 8 [400/9840 (4%)]\tLoss: 0.450217\n",
      "Train Epoch: 8 [800/9840 (8%)]\tLoss: 2.122794\n",
      "Train Epoch: 8 [1200/9840 (12%)]\tLoss: 1.418554\n",
      "Train Epoch: 8 [1600/9840 (16%)]\tLoss: 3.319034\n",
      "Train Epoch: 8 [2000/9840 (20%)]\tLoss: 2.301819\n",
      "Train Epoch: 8 [2400/9840 (24%)]\tLoss: 0.468808\n",
      "Train Epoch: 8 [2800/9840 (28%)]\tLoss: 0.135752\n",
      "Train Epoch: 8 [3200/9840 (33%)]\tLoss: 2.086510\n",
      "Train Epoch: 8 [3600/9840 (37%)]\tLoss: 0.700050\n",
      "Train Epoch: 8 [4000/9840 (41%)]\tLoss: 1.667415\n",
      "Train Epoch: 8 [4400/9840 (45%)]\tLoss: 0.449905\n",
      "Train Epoch: 8 [4800/9840 (49%)]\tLoss: 1.229618\n",
      "Train Epoch: 8 [5200/9840 (53%)]\tLoss: 1.765563\n",
      "Train Epoch: 8 [5600/9840 (57%)]\tLoss: 0.674707\n",
      "Train Epoch: 8 [6000/9840 (61%)]\tLoss: 1.186331\n",
      "Train Epoch: 8 [6400/9840 (65%)]\tLoss: 0.425984\n",
      "Train Epoch: 8 [6800/9840 (69%)]\tLoss: 0.238155\n",
      "Train Epoch: 8 [7200/9840 (73%)]\tLoss: 2.035483\n",
      "Train Epoch: 8 [7600/9840 (77%)]\tLoss: 0.251810\n",
      "Train Epoch: 8 [8000/9840 (81%)]\tLoss: 2.482581\n",
      "Train Epoch: 8 [8400/9840 (85%)]\tLoss: 0.670827\n",
      "Train Epoch: 8 [8800/9840 (89%)]\tLoss: 1.015468\n",
      "Train Epoch: 8 [9200/9840 (93%)]\tLoss: 1.107513\n",
      "Train Epoch: 8 [9600/9840 (98%)]\tLoss: 1.147302\n",
      "\n",
      "Test set: Average loss: 0.3096, Accuracy: 1558/2468 (63%)\n",
      "\n",
      "Train Epoch: 9 [0/9840 (0%)]\tLoss: 0.717715\n",
      "Train Epoch: 9 [400/9840 (4%)]\tLoss: 1.942086\n",
      "Train Epoch: 9 [800/9840 (8%)]\tLoss: 1.040428\n",
      "Train Epoch: 9 [1200/9840 (12%)]\tLoss: 2.142265\n",
      "Train Epoch: 9 [1600/9840 (16%)]\tLoss: 0.282197\n",
      "Train Epoch: 9 [2000/9840 (20%)]\tLoss: 0.934079\n",
      "Train Epoch: 9 [2400/9840 (24%)]\tLoss: 1.952730\n",
      "Train Epoch: 9 [2800/9840 (28%)]\tLoss: 0.904280\n",
      "Train Epoch: 9 [3200/9840 (33%)]\tLoss: 1.269118\n",
      "Train Epoch: 9 [3600/9840 (37%)]\tLoss: 1.465212\n",
      "Train Epoch: 9 [4000/9840 (41%)]\tLoss: 0.381995\n",
      "Train Epoch: 9 [4400/9840 (45%)]\tLoss: 2.790221\n",
      "Train Epoch: 9 [4800/9840 (49%)]\tLoss: 1.874024\n",
      "Train Epoch: 9 [5200/9840 (53%)]\tLoss: 0.940682\n",
      "Train Epoch: 9 [5600/9840 (57%)]\tLoss: 0.513140\n",
      "Train Epoch: 9 [6000/9840 (61%)]\tLoss: 1.477086\n",
      "Train Epoch: 9 [6400/9840 (65%)]\tLoss: 0.937152\n",
      "Train Epoch: 9 [6800/9840 (69%)]\tLoss: 1.660645\n",
      "Train Epoch: 9 [7200/9840 (73%)]\tLoss: 6.231034\n",
      "Train Epoch: 9 [7600/9840 (77%)]\tLoss: 4.813499\n",
      "Train Epoch: 9 [8000/9840 (81%)]\tLoss: 2.230612\n",
      "Train Epoch: 9 [8400/9840 (85%)]\tLoss: 3.729224\n",
      "Train Epoch: 9 [8800/9840 (89%)]\tLoss: 2.621640\n",
      "Train Epoch: 9 [9200/9840 (93%)]\tLoss: 3.858220\n",
      "Train Epoch: 9 [9600/9840 (98%)]\tLoss: 1.923868\n",
      "\n",
      "Test set: Average loss: 0.6198, Accuracy: 727/2468 (29%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train(epoch):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        permutations = torch.randperm(2048)\n",
    "        data = data[:,permutations]\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.long().cuda()\n",
    "        data, target = Variable(data), Variable(target[:,0])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        ##print(\"huehue\",data.size())        \n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "def test():\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    for (data, target) in test_loader:\n",
    "        data, target = data.cuda(), target.long().cuda()\n",
    "        data, target = Variable(data), Variable(target[:,0])\n",
    "#         data = data.transpose(2,1)\n",
    "#         data = data.unsqueeze(-1)\n",
    "        output = model(data)\n",
    "\n",
    "        # sum up batch loss\n",
    "        test_loss += criterion(output, target).data[0]\n",
    "        # get the index of the max log-probability\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        #print(pred.size())\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'\n",
    "          .format(test_loss, correct, len(test_loader.dataset),\n",
    "                  100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "for epoch in range(10):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with more epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### without permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/9840 (0%)]\tLoss: 1.955693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-386:\n",
      "Process Process-385:\n",
      "Process Process-388:\n",
      "Process Process-387:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yi/.virtualenvs/py3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 35, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/yi/.virtualenvs/py3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 35, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/yi/.virtualenvs/py3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 35, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/yi/.virtualenvs/py3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 35, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <bound method TensorDescriptorArray.__del__ of <torch.backends.cudnn.TensorDescriptorArray object at 0x7ffb8c235048>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yi/.virtualenvs/py3/lib/python3.5/site-packages/torch/backends/cudnn/__init__.py\", line 152, in __del__\n",
      "    check_error(lib.cudnnDestroyTensorDescriptor(ctypes.c_void_p(ptr)))\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-a0e65f3ac9ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-a0e65f3ac9ec>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3/lib/python3.5/site-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrebuild_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mdetach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;34m'''Get the fd.  This should only be called once.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_resource_sharer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mget_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mident\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/connection.py\u001b[0m in \u001b[0;36mClient\u001b[0;34m(address, family, authkey)\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSocketClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mauthkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/connection.py\u001b[0m in \u001b[0;36mSocketClient\u001b[0;34m(address)\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetblocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mConnection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory"
     ]
    }
   ],
   "source": [
    "def train(epoch):\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.long().cuda()\n",
    "        data, target = Variable(data), Variable(target[:,0])\n",
    "        \n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        ##print(\"huehue\",data.size())\n",
    "        \n",
    "        \n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), 0.25)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-learning_rate, p.grad.data)\n",
    "            \n",
    "        optimizer.step()\n",
    "        \n",
    "#         pred = output.data.max(1, keepdim=True)[1]\n",
    "#         correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t '.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "def test():\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    for (data, target) in test_loader:\n",
    "        data, target = data.cuda(), target.long().cuda()\n",
    "        data, target = Variable(data), Variable(target[:,0])\n",
    "#         data = data.transpose(2,1)\n",
    "#         data = data.unsqueeze(-1)\n",
    "        output = model(data)\n",
    "\n",
    "        # sum up batch loss\n",
    "        test_loss += criterion(output, target).data[0]\n",
    "        # get the index of the max log-probability\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        #print(pred.size())\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'\n",
    "          .format(test_loss, correct, len(test_loader.dataset),\n",
    "                  100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        permutations = torch.randperm(2048)\n",
    "        data = data[:,permutations]\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.long().cuda()\n",
    "        data, target = Variable(data), Variable(target[:,0])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        ##print(\"huehue\",data.size())        \n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "def test():\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    for (data, target) in test_loader:\n",
    "        data, target = data.cuda(), target.long().cuda()\n",
    "        data, target = Variable(data), Variable(target[:,0])\n",
    "#         data = data.transpose(2,1)\n",
    "#         data = data.unsqueeze(-1)\n",
    "        output = model(data)\n",
    "\n",
    "        # sum up batch loss\n",
    "        test_loss += criterion(output, target).data[0]\n",
    "        # get the index of the max log-probability\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        #print(pred.size())\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'\n",
    "          .format(test_loss, correct, len(test_loader.dataset),\n",
    "                  100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
